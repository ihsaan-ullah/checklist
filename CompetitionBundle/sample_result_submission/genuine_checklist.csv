Question,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Yes,Our main claims are listed in section 1 and highlight this paper's contributions.,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ","The answer provided by the authors is consistent with the content of the paper. The abstract and introduction sections accurately outline the paper's primary contributions and set clear expectations for readers regarding the scope of the research. The authors claim to address significant issues in Neural Architecture Search (NAS) for Spatio-Temporal Graph Neural Networks (STGNNs), specifically the integration of hyperparameters into the NAS framework and the challenge of navigating a large search space. These claims are substantiated through the detailed explanation and results presented in the subsequent sections.

**Justification:**
- **Reflection of Contributions:** The abstract clearly states the paper's contributions, including the proposition of a disentangled framework to simplify the architecture search and the methodological advancements to allow fair and efficient evaluation of model configurations. These are major contributions that directly address the paper's goals, as introduced.
- **Scope Accuracy:** Both the abstract and introduction accurately reflect the scope of the paper, setting realistic expectations for the readers. The paper focuses on STGNNs, particularly in the context of traffic prediction, and aims to improve the efficiency and accuracy of architecture searches. This scope is well communicated and adhered to throughout the paper.
- **Match Between Claims and Content:** The main claims made in the introductory sections match the theoretical insights and empirical results presented later in the document. The authors not only propose a novel framework but also demonstrate its efficacy through comprehensive experiments and comparisons with state-of-the-art methods.

**Constructive Suggestions:**
1. **Clarify Limitations Early:** Although the paper does discuss limitations and potential areas for future work, integrating a brief mention of the primary constraints or challenges faced by the proposed methodologies early in the paper might help set more precise expectations.
2. **Comparative Analysis:** While the paper provides a comparative analysis demonstrating the superiority of the proposed methods, including additional discussion on why certain methodologies outperform others could enrich the reader's understanding.
3. **Exploration Beyond Traffic Prediction:** Highlighting potential applicability or the need for adaptation of the proposed framework for other types of spatio-temporal graph data early in the paper could expand its perceived scope and relevance.",1
Does the paper discuss the limitations of the work performed by the authors?,Yes,"We have discussed the potential limitations in section 6, including the application domain and considered temporal modeling methods.","• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ","The justification provided by the authors clearly states that they discussed potential limitations in Section 6 of their paper, including the application domain and the considered temporal modeling methods. This directly answers the question, confirming that the paper indeed discusses the limitations of the work performed by the authors. Based on the guidelines provided, discussing limitations is a crucial part of a research paper as it provides transparency about the scope and applicability of the research findings. The authors' decision to include a discussion on the limitations aligns with the expectation of acknowledging and reflecting on the strengths and constraints of their work, as well as its potential impact on the robustness and generalizability of the results.

However, to further improve the limitations section and ensure it meets the highest standards of completeness and transparency, the authors might consider the following constructive suggestions:

1. **Expand on Assumption Limitations**: Clearly outline any assumptions made during the research and discuss potential scenarios where these assumptions may not hold. This can include assumptions on data distribution, model specifications, and environmental settings in which the model was tested.
   
2. **Robustness and Generalizability**: Elaborate on how the model's performance could vary with different datasets, outside the ones tested, or under varying conditions such as noise levels, dataset sizes, and feature distributions.

3. **Computational Efficiency**: If not already included, discuss the computational demands of the proposed framework, especially in comparison to existing methods. This is important for understanding the practicality of deploying the model in real-world applications.

4. **Impact of Hyperparameter Choices**: Provide insights into how sensitive the model's performance is to hyperparameter settings, and whether there are challenges in finding optimal hyperparameters.

5. **Potential for Bias and Fairness Concerns**: Reflect on any aspects of the model or data that could introduce bias or fairness issues, particularly if the model is used in diverse application domains.

6. **Future Directions**: Briefly discuss potential avenues for future research that could overcome the current limitations, providing a roadmap for advancing the work presented.

By addressing these areas, the authors can offer a more detailed and nuanced view of their work's limitations, contributing to a well-rounded and thoroughly vetted piece of research. This not only fosters trust and credibility but also provides a foundation for future work to build upon.",1
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Not Applicable,This paper does not include theoretical results.,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. ","The provided answer is consistent with the content of the paper. The paper primarily focuses on disentangling the framework for Spatio-Temporal Graph Neural Networks (STGNNs), studying the influence of different architectural choices and hyperparameters on the performance of STGNNs, and proposing a methodology to simplify the architecture search process. The main contributions revolve around empirical findings and methodological innovations rather than theoretical results. Therefore, the response ""NA"" accurately reflects that the paper does not introduce theoretical results, thus not requiring the full set of assumptions or complete proofs as outlined in the guidelines.

Despite the correctness of the answer, for completeness and to encourage thoroughness in future work, the review might include suggestions to the authors, emphasizing areas for potential theoretical exploration that could strengthen the work:
1. Encourage the exploration of theoretical aspects related to the proposed disentangled framework. For instance, formalizing the implications of the identified design principles on the STGNNs' representational capabilities or convergence properties could provide deeper insights.
2. Suggest the consideration of theoretical analysis that could quantify the impact of hyperparameters on model performance, beyond empirical observations. Establishing theoretical bounds or relationships could provide a more robust understanding of the observed phenomena.
3. Recommend investigating the theoretical properties of the simplified NAS method introduced, particularly in the context of search efficiency and scalability. Understanding the theoretical limits of such a methodology could guide further improvements.
4. Highlight the importance of theoretical contributions in providing a bridge between empirical observations and foundational principles. Encouraging the authors to think along these lines for future iterations of their work could enhance the rigor and depth of their research.

By addressing these points, the authors might find avenues to incorporate theoretical aspects into their work, enhancing its comprehensiveness and appeal to a broader audience within the research community.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Yes,"We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.","• The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. Open Access to Data and Code","The paper provides thorough details and guidelines for reproducing the main experimental results, covering various aspects such as the experiment settings, dataset descriptions, baseline comparisons, implementation details, and results across multiple sections and appendices. Additionally, the availability of the code through a provided GitHub link further enhances the reproducibility of the paper's contributions. Given this comprehensive approach to disclosing experimental procedures and results along with the provision of code, the justification aligns with the efforts made by the authors to ensure the paper meets the necessary standards for reproducibility.

However, to further enhance the reproducibility and clarity of the paper, the following constructive and actionable suggestions are offered:
1. **Dataset Accessibility**: While the GitHub link is mentioned, ensuring that the link is functional and that the repository contains clear instructions on how to access and use the datasets for reproduction would be beneficial.
2. **Baseline Implementation Details**: More explicit details or references regarding the implementation of baseline models could help readers better understand any nuances in replicating these comparisons.
3. **Hyperparameters and Model Configurations**: Providing a dedicated section or table summarizing all critical hyperparameters and model configurations used in the experiments could help streamline the reproduction process.
4. **Experiment Environment**: Details about the computational environment (e.g., hardware configurations, software versions) can significantly impact reproduction efforts, especially for computationally intensive models.
5. **Additional Examples**: Including a few examples of running the code for specific experiments would guide readers through the process and help clarify any potential ambiguities.

By addressing these points, the authors can further solidify the reproducibility of their work and assist the research community in building upon their findings.",1
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Yes,"All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.","• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ","The paper does address the requirement for open access to data and code, providing both the URL to the GitHub repository containing the code and specifying the datasets used in the experiments. The datasets mentioned (PeMS03, PeMS04, PeMS07, and PeMS08) are publicly available and commonly used in traffic forecasting research, which supports reproducibility. Furthermore, the detailed description in section B of the Appendix regarding the datasets and problem setting gives sufficient context for readers wanting to reproduce the experiments. Also, the mention of the GitHub URL (https://github.com/AutoML-Research/SimpleSTG) for accessing the code meets the guidelines for code submission.

Justification Review:
1. **Open Access to Data and Code**: The paper explicitly mentions the GitHub URL for the code and references the sections and appendices where datasets are detailed, which aligns with the open-access criteria.
2. **Sufficiency for Reproducing Results**: The citation of both the code repository and datasets, alongside the details provided in section B of the appendix regarding dataset statistics and the problem setting, indicates an effort to allow for reproducibility. However, to fully validate the sufficiency, a closer look at the GitHub repository to ensure it contains necessary instructions and scripts for reproducing the results would be ideal, which is beyond the scope of this review based on the content provided.

Constructive Suggestions:
- **Detailed Reproduction Instructions**: To enhance the reproducibility further, the authors should ensure that the repository contains a README file with step-by-step instructions on how to set up the environment, preprocess the data, and execute the scripts to reproduce the experimental results. Including requirements.txt or environment.yml files to facilitate environment setup would also be beneficial.
- **Clarity on Reproducible Experiments**: Clarify in the paper or GitHub README which specific results or experiments from the paper can be exactly reproduced with the provided code and data. If there are any results that cannot be reproduced (due to randomization or extensive computational requirements), this should be explicitly stated.
- **Anonymity Preservation**: Since the review mentions maintaining anonymity, it is assumed this is a double-blind review process. The authors should ensure that the GitHub repository they link to doesn't break the double-blind review requirement by inadvertently revealing their identity. An anonymized version of the repository should be maintained during the review process.

Overall, the paper meets the criteria for providing open access to data and code, supporting the reproducibility of the main experimental results as described.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Yes,Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. ","The authors provided a detailed justification for their answer, confirming that they have included necessary training and test details such as data splits, hyperparameters, and the types of optimizers used in the paper. Additionally, they mentioned that the methodology for hyperparameter sampling and specifics about the training and testing setups are explained in sections and appendices, ensuring a comprehensive understanding of the experimental results.

While the response is satisfactory, ensuring comprehensive detail that facilitates reproducibility, it's important to critically evaluate whether these details indeed meet the standard required for full reproducibility and clarity. Based on the extensive excerpts and descriptions provided in the overview of the paper, it appears that the authors have made considerable efforts to detail their experimental procedures, hyperparameter choices, and model configurations. This includes discussions on the datasets used, problem settings, design and understanding of architectures, and the specific configurations of their proposed models, along with appendices that delve deeper into these aspects.

Constructive and actionable suggestions for further improvement could include:
- Ensure that the versioning of datasets and any preprocessing steps are clearly documented, to avoid any ambiguity in data handling.
- While the paper mentions the general approach to hyperparameter selection and optimization setups, including the ranking and distribution strategies used, providing direct links to code repositories can greatly enhance reproducibility. It's noted that the authors have made their code available, which is commendable.
- Clarify any dataset-specific tuning or model adjustments made for different experiments, as this could impact the generalizability of the findings across different spatio-temporal forecasting tasks.
- For the hyperparameter study, it might be helpful to include a sensitivity analysis or discussions on the robustness of model performance with respect to varying hyperparameters, which would benefit readers looking to apply these models in different contexts.

Overall, the paper seems to satisfy the requirements specified in the question and guidelines, with the aforementioned suggestions potentially enhancing clarity and reproducibility further.",1
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Yes,Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.,"• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ","The authors have adhered to the guidelines by including variance as error bars in the main experiments sections, indicating the statistical significance of their results. By incorporating mean and variance calculations across multiple runs, they provide a clear understanding of the variability and reliability of their findings. This practice is crucial for evaluating the robustness of the presented models and methods, allowing readers and other researchers to assess the confidence level in the experimental results critically.

However, while the authors have fulfilled the basic requirements regarding the reporting of statistical significance, there are areas where they could enhance the transparency and depth of their statistical analysis:

1. **Clarification of Variability Sources**: The authors should explicitly state the sources of variability captured by the error bars (e.g., initialization, data splits, or hyperparameter settings). This additional detail will help readers understand what factors might influence the variability in performance.

2. **Methodology for Error Calculation**: It would be beneficial for the authors to describe more explicitly the methodology used to calculate the error bars. Whether they are employing a specific statistical method, utilizing a library function, or conducting a bootstrap analysis, detailing this process would improve the paper's rigor.

3. **Error Bar Representation**: The authors should specify whether the error bars represent standard deviations or standard errors of the mean. This distinction is crucial for correctly interpreting the results' stability and reliability.

4. **Assumptions and Confidence Intervals**: If possible, the authors should discuss any assumptions underlying their statistical analyses, such as the normality of errors. Additionally, considering the exploration of confidence intervals, specifically whether they opted for 1-sigma or 2-sigma error bars, and the rationale behind this choice, would contribute to a more nuanced understanding of their statistical significance assessments.

Overall, the submission effectively meets the requirements for reporting statistical significance, with room for enhanced clarity and depth in their presentation of statistical analyses. Including these suggested details would bolster the paper's transparency and allow for a more informed evaluation of the presented research findings.",1
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Yes,"In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). Code of Ethics","Justification: While the paper mentions the GPU hours required for generating reasonable answers in comparison to multiple baselines, detailed information about the specific computer resources used (e.g., type of GPUs, memory, and storage) is not provided. Furthermore, there is a lack of detailed explanation regarding the execution time for each individual experimental setup, which is crucial for reproducibility. Without specific details on the computational environment, including CPU/GPU types, memory, storage, and the cloud provider or internal cluster information, it is challenging for other researchers to precisely replicate the experiments and validate the results presented in the paper. Additionally, the paper does not discuss if the full research project required additional computational resources beyond what was reported for the experiments.

Constructive and Actionable Suggestions:
1. Add a dedicated section or append supplementary material detailing the computational environment used for the experiments. This should include information about the type of GPUs, CPU specifications, the amount of RAM and storage, and whether the experiments were conducted on an internal cluster or through a cloud provider.
2. Provide a breakdown of execution time for each experimental run, outlining any variations in computational requirements for different experiments or models compared.
3. Discuss any preliminary or failed experiments that required computational resources but were not included in the final paper. This transparency helps in understanding the full extent of computational resources necessary for the project.
4. If possible, provide an estimate of the cost of experiments, especially if cloud computing resources were used. This information can help researchers with limited budgets to plan their experiments accordingly.

By addressing these suggestions, the authors can significantly enhance the reproducibility of their experiments and offer valuable insights into the computational demands of their proposed methods.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",Yes,We review the code of ethics and our paper conform with it.,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ","The authors affirm that their research conforms to the NeurIPS Code of Ethics. Based on the detailed review of the paper's content, there are no apparent violations of ethical guidelines. The paper presents a technical advancement in the field of graph neural networks, specifically focusing on spatio-temporal forecasting in traffic prediction, and doesn't involve human subjects, unethical data usage, or other sensitive ethical issues. The provided research appears to be in line with ethical standards concerning technical research and development in machine learning and artificial intelligence.

However, as with any affirmation of ethical compliance, this is taken at face value based on the information provided in the paper and the authors' declaration. It's worth noting that a more thorough investigation into all aspects of the research, such as data sourcing, model deployment implications, and broader impacts, would be required for a comprehensive ethical review.

Constructive and Actionable Suggestions for Authors Regarding Ethics:

1. **Data Usage Transparency**: Ensure full transparency about the data sources, especially because traffic prediction models could potentially use data with privacy implications. Clearly articulate data anonymization and handling procedures if applicable.

2. **Model Deployment Considerations**: Discuss any potential negative impacts this model could have if deployed in the real world. For example, could optimized traffic predictions lead to undesirable consequences such as environmental impacts or biases in resource allocation? Consider adding a section that discusses these aspects.

3. **Broader Impact Statement**: While the paper briefly touches on the broader impacts, expanding this section to include detailed analysis of both positive and negative implications would strengthen the ethical review of the work. Specifically, discuss long-term implications of NAS in traffic prediction systems and their societal, environmental, and economic impacts.

4. **Code and Data Accessibility**: Ensure that the code and any datasets used are made publicly available in a manner that respects any copyright or privacy concerns. This accessibility supports ethical standards by promoting reproducibility and transparency.

5. **Ethical AI Principles**: Briefly discuss how the developed model aligns with broader ethical AI principles, such as fairness, accountability, and transparency. For instance, how might the model foster equitable outcomes in traffic management?

Addressing these suggestions would not only bolster the ethical rigor of the paper but also contribute to a more holistic understanding of the research's implications in the broader context of AI and society.",1
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Yes,"We discuss the broader impacts in the section ""Broader Impact Statement"".","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ","Justification for Score:
The paper briefly mentions a section titled ""Broader Impact Statement,"" suggesting that it discusses the societal impacts of the work performed. However, the provided content does not offer any detailed information regarding either positive or negative societal impacts, nor does it suggest any mitigation strategies for potential negative impacts. The statement ""Our work does not contain potential harms to people, environment, society or any possible negative impacts listed on the NeurIPS Guidelines"" lacks critical reflection on the broader implications of the research.

Constructive Suggestions:
1. **Elaborate on Positive Impacts**: Clearly articulate the potential positive societal impacts of your work. For example, improvements in traffic prediction models can significantly enhance urban planning, reduce congestion, and contribute to smarter transportation systems. Detailing these can provide a clearer understanding of the value of your research.

2. **Consider and Discuss Potential Negative Impacts**: Reflect on how the advancements or applications stemming from your research might lead to unintended consequences. For instance, could the deployment of highly accurate traffic prediction models lead to privacy concerns? Could they be misused in ways that could harm certain communities or individuals?

3. **Mitigation Strategies**: If there are potential negative impacts, it is crucial to discuss possible strategies to mitigate these. This could include technical measures, ethical guidelines for use, or recommendations for policymakers.

4. **Expand the ""Broader Impact Statement""**: Provide a more detailed and thoughtful exploration of the societal implications of your work in the designated section. Discuss both positive impacts and potential risks, even if your work is foundational and seems removed from direct applications.

5. **Engage with Ethical Considerations**: Encourage a discussion on ethical considerations related to the deployment of your research findings. This includes considering fairness, accountability, and transparency in the use of Spatio-Temporal Graph Neural Networks for traffic prediction and other applications.

6. **Consider Future Work**: In discussing societal impacts, also consider future directions of your research and how they might address or ameliorate potential negative impacts identified.",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Not Applicable,This paper poses no such risks.,"• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. Licenses for Existing Assets","The justification provided by the authors in saying ""NA"" because the paper poses no such risks is consistent with the paper's content and focus. The paper is based on refining and simplifying the architecture search process in Spatio-Temporal Graph Neural Networks (STGNNs), particularly for traffic prediction scenarios. The subject matter inherently does not involve the types of data or models that typically raise concerns about misuse or require special safeguards for responsible release, as outlined in the guidelines.

However, it's still good to critically assess all aspects of the paper, including the potential future applications of the researched models and methods. The concern about misuse of AI models generally pertains to their ability to generate false or misleading information, privacy issues, or enabling surveillance capabilities beyond intended ethical boundaries. The application area of traffic prediction is less likely to directly engage with these concerns, especially given that the data used and models developed are tightly focused on forecasting traffic patterns.

Constructive and actionable suggestions:

1. **Data Privacy and Consent**: If not already done, the authors should ensure that any datasets used, including traffic data, comply with privacy norms and that there's an explicit mention of consent or anonymization where applicable. This is important even in seemingly benign contexts like traffic data, where individual patterns might inadvertently be trackable.

2. **Ethical Considerations**: A brief section discussing the ethical use of the developed models and the potential societal impact of more accurate traffic prediction could enrich the paper. This includes the benefits of improved traffic management and potential drawbacks, such as misapplication of the technology.

3. **Future Misuse Potential**: Although the current application may not raise immediate concerns, any technology can be repurposed. A forward-looking commentary about ensuring the ethical use of the technology as it evolves and perhaps intersects with other AI capabilities could be valuable.

4. **Licensing and Open-source Release**: If the authors plan to release their code or models, clear licensing that outlines ethical use guidelines can be helpful. It may not prevent misuse but creates a legal and moral framework encouraging responsible use.

5. **Community Engagement**: Encourage ongoing dialogue within the research community about the responsibilities of AI researchers in different applications, including those seemingly distant from controversial uses. It helps foster a culture of proactive consideration of potential future implications.",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Yes,"We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. ","The authors have properly credited the original creators of the datasets used in their work by explicitly mentioning and citing the sources [13, 1, 10]. Furthermore, they have provided a URL for accessing the datasets, thus facilitating reproducibility and transparency. However, the response does not explicitly mention the license under which these datasets are distributed or the terms of use, which would have been ideal for fully satisfying the guidelines' requirement regarding explicitly stating the license and terms of use.

Constructive suggestions for improvement:
1. **License Information**: It would be beneficial for the authors to include the specific license under which each dataset is distributed. If the datasets are publicly available on GitHub without explicit licensing information, stating this fact could be useful for readers and future researchers who might wish to use these datasets.
2. **Terms of Use**: If there are specific terms of use associated with the datasets (e.g., limitations on commercial use, required attribution, etc.), mentioning these would provide clarity to readers and help ensure compliance with the original creators' conditions.
3. **Version and URL**: While a URL is provided, the authors might consider specifying the version of the dataset used, if applicable. This detail enhances reproducibility, especially if the dataset undergoes updates or changes over time.

In summary, the authors have met the basic expectations by citing the sources and providing a link to the datasets. For complete compliance with the guidelines and to enhance transparency, elaborating on the license and terms of use would be recommended in future work.",1
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Not Applicable,The paper does not release new assets.,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ","Justification provided in the review (NA for not introducing new assets) seems inconsistent with the content documented in the paper. The paper explicitly states that their code is available on a public GitHub repository, as seen in the introduction section where it mentions ""Our code is available at https://github.com/AutoML-Research/SimpleSTG."" This indicates that new assets, such as code or models potentially constituting a significant contribution to the field, have indeed been introduced and made available alongside the paper. Therefore, the paper should have been assessed based on the framework for new assets documentation, including but not limited to the quality of the documentation, accessibility, usability, and licensing of the provided code or model.

Constructive and Actionable Suggestions:
1. Assess the GitHub repository linked in the paper for completeness and clarity. Ensure that it contains comprehensive documentation detailing the setup, dependencies, how to run the code, expected outputs, and any other relevant information that facilitates replication and understanding of the research work.
2. Evaluate the license under which the code/model is released. It should promote open access and be clear on the permissions granted to the users, such as for modification, distribution, and commercial use.
3. Encourage the authors to elaborate on any limitations, prerequisites, and expected environments/settings for running their released code, ensuring a wider accessibility and usability by the research community.
4. Suggest that the authors include a discussion on the potential ethically-relevant aspects or impacts of deploying their models, especially if their research is directly applicable to real-world scenarios.
5. Finally, prompt the authors to provide a clear versioning of their repository to ensure traceability of changes in relation to the paper’s findings and claimed contributions.",0
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ","The answer provided by the authors as ""NA"" is correct and consistent with the content of the reviewed paper, which focuses on improving the efficiency and effectiveness of architecture search in Spatio-Temporal Graph Neural Networks for tasks like traffic prediction. The paper primarily deals with algorithmic advancements and does not involve human subjects or crowdsourcing experiments. Therefore, the requirement for including the full text of instructions given to participants, screenshots, or details about compensation does not apply to this paper.

Considering the nature of the work, which is computational and algorithmic, the authors are not expected to include such details. Their focus on proposing a new framework, understanding training hyperparameters, and conducting comprehensive experiments to demonstrate the efficacy of their proposed methods aligns with the scope of the NeurIPS conference and does not involve direct human subject research.

No suggestions for improvement regarding this specific point are necessary.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The answer provided by the authors is consistent with the content and guidelines. The paper is focused on developing and improving Spatio-Temporal Graph Neural Networks (STGNNs) for traffic prediction, and there is no mention or indication of involving human subjects or crowdsourcing data in their research. Consequently, the concerns related to potential risks to study participants, the disclosure of such risks, and the requirement of Institutional Review Board (IRB) approvals do not apply to this study. The justification provided aligns with the scope and nature of the research, as well as follows the guidelines regarding the exclusion of IRB-related disclosures in studies that do not involve human subjects research.

However, since actionable and constructive suggestions were requested, it's worth noting that the paper is already within the appropriate boundary concerning this aspect. Should future work or extensions of this research involve human subjects (for instance, in evaluating the user experience of a traffic prediction system), the authors would need to seek IRB approval or its equivalent and ensure ethical considerations are discussed.

To maintain completeness in addressing the question, no further action is required from the authors regarding this specific query about IRB approvals and potential risks to study participants.",1
