Question,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","The answer provided matches the content and contributions described in the paper. The paper introduces Meta-Album, a large and diverse meta-dataset aimed at facilitating research in various machine learning tasks, including few-shot learning, meta-learning, transfer learning, and benchmarking cross-domain generalization capabilities. Both the abstract and introduction sections clearly outline these contributions, accurately reflecting the paper's scope and the significance of its contributions to the field. Additionally, the paper outlines the structure of Meta-Album, including the number of datasets, image classes, and versions available, which is indeed a critical component of the contributions mentioned.

However, to further improve the answer and provide a more critical yet constructive review, one might consider the following suggestions:

1. While the claim regarding the contributions is accurately reflected, it would be beneficial to have a more detailed explanation in the review about how these contributions are innovative or advance the current state of research in few-shot learning and meta-learning. Specifically, the unique aspects of Meta-Album compared to existing datasets and benchmarks could be highlighted.

2. The review could also address the potential impact of Meta-Album on future research, including how it might enable new types of experiments or methodologies that were not previously feasible due to data limitations.

3. It would be useful to discuss any limitations or assumptions explicitly mentioned in the introduction and abstract sections. While the answer suggests that the claims accurately reflect the paper's contributions and scope, any critical examination of how these claims are justified or any possible gaps in the justification would add depth to the review.

4. Lastly, suggestions for future work or areas that the paper could potentially expand upon (as often hinted at in introductions or abstracts) could provide constructive feedback for the authors. This includes any aspirational goals not fully attained in the paper, as per the guidelines.

In summary, while the answer correctly agrees with the question based on the paper's content, incorporating these suggestions could lead to a more thorough and constructive review that not only acknowledges the paper's contributions but also provides valuable insights for further improvement and exploration.",1
Does the paper discuss the limitations of the work performed by the authors?,Yes,We have clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","The authors have adequately addressed the question concerning the discussion of the limitations of their work. In the ""Discussion and Conclusion"" section, the authors have indeed outlined various limitations, which indicates a thorough self-assessment of their research comprehensively. They discuss aspects like biases introduced in datasets, the need for quality control, challenges in de novo training, and issues that may arise from their methodology and dataset curation. This reflection is critical for transparent and honest research, showcasing a depth of understanding of their work's impact and scope.

Constructive suggestions for further enhancement could include:
1. **More Detailed Elaboration on Limitations**: While the authors have mentioned limitations, expanding on how these limitations could specifically impact the results or future applications of their work could be beneficial. For example, detailing how biases identified in datasets might skew the performance or application of their meta-dataset in specific scenarios could offer deeper insights.
   
2. **Addressing Computational Efficiency**: The authors should consider including a discussion regarding the computational resources required to utilize their dataset effectively. Given that their meta-dataset facilitates a wide range of tasks, understanding computational demands can help researchers align their resources appropriately.

3. **Generalization Concerns**: Providing more insight into how well they expect their findings or dataset to generalize across different conditions or unseen domains would be valuable. This could include discussing the diversity of the datasets included and any limitations in representation.

4. **Scalability Analysis**: An overview of how their meta-dataset scales with the addition of new domains or datasets could help in understanding the longevity and maintenance challenges of such a project.

5. **Solutions and Future Directions**: Beyond identifying limitations, proposing potential solutions or directions for future research to address these limitations can be incredibly helpful. This could involve suggesting methodological improvements, alternative data processing techniques, or new research questions that emerge from these limitations.

Acknowledging and discussing these limitations transparently not only strengthens the current paper but also guides future work in the field more effectively.",1
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",N/A,This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The answer provided by the authors is correct based on the content of the paper. The primary focus of the paper is the creation of a meta-dataset named Meta-Album and the execution of various experiments utilizing this dataset. It is an empirical research work rather than theoretical, involving the compilation and preprocessing of data across multiple domains to facilitate few-shot learning, meta-learning, transfer learning, and similar tasks. Therefore, it does not deal with theoretical results such as theorems or mathematical proofs, which renders the question of providing a complete set of assumptions and proofs inapplicable.

Constructive and Actionable Suggestions:
1. Although the question about theoretical proofs does not apply, it might be helpful to include a brief explicit statement in the paper's methodology section clarifying that the work is empirical and focuses on dataset creation and experimental validation. This can help set clear expectations for the reader from the outset.

2. Consider discussing any theoretical considerations or principles that guided the dataset creation process or the choice of experiments, even if theoretical proofs are not part of the work. This could provide additional depth to the reader's understanding of the project's foundations.

3. For future work, if any theoretical models or algorithms are developed or applied in relation to the Meta-Album dataset, ensure to include detailed assumptions and complete proofs or provide appropriate references if the proofs are commonly known.

4. Engage with the broader context of your work by discussing how it fits into the current landscape of few-shot learning and meta-learning research. This could include theoretical challenges that the field faces and how empirical datasets like Meta-Album can contribute to addressing these challenges.

These suggestions aim to enhance the clarity and depth of the paper, ensuring that readers fully appreciate the empirical nature of the work and its contributions to the field of few-shot learning and meta-learning.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","Justification:
The answer provided by the authors suggests that they have made an effort to ensure reproducibility by releasing the code on their GitHub repository and detailing how to access the datasets on their website. However, the justification does not fully address the question regarding the disclosure of all information needed to reproduce the main experimental results. Specifically, it lacks details about the environment setup, exact commands for reproducing results, data preparation steps, and whether all experimental results or only a subset are reproducible. Without these details, reproducing the experiments to validate the main claims and/or conclusions of the paper could be challenging for other researchers.

Constructive Suggestions:
1. **Environment Specification**: Provide a detailed description of the computational environment used for the experiments, including the specific versions of the programming languages, libraries, and any other software or tools required.

2. **Exact Commands**: Include the exact commands that need to be run to reproduce the main experimental results. This should cover model training, evaluation, and any preprocessing steps. 

3. **Data Preparation**: Elaborate on the process of data access and preparation. This includes information on how to access raw data, any preprocessing steps required to transform the data into a usable format for the experiments, and how to generate any intermediate datasets used in the study.

4. **Reproducibility Scope**: Clearly state which parts of the experiments are reproducible with the provided code and data. If there are any limitations or if only a subset of experiments can be reproduced, explain why and provide guidance on how to replicate those specific results.

5. **Supplemental Material**: Consider adding more detailed information in the supplemental material appended to the paper. This could include configuration files, detailed setup instructions, and additional notes on troubleshooting common issues during experimentation.

6. **Anonymized Release**: Ensure that any code or data released for review is appropriately anonymized in line with conference guidelines. After the review period, consider updating the repositories with non-anonymized versions or additional resources to aid in reproducibility.

By addressing these points, the authors can significantly enhance the reproducibility of their work, making it easier for other researchers to validate their claims and build upon their research findings.",0
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The provided answer aligns well with the content of the paper, which emphasizes the open-access nature of the data and code. The authors have made the Meta-Album meta-dataset publicly available, including both the data and the code necessary for reproducing the main experimental results. The website URL and GitHub repository are indicated, which is essential for ensuring transparency, reproducibility, and facilitating further research by the community.

However, while the answer indicates that the necessary resources are publicly provided and implies that instructions are available, it does not explicitly mention the thoroughness of the provided instructions. For full reproducibility, detailed instructions, including the exact commands and environment settings needed, should be readily accessible. As per the guidelines, this is crucial for enabling other researchers to faithfully replicate the experimental results.

**Constructive and Actionable Suggestions:**
1. **Explicit Instructions:** Ensure that the provided GitHub repository and the website contain explicit and detailed instructions for faithfully reproducing the main experimental results. This should include instructions on data access and preparation, environment setup, and the exact commands to run the experiments.

2. **Clarify Reproducible Experiments:** Clearly state which experiments from the paper can be reproduced with the provided scripts. If any experiments are not included in the reproducibility scripts, explain why and provide sufficient detail for those interested in replicating those specific results.

3. **Environment Details:** Provide a requirements file (e.g., `requirements.txt` for Python environments) or a Docker container to facilitate the setup of the exact software environment used in the experiments. This will help mitigate issues related to software versions and dependencies.

4. **Demonstrate Reproducibility:** It might be beneficial to include a section in the README or documentation that shows examples of output from running the provided scripts, demonstrating that the expected results can be obtained.

5. **Ongoing Support:** Consider establishing a process for addressing issues or questions from those trying to replicate the results. A simple way to do this could be to enable issues on the GitHub repository and monitor them for questions related to reproducibility.

These suggestions aim to enhance the reproducibility of the paper's results and foster an environment where researchers can easily build upon the work presented.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Yes,We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","Justification for Score:
The answer provided by the authors indicates that the experimental settings or details are explained in Section 3.2 and that additional details can be found in the Meta-Album GitHub repository. However, the justification for selecting specific hyperparameters, data splits, and the type of optimizer used for the experiments is not explicitly detailed within the provided content. While it is helpful that the paper directs readers to the GitHub repository for more information, the core paper should contain sufficient details to understand the results without necessitating external resources for basic experimental setup information.

Constructive and actionable suggestions:
1. Provide a more detailed explanation in the paper regarding the choice of hyperparameters and their impact on the model's performance. This could include reasons for selecting specific values and any hyperparameter tuning processes undertaken.

2. Include a detailed description of how data splits were decided. For example, the rationale behind the proportions for training, validation, and testing sets, and whether any stratification was used to ensure balanced splits.

3. Specify the type of optimizer used and why it was chosen. Include details such as learning rate, decay schedules if any, and other relevant optimizer settings.

4. If space constraints within the paper limit the amount of detail that can be provided, consider adding an appendix or supplementary material that addresses the points above. Ensure this supplementary information is easily accessible and clearly referenced in the main paper.

5. To enhance reproducibility and understanding, ensure that the GitHub repository not only includes code but also a comprehensive README file that details the experimental settings, setup instructions, and how to replicate the results presented in the paper.

By addressing these suggestions, the paper can improve its clarity regarding experimental settings and enhance the reproducibility of the results.",0
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Yes,"In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","The answer provided by the authors is consistent with the content of the paper. Throughout the paper, particularly in figure-2, figure-3, and Appendices E and F, error bars and statistical significance are reported correctly and are suitably defined. This aligns well with the guidelines provided for answering this question.

Justification for agreement:
- The paper presents a significant amount of experimental results across various setups, including within-domain and cross-domain few-shot learning, and reports error bars for these experiments.
- The authors clearly state in the justification that error bars are reported in specific figures and appendices, implying that the variability factors (e.g., train/test split, initialization) and the method of calculation were considered in their analysis, even if not detailed explicitly in the provided answer.
- Although the detailed methodology for calculating error bars (e.g., bootstrap or other statistical methods) is not elaborated in the justification, the mention of their presence in figures and appendices suggests adherence to statistical reporting practices.

Constructive and actionable suggestions for improvement:
1. Include a brief description of how the error bars were calculated directly in the main text where results are initially discussed, to make it easier for readers to understand the statistical methodology without needing to infer or search for details.
2. Clarify in the main document or in the appendices the assumptions made for error calculations such as the distribution of errors, whether the errors represent standard deviation or standard error of the mean, and if the reported error bars correspond to a specific confidence interval (e.g., 95% CI).
3. For any future work or extensions of this paper, consider reporting the details of the statistical tests used for assessing the significance of the findings where relevant, especially if comparisons are made between different methods or setups.
4. Wherever possible, elaborate on the factors contributing to variability within the experiments directly in the captions of figures or tables where error bars are shown, in order to provide immediate context to the readers.

By addressing these suggestions, the authors could enhance the clarity and robustness of their statistical reporting, further strengthening the credibility of their experimental findings.",1
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Yes,"We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","The response from the authors accurately reflects the content of the paper concerning the provision of information on computer resources needed for experiment reproduction. In the referenced sections (Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning), the paper details the computational resources used, including the types of GPUs (e.g., PNY GeForce RTX 2080TI GPUs with 11GB of VRAM or a single NVIDIA V100 with 16GB of VRAM), and provides an estimate of the time each experimental run took (e.g., at most 24 hours on the former GPU). This description follows the guidelines set for this question, indicating that the paper indeed provides adequate details on computation resources for replicating the experiments.

Actionable suggestions for further improvement:
1. **Clearly State Execution Times and Compute Types in the Main Text**: While Appendix E and F do describe compute resources, ensuring a brief mention or summary in the main text helps improve readability and accessibility of the information.
2. **Detailed Breakdown of Computations**: Provide a more detailed breakdown of compute resources for each type of experiment. This includes not just the type of GPU and the execution time but also information about CPU specifications, memory requirements, and the specific configurations used to optimize performance.
3. **Discuss Computational Efficiency**: Beyond indicating what resources were used, discuss the computational efficiency of different algorithms and potential optimizations for researchers with limited access to high-end GPUs.
4. **Include Cloud Compute Information if Applicable**: If any part of the experiments was conducted using cloud compute resources, specify the type of instances used and any relevant details about the cost or configuration that could help in replication.
5. **Mention Failures and Preliminary Experiments**: In line with the guidelines, if significant compute was dedicated to preliminary or failed experiments not detailed in the paper, providing a brief mention or discussion could offer a more comprehensive view of the computational demands of the research project.",1
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",Yes,We completely comply with the NeurIPS Code of Ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","The provided answer and justification by the authors stating compliance with the NeurIPS Code of Ethics align with standard expectations for research submissions. The authors explicitly confirm their compliance without detailing any exceptions, which directly addresses the review question regarding adherence to ethical guidelines.

However, to enhance the rigor of the review process and uphold the integrity of the conference's ethical standards, it is pivotal to consider a few critical areas where ethical considerations are paramount, especially for submissions involving extensive data collection, processing, and potential implications on privacy and fairness. While the authors' response is affirmative, a more detailed justification would have been beneficial for complete transparency and understanding.

Constructive Suggestions:
1. Data Ethics and Privacy: Given the dataset's compilation encompasses various domains and sources, clarify any consent obtained for using these datasets, especially concerning data that might entail personal or sensitive information. Detailing anonymization or de-identification processes applied, if any, would strengthen ethical compliance.

2. Fairness and Bias Consideration: Given Meta-Album's intended use in fostering advancements in few-shot learning, expressly discuss measures undertaken to ensure the dataset's diversity and representation. Address any steps taken to mitigate bias in dataset curation and algorithm development, reflecting commitment towards fairness in machine learning applications.

3. Impact Assessment: Provide a brief discussion on the potential societal impact of deploying systems trained with Meta-Album, acknowledging both positive contributions and possible negative outcomes. Highlighting awareness and mitigation strategies for unintended consequences would demonstrate a proactive approach to ethical considerations.

4. Acknowledgment of Limitations: Including a section discussing the ethical limitations of your work, such as possible exclusivity in languages, cultures, or demographics represented in the dataset, would reflect a thorough ethical evaluation.

In conclusion, while the authors confirm their compliance with the NeurIPS Code of Ethics, further elaboration on ethical considerations specific to their dataset creation and usage could enhance the submission’s integrity. Implementing these suggestions would not only strengthen the paper's ethical grounding but also contribute to fostering responsible AI research practices within the community.",1
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Yes,"There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","The justification provided by the authors for not discussing negative societal impacts is insufficient and does not thoroughly explore potential risks associated with their work. While it's acknowledged that the main focus of the paper is on fostering progress in fields of few-shot learning and meta-learning, it's critical to consider that any technological advancement, including datasets and algorithms for machine learning, can have unintended consequences or be misused in ways that might harm society.

Constructive Suggestions:
1. **Broader Impact Discussion**: The authors should include a section that thoughtfully considers potential negative societal impacts. This discussion should not only highlight the intended positive impacts but also deeply consider how the dataset could be misused or how biases in the dataset could perpetuate or exacerbate existing inequalities.
   
2. **Ethical Considerations**: Include a subsection that addresses ethical considerations related to the collection, use, and distribution of the datasets included in Meta-Album. Given the diverse domains covered by the datasets, it is crucial to discuss any privacy, consent, and copyright issues associated with using these images.

3. **Bias and Fairness**: Although the primary intention of Meta-Album is constructive, the authors should analyze potential biases within the datasets and how these could impact the fairness of models trained on them. This is particularly important since biases in training data can lead to biased algorithms, which could have harmful societal impacts, especially when deployed in real-world applications.

4. **Mitigation Strategies**: If any potential negative impacts are identified, the authors should propose mitigation strategies to prevent misuse or reduce harm. This could include recommendations for dataset users on conducting bias audits, ethical guidelines for usage, or technical solutions to minimize risks.

5. **Future Work**: The paper should suggest areas for future research that specifically focus on addressing any identified societal risks or exploring ways to enhance the positive impacts of Meta-Album.

By expanding the discussion on societal impacts and addressing these recommendations, the paper would not only meet ethical research standards but also contribute to responsible AI development.",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Yes,"Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","The answer provided by the authors is consistent with the content and intentions described in the paper regarding the responsible release of data. They have specifically addressed safeguards by mentioning the inclusion of licenses and terms of use for their datasets. This ensures a level of control and guidance on how the datasets can be responsibly used, in alignment with the guidelines for responsible data release. 

However, to further strengthen the paper's commitment to ethical considerations and responsible data use, the authors might consider expanding on the following points in their justification or in the main content of the paper:

1. **Detail on License Types:** Expanding on what types of licenses are used for each dataset and the implications of these licenses on the use of data would provide clearer guidance to potential users. Different licenses have different restrictions and freedoms, and clarifying these can help ensure the data is used responsibly.

2. **Data Privacy and Anonymization:** If applicable, especially for datasets involving human subjects or potentially sensitive information, the authors should detail any measures taken to protect privacy, such as anonymization processes or consent protocols.

3. **Use Case Limitations:** While the authors mention recommended usage, specifically delineating appropriate and inappropriate uses of the data/models can provide clearer ethical guidelines to potential users.

4. **Monitoring and Enforcement Strategy:** Discussing how the terms of use will be monitored and what enforcement mechanisms are in place for misuse would strengthen the safeguarding strategy.

5. **Engagement with Ethical Review:** If there was any ethical review process (e.g., by an institutional review board), mentioning this process and its outcomes can add a layer of assurance regarding the ethical consideration of the data/model release.

These suggestions aim to support the authors' intentions for responsible release and use of their datasets while providing a more comprehensive framework for ethical considerations in line with the provided guidelines.",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Yes,"We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","The answer provided by the authors is consistent with the content of the paper and the guidelines provided. The authors explicitly mention the release of their datasets with proper licensing and make specific reference to including the original licenses for the datasets utilized in Appendix B. This approach fully respects and credits the creators or original owners of the assets (datasets) used in the study, fulfilling the requirement to properly acknowledge and respect licenses and terms of use.

However, to ensure complete clarity and adherence to best practices, I recommend the following actionable suggestions:

1. **Explicit Mention in the Main Body**: While Appendix B details the licenses, it will enhance visibility and accessibility if a brief mention or acknowledgment of these licenses also occurs within the main body of the paper, especially in sections where these datasets are introduced or discussed extensively.

2. **Direct URLs to Licenses**: If possible, include direct URLs or links to the specific licenses or terms of use pages for each dataset within Appendix B. This practice ensures that readers and future researchers can easily verify the license details and terms directly.

3. **Clarification on Any Derived Work**: If any of the datasets have been modified or extended (which seems to be part of the objective with Meta-Album), it's crucial to also mention the license under which these modifications are released. This clarification helps in understanding the permissions and restrictions applied to the derivative works.

4. **Guidance for Future Contributors**: Since the Meta-Album is designed to be extendable with contributions from the community, providing clear guidelines on how to contribute datasets, including requirements for licensing and credit to original creators, will ensure consistency and respect for copyright as the dataset grows.

5. **Update on Asset Release**: Ensure that any code, models, or additional assets created as part of this study are also released under clear licenses, and these licenses are properly mentioned both in the paper and the repository where the assets are hosted.

Acknowledging the inclusion and proper reference to licensing in Appendix B and suggesting improvements chiefly aim at enhancing transparency, accessibility, and compliance with copyright laws, thus fostering a respectful and legally aware research environment.",1
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Yes,We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","The answer provided by the authors is consistent with the content of the paper and the guidelines for documenting new assets. The authors have mentioned the use of meta-data files and datasheets for datasets, which are included in the appendices. Additionally, they have developed a website that allows users to easily access all datasets and related information. This demonstrates a clear effort to document the new assets introduced in the paper properly.

However, to further strengthen the documentation and user understanding of the new assets, the authors may consider the following suggestions:
1. **Detailed Description**: Enhance the meta-data files and datasheets with more detailed descriptions of each dataset, including the specific challenges they pose, the context in which they were collected, and any preprocessing steps applied.
2. **License Information**: Ensure detailed license information for each dataset is clearly provided, making it straightforward for potential users to understand how they can legally use the datasets.
3. **Limitations Section**: Consider adding a limitations section in the documentation, where any known biases, imbalances, or other limitations of the datasets are discussed. This transparency can help researchers to properly interpret results obtained using these datasets.
4. **Consent and Ethics**: Provide information on how consent was obtained for the data used, especially if it involves human subjects, and discuss any ethical considerations taken into account during data collection and preparation.
5. **Anonymization**: If applicable, explain the steps taken for data anonymization to protect privacy, especially for datasets that may include personal or sensitive information.
6. **Tutorial or Usage Examples**: Offer tutorials or usage examples on the website, guiding researchers on how to effectively use the datasets for their specific research purposes.

These additions would not only adhere to the guidelines provided but also enhance the clarity, completeness, and accessibility of the documentation for the new assets introduced in the paper.",1
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",N/A,This does not apply on our research as we are not doing any crowdsourcing experiments,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","The answer given by the authors is consistent with the content of the paper. From the provided excerpts, the research presented focuses on the introduction of the Meta-Album, a machine learning meta-dataset designed for few-shot image classification, not involving crowdsourcing experiments or research with human subjects. Therefore, the authors' response that the question on human subject research and compensation does not apply to their work is accurate and justified.

However, to enhance the review:

1. **Clarify Methodology**: Even though this question does not apply, it's crucial for the paper to clearly state the methodology used for dataset gathering and preparation, emphasizing the absence of human subjects to avoid any ambiguity.
   
2. **Ethical Considerations**: If any part of dataset gathering indirectly involved human contribution (e.g., via existing datasets that used human annotation), it would be beneficial to briefly discuss the ethical considerations and confirm adherence to ethical standards.

3. **Provide Guidelines for Future Work**: For researchers who may use Meta-Album in applications involving human subjects, consider adding a brief section discussing potential ethical considerations or guidelines to ensure ethical standards are maintained.

These suggestions aim at ensuring the research's clarity regarding human subjects and ethics, even when not directly applicable, thereby reinforcing the paper's integrity and consideration for ethical research practices.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",N/A,This does not concerns us as we are not dealing with human subjects in our research.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The answer provided by the authors is consistent with the content and context of the paper, as described. The paper introduces Meta-Album, a meta-dataset designed to facilitate research in few-shot learning, meta-learning, and other related computational tasks. It focuses on image classification across various domains, utilizing preexisting datasets that have been carefully curated to ensure uniformity and facilitate research. Given the nature of the work, which revolves around computational methods and datasets rather than direct human subject research, there's no apparent requirement for IRB approval or a detailed discussion of risks to study participants.

Additionally, the work doesn't involve crowdsourcing or any form of direct interaction with human subjects that would necessitate ethical concerns related to privacy, consent, or psychological impact. The datasets compiled into Meta-Album are sourced from publicly available or previously collected image datasets, which have their own ethical considerations and permissions for use in academic research. 

However, to further enhance the review:

1. **Clarify Ethical Consideration for Dataset Use**: It would be beneficial for the authors to briefly clarify the ethical considerations involved in using these precompiled datasets. This includes confirming that the datasets were ethically sourced and that their use complies with the original licensing agreements and ethical guidelines.

2. **Dataset Contribution and Extensibility Ethical Guidelines**: Since the paper encourages community contributions to Meta-Album, it would be prudent to outline ethical guidelines or considerations for future contributors, ensuring that newly added datasets adhere to ethical standards regarding privacy and consent.

3. **Future Work Implications**: As the dataset will be used for future machine learning research, a brief discussion on the ethical implications of deploying models trained on Meta-Album in real-world applications could be insightful. This includes considerations about bias, fairness, and the general impact on society.

These suggestions aim to preemptively address any ethical concerns related to the dataset's content and its use in future research and applications.",1
