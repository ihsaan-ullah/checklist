Question,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","Justification:
The provided answer lacks a critical and detailed evaluation of the claims made in the abstract and introduction against the actual contributions and scope detailed throughout the paper. It does not specify which main contributions are accurately reflected or how the scope mentioned aligns with the paper's content. A more thorough analysis is required to justify agreement fully.

Constructive and actionable suggestions:
1. **Detail Accurate Reflections**: Clearly identify the specific claims made in the abstract and introduction. Then, correlate each of these claims to specific sections or findings in the paper that support these claims. This analysis would offer a more solid foundation for the given answer.
   
2. **Highlight Any Discrepancies or Overstatements**: If any claims in the abstract or introduction do not fully align with the actual contributions or findings described in the paper, these should be highlighted as areas for improvement. Clarify whether any claim seems overstated or not fully substantiated by the paper's results.

3. **Assess Scope versus Contributions**: Evaluate whether the scope mentioned in the abstract and introduction is fully explored and addressed within the paper. If the scope is too broad or not fully covered, suggest narrowing the scope or expanding the sections to match the stated scope.

4. **Suggestions for Improvement**: If any discrepancies are found between the claims and the actual contributions, recommend specific ways the authors can either adjust the claims to more accurately reflect the paper's contents or expand on their work to fulfill the initially stated contributions. 

5. **Comment on Generalization**: Critically assess the authors' comments on the expected generalization of their results to other settings, as mentioned in the guidelines. If the paper lacks such a discussion, suggest the authors include an evaluation of the potential generalization and limitations of their work.

By providing a detailed critique along with constructive feedback, the review will not only justify the score more effectively but also guide the authors on how to enhance the clarity and accuracy of their manuscript's abstract and introduction sections.",0
Does the paper discuss the limitations of the work performed by the authors?,No,We haven't clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","The justification provided by the authors for not discussing the limitations is inconsistent with the expectations outlined in the guidelines. It's crucial for papers, especially those introducing new datasets or methodologies like Meta-Album, to thoroughly discuss their limitations to ensure a comprehensive understanding of their work's boundaries, robustness, and potential areas for improvement.

The authors should consider adding a separate ""Limitations"" section or integrate discussions of the limitations into relevant sections of their paper. This could include, but not be limited to, considerations of:

1. **Data Scope:** Since Meta-Album is a multi-domain dataset, a discussion on whether all major domains of interest are covered or if there are notable omissions would be valuable.
2. **Bias and Representativeness:** Reflect on the potential biases in the data collection process (e.g., geographical, cultural, etc.) and how representative the datasets are of real-world scenarios.
3. **Scalability and Computational Efficiency:** Discuss the computational demands of working with the Meta-Album dataset, including processing and training models, particularly in the context of few-shot learning where dataset scalability can be a limitation.
4. **Generalization:** Consider the extent to which the findings and the utility of the Meta-Album dataset may generalize beyond the specific tasks or domains tested, including any limitations in the experimental design.
5. **Privacy and Fairness:** Address any potential concerns related to privacy, especially if any datasets contain personal information, and fairness, particularly in how well the dataset supports the development of models that perform equitably across diverse groups.
6. **Acknowledgment of Limits in Experimental Validation:** If the Meta-Album’s utility was validated on a limited number of tasks, models, or settings, this limitation should be acknowledged, and the need for further testing should be emphasized.

By addressing these areas, the authors can provide a more accurate and credible account of their work, contribute to the integrity of the research community, and guide future research directions effectively.",0
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Not Applicable,Our work is not theoretical so this does not apply on our research,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The authors stated that their work is not theoretical, so the question about providing the full set of assumptions and a complete (and correct) proof for each theoretical result does not apply to their research. This response is consistent with the content of the paper, which focuses on introducing Meta-Album, a multi-domain meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks. Therefore, the paper primarily pertains to dataset creation, benchmarking, and empirical evaluation, rather than theoretical analysis or results that would require formal proofs.

Constructive and Actionable Suggestions for Improvement:
1. While the NA response to this question is justified, the authors could enhance the clarity of the paper by briefly mentioning in the main text that the focus is on empirical contributions rather than theoretical advances. This would preemptively address similar questions from readers or reviewers unfamiliar with the paper's objectives.
2. If any future version of the work or related projects involves theoretical contributions, the authors should ensure to follow the guidelines by clearly stating all assumptions, providing numbered and cross-referenced theorems, lemmas, and proofs, and offering both formal proofs and proof sketches for intuition as appropriate.
3. Although not directly related to theoretical results, the authors might consider discussing the foundational principles or existing works that underpin the methodologies used in creating and analyzing the Meta-Album dataset. This could provide deeper insights into the design decisions and potential theoretical considerations relevant to dataset creation and evaluation in few-shot learning and related areas.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Yes,We have made our code and data publicly available on our github repository and on our website,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The justification provided by the authors for the question regarding the complete disclosure of all information needed to reproduce the main experimental results is insufficient and not aligned with the comprehensive guidelines provided. Simply stating that the code and data are available on a GitHub repository and a website does not ensure that all criteria for reproducibility are met, as outlined in the guidelines. 

Constructive and actionable suggestions for the authors to improve the reproducibility of their main experimental results:

1. **Detailed Instructions**: Provide detailed instructions within the paper or as part of the supplementary material on how to use the code and data to reproduce the experiments. This includes specific commands, environment configurations, and any dependencies.
2. **Access and Preparation of Data**: Clarify the process for accessing both the raw and preprocessed data. If there are intermediate steps required to prepare the data for experiments, these should be explicitly described or included in the form of scripts.
3. **Experimental Setup**: Ensure that the paper includes detailed information about the experimental setup, including hardware specifications and software versions used. This is crucial for ensuring that results can be replicated in a similar environment.
4. **Baseline Comparison**: If scripts are provided only for a subset of experiments or only for the proposed method and not for baselines, clearly state which experiments are omitted and provide a rationale. It’s important that readers can reproduce results for both the proposed method and the baselines to validate the claims made in the paper.
5. **Anonymity Preservation**: If the submission is still under review, make sure any external links to code and data are anonymized to preserve the double-blind review process. This can be done via an anonymous GitHub repository or a placeholder URL in the submission, which can be updated upon acceptance.
6. **Supplemental Material**: Consider providing additional supplemental material that supports reproducibility, such as configuration files, additional scripts, and extended results. This material can be appended to the paper for review purposes.

Addressing these points will significantly improve the reproducibility of the paper's experiments and ensure that the paper meets NeurIPS guidelines and standards for experimental reproducibility.",0
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Yes,We have made our code and data publicly available on our github repository and on our website,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The answer provided by the authors aligns with the paper content and the guidelines. They clearly state that both code and data are publicly accessible via their GitHub repository and website. This is a strong indication that they have met the requirements for open access to resources necessary for reproducing the main experimental results. However, this response would be even stronger with specific details about the comprehensiveness of the instructions provided for replication, which is a pivotal aspect of the review guidelines.

Justification Analysis:
The justification briefly mentions that the code and data are public, which is crucial. Nonetheless, for a thorough review, the justification should have confirmed that there are ""sufficient instructions to faithfully reproduce the main experimental results,"" as explicitly asked in the question. This includes ensuring that the environment setup, command lines for execution, and data access procedures are clearly outlined and accessible. Although ""Yes"" is a valid response based on the information provided, a more detailed examination against the guidelines reveals these additional considerations for thorough validation of reproducibility.

Constructive suggestions for the authors to enhance their submission:
1. **Detailed Reproduction Instructions:** Ensure that the repository includes a README file with detailed step-by-step instructions for setting up the environment, accessing and preparing the data, and running the experiments to reproduce the main results.

2. **Environment Details:** Provide a requirements.txt or an environment.yml file to help users recreate the exact software environment used for the experiments.

3. **Data Accessibility:** Clearly describe the process for accessing both raw and preprocessed data within the repository or supplemental material. If any data cannot be shared publicly due to privacy or licensing issues, explain these limitations and provide synthetic or sample data when possible.

4. **Script for Baseline Comparisons:** If your work includes comparisons to baseline methods, include scripts or notebooks that reproduce these comparisons. Clarify which experiments, if any, are omitted from automated reproduction scripts and why.

5. **Anonymization for Double-blind Review:** If applicable, ensure that the code and data repositories are set up in a manner that preserves anonymity during the review process.

6. **Supplemental Material:** Consider including more detailed information about the experimental setup, data preparation, and additional results in a supplemental material document appended to the paper.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",No,We have omitted many experimental details and hyper parameters,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","Answer and Justification Review:
The authors have explicitly acknowledged the omission of many experimental details and hyperparameters in their justification, which directly answers the question posed. This level of transparency about what the paper does and does not include regarding experimental setup is crucial for understanding the reproducibility and depth of the results presented. Hence, the answer provided aligns well with the content and the spirit of the question.

Constructive Suggestions:
To enhance the clarity and utility of the paper for readers and potential researchers who might wish to build upon this work, consider the following suggestions:
1. **Appendix or Supplementary Material:** Consider including an appendix or supplementary material that details the training and test setups, including data splits, hyperparameters, optimizer types, and rationale behind their choices. This would greatly aid in reproducing the results.
2. **Code Availability:** If possible, provide access to the source code, including training scripts and model configurations. This often serves as the best type of documentation and enables others to validate results and experiment further.
3. **Rationale for Choices:** Wherever specific choices have been made (e.g., type of optimizer, learning rate, etc.), briefly discuss the rationale behind these choices, especially if empirical tests influenced them.
4. **Baselines and Comparisons:** Ensure a clear distinction between the novel approaches proposed in the paper and established baselines. Detailed configurations for both can help in understanding the improvements brought about by the new methods.
5. **Versioning of Datasets and Code:** If datasets or code are made available externally, consider version controlling or providing a permanent DOI (Digital Object Identifier), which ensures that future readers can access the exact versions used in this study.

By addressing these points, the authors can substantially improve the reproducibility of their research and facilitate a clearer understanding of the experimental nuances that led to the reported results.",1
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,No,We haven't reported error bars for our experiments,"• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","The justification provided by the authors for not reporting error bars in their experiments is inconsistent with the guidelines for proper scientific reporting of experimental results. The absence of error bars, confidence intervals, or statistical significance tests makes it challenging to assess the reliability and the variability of the reported results. This omission is particularly critical in a paper presenting a new meta-dataset designed for few-shot learning, where it is essential to understand the statistical properties and robustness of the algorithms tested on the dataset.

Error bars and statistical significance tests are fundamental for comparing the performance of different methods and understanding whether observed differences are meaningful or could be attributed to random variations in the data or experimental setup. Without this information, the conclusions drawn from the experiments might not be fully supported by the evidence.

Constructive and actionable suggestions:
1. For future revisions or related research, the authors should include error bars or confidence intervals in their results. This could involve reporting the standard deviation, standard error, or a confidence interval around mean performance metrics across multiple runs of their experiments.
2. The authors should clarify the source of variability considered (e.g., different splits of the data, initialization of models, or other stochastic elements in the training process) when calculating these statistics.
3. It would be beneficial to explain the method used to calculate the error bars (e.g., analytical methods, software library functions, bootstrap methods) and the assumptions made in the process.
4. When presenting asymmetric distributions or results where the error could lead to unrealistic values (e.g., negative accuracy rates), the authors should consider reporting median performance and interquartile ranges or using other appropriate statistics that accurately represent the data distribution.
5. In addition to including error bars in graphs and tables, the authors should discuss the statistical significance of their findings in the text, highlighting cases where differences between methods are or are not statistically significant.

Incorporating these suggestions would significantly strengthen the paper by providing a clearer understanding of the results' robustness and helping readers to accurately interpret the findings.",0
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",No,We haven't provided details about compute resources used,"• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","Justification: The answer provided by the authors does not align with the content of the paper. The paper clearly mentions running times and computational resources for the experiments. In various sections, specifically in the additional results sections (e.g., Tables 17, E, F), detailed statistics about running times for each phase of the experiments across different settings are reported. This includes meta-training, meta-validation, and meta-testing phases, indicating a conscientious effort to disclose computational resources utilized. Moreover, the paper consistently delivers a comprehensive account of the experimental setup, including the number of tasks per phase and various configurations tested, albeit it does not specify the actual hardware (CPU/GPU) used or the memory and storage capacity.

Constructive and actionable suggestions:
1. To fully address the question regarding the disclosure of computational resources, the authors should explicitly mention the type of hardware used (CPU/GPU models), the memory capacity, and whether the experiments were run on an internal cluster or cloud provider. This would enhance reproducibility and allow readers to estimate the cost of replication.
2. The authors are encouraged to include an estimation of the total compute required for reproducing the experiments, considering both successful and any preliminary or failed experiments that significantly contributed to the compute utilization.
3. For future papers, consider creating a dedicated section or a table summarizing the computational resources used for each major experiment, including the type of compute workers, memory requirements, storage requirements, and total compute time. This practice can set a useful standard for reproducibility in computational research.
4. If applicable, discuss practical considerations for researchers with limited access to compute resources, including suggestions on scaling down experiments or utilizing open-source or cloud resources effectively.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",Yes,Our research is completely ethical and we conform to the NeurIPS code of ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","Justification for Score:
The given answer by the authors of the paper is overly simplistic and lacks detailed substantiation regarding how their research adherence to the NeurIPS Code of Ethics. Simply stating that the research is ""completely ethical"" without elaboration does not provide evidence of compliance with the specific guidelines outlined in the NeurIPS Code of Ethics. Additionally, the response fails to address any potential ethical considerations related to the creation, use, or distribution of the Meta-Album meta-dataset or the implications of their research, especially given the wide variety of domains and datasets included in Meta-Album, which could raise concerns such as data privacy, consent for data use, bias in AI models trained on this meta-dataset, and implications of the AI models developed through their research.

Constructive and Actionable Suggestions:
1. Detail Ethical Considerations: The authors should provide a more detailed explanation of specific ethical considerations they took into account during their research, particularly regarding data collection, processing, and use. This includes aspects such as ensuring the datasets included in Meta-Album have the appropriate licenses and consent for use in research, measures taken to minimize bias in the datasets, and any steps taken to ensure the privacy of individuals potentially represented in the datasets.

2. Address Potential Implications: The research may have broader implications, especially if the meta-dataset and the resulting models are used in real-world applications. The authors should discuss any potential societal impacts, both positive and negative, that their research may have. This includes considerations on fairness, accountability, and transparency of the models trained using Meta-Album.

3. Compliance with NeurIPS Code of Ethics: The authors should explicitly map their ethical considerations and practices to the specific guidelines outlined in the NeurIPS Code of Ethics. This may include discussing how their research promotes integrity, fairness, transparency, and respect for all participants and stakeholders involved.

4. Future Ethical Guidelines: If applicable, the authors should suggest guidelines or recommendations for other researchers who might use the Meta-Album meta-dataset or similar datasets in the future. This could include best practices for ensuring ethical use of the data, considerations for model deployment, and steps for continual ethical evaluation of AI systems trained on this data.

By addressing these points, the authors can provide a comprehensive and substantiated response that clearly demonstrates their research's adherence to ethical guidelines, including specific compliance with the NeurIPS Code of Ethics.",0
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,No,We haven't discussed the limitations in detail,"• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","The justification provided by the authors, stating ""We haven't discussed the limitations in detail,"" does not align well with the question asked, which specifically inquires about the discussion of potential societal impacts, both positive and negative. The context of societal impact is broader than just discussing limitations of the work. It encompasses the broader implications of the research on society, which could include benefits such as advancements in AI and machine learning, improved datasets for research, and negative impacts including privacy, security, and ethical concerns.

Constructive and Actionable Suggestions:
1. The authors should revisit the societal implications of their work, considering both the positive aspects, such as the potential for their meta-dataset to advance few-shot learning and meta-learning research across various domains, and the negative potential, including privacy concerns (since images are involved), misuse of refined image classification models (e.g., for surveillance without consent), and biased datasets leading to unfair AI systems.
2. It would be helpful if the authors include a dedicated section or paragraphs within existing sections to address these points more explicitly. This could involve speculative discussions on how their dataset and associated research might be used, misused, and what measures might mitigate such risks.
3. Where negative societal impacts are discussed, the authors should indeed consider proposing mitigation strategies—suggesting guidelines for ethical use, potential for future research on fairness and privacy considerations with their dataset, and how the community can ensure responsible use of their contributions.
4. If the work truly does not directly connect with societal impacts, it's valuable to at least acknowledge this explicitly and justify why that is the case, to meet reviewer expectations on this criterion.

Overall, enhancing the discussion on societal impacts would not only strengthen the paper's relevance to broader concerns but also showcase the authors' awareness of the implications of their work in real-world settings.",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",No,We haven't put any safeguards e.g. recommended use of datasets or license for datasets,"• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","The justification provided by the authors is not consistent with the content of the paper and the guidelines for responsible release of data or models. While the authors acknowledge the absence of safeguards and licenses for datasets, the Meta-Album paper describes an extensive and detailed process of dataset curation, preprocessing, and quality control, which indirectly contribute to the responsible use and sharing of the data. However, explicitly addressing safeguards and responsible release practices is critical, especially for a dataset as extensive and diverse as Meta-Album, which includes data from various domains and has the potential for wide usage across different research and application areas.

Constructive and actionable suggestions:
1. **License Information**: Clearly mention and provide the license under which the Meta-Album dataset and any related models are released. Choose licenses that encourage academic and responsible use while restricting misuse.
2. **Usage Guidelines**: Develop and include a section in the documentation or the paper that outlines recommended usage guidelines for the Meta-Album dataset. Highlight any sensitive domains within the dataset and suggest best practices for their use.
3. **Ethical Considerations**: Include a section discussing the ethical considerations taken into account during the dataset curation process, such as privacy, consent, and fairness. Discuss how these considerations affect the dataset's usage.
4. **Access Control**: If applicable, consider implementing access control mechanisms for particularly sensitive parts of the dataset, requiring researchers to submit use cases for review before gaining access.
5. **Community Engagement**: Encourage the community to report any issues or potential misuse associated with the dataset. Establish a review process to address these concerns.

In conclusion, while the Meta-Album dataset is a valuable resource for the research community, explicitly addressing safeguards for responsible use is important to ensure that the dataset serves as a positive contribution to the field.",0
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",No,We are releasing the datasets without any proper license,"• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","Justification Explanation:
The justification provided in the review answer contradicts the information presented in the paper. The paper explicitly mentions that all datasets included in the Meta-Album are verified for licenses and are free for use in academic research, with their original licenses provided. This indicates a clear effort by the authors to credit the creators or original owners of the datasets and respect the terms of use and licenses.

Constructive and Actionable Suggestions:
1. **Clarification and Citations:** Ensure that each dataset used in the Meta-Album meta-dataset is properly cited with the original source. Where applicable, include specific version information and URLs to enhance traceability.

2. **License Information:** It would be beneficial to explicitly mention the type of licenses under which each dataset is released. This information could be included either directly in the paper or in an easily accessible online documentation or repository linked from the paper. This clarification would directly address the review question and adhere to best practices for asset use in academic publications.

3. **Updated Justification:** Update the review justification to accurately reflect the efforts made in sourcing and using the datasets. Highlight the section or appendix in the paper where the licensing and terms of use are discussed.

4. **Derivative Works:** If any derivative datasets were created as part of the Meta-Album project (e.g., through preprocessing or formatting), specify the licenses under which these derived datasets are released. Ensure these licenses are compatible with the original datasets' licenses.

5. **Online Repository:** Consider setting up a dedicated webpage or GitHub repository for Meta-Album that includes detailed licensing information, dataset citations, and acknowledgment of original creators. This central resource could serve as a reference for users of Meta-Album and facilitate compliance with asset licenses and terms of use.

By addressing these suggestions, the authors will enhance the transparency and reproducibility of their research, while ensuring proper credit and adherence to licensing requirements for all assets used in their work.",0
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,No,We haven't provided proper documentation fo our new datasets in the paper,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","The justification provided by the authors appears to be inconsistent with the extensive documentation and details shared within the paper content. Throughout the paper, there is clear evidence of comprehensive documentation for the new assets introduced, namely the Meta-Album meta-dataset. This includes detailed descriptions of the datasets incorporated into Meta-Album, information on preprocessing steps, dataset licenses, the motivation behind the creation of the datasets, as well as insights into the datasets' structure, domains, and versions. Furthermore, the paper provides numerous links to additional resources, including the Meta-Album website, GitHub repository, and specific scripts related to data formatting and factsheet generation, which further supports the accessibility and documentation of these assets.

**Constructive and Actionable Suggestions:**

1. **Clarify Asset Documentation in the Text:** Although the documentation seems comprehensive, ensure that the paper explicitly states where and how the datasets and associated resources can be accessed and utilized. This helps in avoiding any confusion regarding the availability of documentation.

2. **Enhance Dataset Descriptions:** While the paper does an excellent job documenting the datasets, consider including more detailed discussions on the potential use cases of each dataset within Meta-Album and how they could benefit various research areas.

3. **Standardize Documentation:** To improve the dataset's accessibility, standardize the documentation across all provided links and repositories. Ensure consistency in how dataset specifications, license information, and usage limitations are presented.

4. **Address Consent and Ethics:** As per the guidelines, make explicit mention of how consent was obtained for the use of assets within the datasets, where applicable, and discuss any ethical considerations relevant to the dataset's creation and use.

5. **Provide Examples of Practical Application:** Include case studies or examples of research projects that have successfully leveraged the Meta-Album datasets. This could provide valuable insights for future users and highlight the datasets' versatility.

6. **Improve Anonymization Instructions:** If applicable, offer clear guidance on how users can anonymize data derived from Meta-Album for their publications, ensuring the ethical use and sharing of research outcomes.

By addressing these suggestions, the authors can reinforce the utility and accessibility of the Meta-Album assets while ensuring adherence to best practices in documentation and ethical considerations.",0
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Not Applicable,Our work is not related to any crowdsourcing experiments.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","Justification:
The paper indeed involves work and data that are directly associated with crowdsourcing efforts and human subjects. Specifically, it talks about the ""SPIPOLL"" project (part of the Small Animals domain), where hundreds of volunteers are involved in picturing and identifying insects, indicating involvement of human subjects in data collection. Additionally, the ""Pl@ntNet Project"" (part of the Plants domain) is mentioned, which involves citizen botanists from around the world contributing images and labels. These instances point to the use of crowdsourced data collection methods involving human subjects.

Constructive Suggestions:
1. **Clarification and Details**: The paper should explicitly clarify the role of human subjects in the data collection process for the mentioned datasets. Even if the authors themselves did not conduct the crowdsourcing, mentioning the source of the datasets implies the use of human subjects indirectly associated with their work.

2. **Ethical Considerations and Compensation**: Given the guidelines, the paper should include a brief statement on the ethical considerations surrounding the use of these datasets, including how the contributors (volunteers, citizen scientists) were acknowledged or compensated, if at all. This information could be included in a dedicated section or as part of the dataset descriptions.

3. **Instructions and Screenshots**: For completeness and transparency, the authors could consider including, in the supplementary material, the full text of instructions given to participants in the original data collection efforts for the crowdsourced datasets, or references to where this information could be found if it's publicly available.

4. **Broader Ethics Statement**: The authors are encouraged to add a general statement on ethical considerations related to their research work, especially since it leverages human-sourced data. This should cover the respect for the rights and dignity of volunteers who contributed to the data collection.

Implementing these suggestions will not only address the inconsistency but will also strengthen the paper's ethical stance and transparency regarding the use of human-collected data.",0
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Not Applicable,This does not apply to our research,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The authors' response of ""NA"" and their justification that this does not apply to their research is consistent with the content of the paper. Based on my review of the paper, the focus is on introducing Meta-Album, an image classification meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks. It includes and processes various open image datasets for research purposes and does not involve crowdsourcing, direct experimentation on, or data collection from human subjects. Therefore, the question about risks to study participants, disclosure of such risks, and Institutional Review Board (IRB) approvals does not apply to the nature of this research.

However, as part of constructive feedback for future iterations of related work or any extensions of this project that might involve human subjects (e.g., for dataset annotation, model evaluation by humans, etc.), the authors might consider the following suggestions:
1. Clearly differentiate between purely computational research and studies involving humans directly or indirectly. For the latter, appropriate disclosures and approvals should be obtained and documented.
2. If future work involves human participants in any capacity (such as evaluation of algorithm outputs, user studies, crowdsourcing for dataset annotation, etc.), the authors should seek IRB approval or an equivalent ethical review based on the requirements of their country or institution. This ensures ethical standards are maintained, and participant welfare is prioritized.
3. Maintain transparency about the sources of the datasets used, explicitly stating any human involvement in their creation or preprocessing, and ensure adherence to the NeurIPS Code of Ethics, respecting privacy and data protection norms.

In summary, the authors' response is consistent with the given guidelines and the content of the paper, which focuses on computational approaches to processing and analyzing image datasets without involving human research subjects.",1
