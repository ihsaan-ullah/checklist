Question,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Yes,Our main claims are listed in section 1 and highlight this paper's contributions.,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ","The answer given by the authors is correct based on the content of the paper provided. The abstract and introduction outline the main contributions and scope of the paper accurately. They address the integration of spatial and temporal modules in Spatio-Temporal Graph Neural Networks (STGNNs), the issues with existing Neural Architecture Search (NAS) methods, and propose a disentangled framework to simplify architecture search. These claims directly reflect the contributions detailed throughout the paper, including a new framework for understanding and simplifying architecture search in STGNNs, empirical evidence supporting the efficiency of their proposed methods, and the performance superiority of models discovered through their framework over state-of-the-art models.

Justification:

1. The abstract clearly states the problem, the identified issues with existing methods, and the approach taken in this paper to address these issues.
2. The introduction provides a more detailed context for the motivation behind the study, the specific contributions of their work, and the basis for these contributions.
3. The claims are consistent with the theoretical and experimental results presented in the paper. The authors provide a thorough explanation and justification for their approach, supported by empirical evidence.
4. The scope of the paper, as laid out in the abstract and introduction, is indeed covered thoroughly in the subsequent sections. The paper delivers on its promises by detailing the proposed framework, the methodology for the analysis of STGNN architectures, and comparisons with existing models.

Constructive feedback for the authors to strengthen their presentation might include:

1. While the abstract and introduction accurately reflect the paper's contributions, it would enhance clarity to specifically mention the measurable impact on the efficiency and performance of NAS for STGNNs early in the abstract. This could help the reader quickly grasp the practical significance of the research.
2. The authors could further highlight the novelty of their disentangled framework in the introduction by contrasting it more directly with the limitations of existing approaches.
3. It might be beneficial to briefly discuss the potential limitations of their approach or any assumptions made that future research could address, ensuring a balanced view of their work's scope.
4. Given the technical depth of the paper, a simplified summary or graphical abstract highlighting the main contributions and how they fit together would help readers not as familiar with the field.

Overall, the authors have accurately presented the scope and contributions of their paper in the abstract and introduction, providing a solid foundation for the detailed exploration that follows.",1
Does the paper discuss the limitations of the work performed by the authors?,No,We did not discuss limitations in the paper.,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ","The justification provided by the authors for not discussing the limitations of their work is insufficient as per the expectations set in the guidelines for the paper submission. Given the complexity and scope of their work on understanding and simplifying architecture search in Spatio-Temporal Graph Neural Networks (STGNNs), it's critical for the research integrity and for the benefit of the community that limitations are acknowledged and discussed. This could cover a range of aspects such as the generalizability of the proposed method across different datasets, computational efficiency, or how the assumptions made in their model might impact the real-world applicability.

Constructive and actionable suggestions:
1. **Assumptions and Robustness**: The authors should articulate the assumptions underpinning their model and experiments, such as data distribution assumptions, and discuss the robustness of their findings against deviations from these assumptions.
2. **Generalizability**: Reflect on the scope of the tested methodologies — were they validated on a broad range of datasets, including those with significantly different characteristics? If not, it's crucial to outline how this might limit the applicability of the findings.
3. **Computational Constraints**: Given the typically resource-intensive nature of Neural Architecture Search (NAS), it’s pertinent for the authors to discuss the computational efficiency of their proposed method and its scalability.
4. **Future Directions**: Identifying limitations is not just about reflecting on the current work but also about setting the stage for future research. Discussing underexplored areas or unaddressed challenges could pave the way for more comprehensive future studies.
5. **Impact on Broader Issues**: If relevant, discuss any potential limitations in the context of privacy, fairness, or ethical considerations. For instance, does the methodology rely on data that could be biased, or could the approach inadvertently magnify existing disparities?

In summary, acknowledging and discussing limitations is critical for the transparency and advancement of scientific research. It provides the community with a clearer understanding of the work's boundaries and encourages further exploration and refinement.",0
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Not Applicable,This paper does not include theoretical results.,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. ","The answer provided by the authors is consistent with the content of the paper under review. Upon careful examination of the paper's content, there are no sections or parts where theoretical results such as theorems or lemmas are discussed or proved. The focus of the paper is on presenting a novel framework for architecture search in Spatio-Temporal Graph Neural Networks (STGNNs), addressing hyperparameters and search space issues, and empirically validating the proposed methods through experiments. The paper's contributions are mainly methodological advancements and empirical findings rather than theoretical analyses or proofs.

Constructive and Actionable Suggestions:
1. Although this work does not present new theoretical results, for future research that might involve theoretical contributions, it would be beneficial to ensure that any theoretical results are accompanied by clear statements of assumptions, and full proofs are provided either in the main body of the paper or supplementary materials. Short sketches or intuitions for the proofs in the main text can greatly aid readers' understanding.
2. In future works that might involve theoretical aspects, it is advisable to systematically number theorems, lemmas, and proofs for easier reference. This structuring will also help in maintaining clarity and coherence in presenting and discussing theoretical results.
3. If theoretical results or optimizations are eventually considered, clearly outlining how these theoretical considerations impact the practical deployment and performance of the proposed methods would be insightful. Demonstrating the theoretical underpinnings can enhance the robustness and reliability of the work.
4. Lastly, for empirical works such as this one, continuing to align the methodological contributions with comprehensive, comparative experimentation and analysis is crucial. It strengthens the proposed model's claims and provides clear insights into its advantages and potential limitations within the context of existing approaches.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Yes,"We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.","• The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. Open Access to Data and Code","Justification: The paper provides a comprehensive account of the experimental setup, including descriptions of datasets, baselines, implementations, and results across various sections (sections 4, 5, and appendices B, C, D, E). Moreover, it offers access to the implementation code through a GitHub link, which significantly aids in reproducibility. These elements collectively facilitate a clear path for reproducing the main experimental results, adhering to the guidelines provided.

Constructive and Actionable Suggestions:
1. **Clarify Dataset Details:** Ensure that the descriptions of the datasets in appendix B and elsewhere in the paper are detailed, including any preprocessing steps or modifications from their original form. This clarity will help researchers understand exactly what data was used and how it was prepared.

2. **Baseline Comparisons:** For the baselines listed, ensure that any deviations from the original implementations (if used) or specific parameter settings that differ are clearly noted. This will help others understand potential differences in performance metrics.

3. **Experiment Details:** While the paper lays out the experimental settings comprehensively, clarifying any hyperparameters, architectural details, or optimization tricks not covered by the GitHub repository in the paper itself would be beneficial. It ensures that researchers without access to the repository (e.g., due to future availability issues) can still attempt to reproduce the results.

4. **Expand on Implementation Details in Appendices:** Given the complexity of the framework and the number of components involved, expanding on certain implementation details in the appendices could provide additional clarity. For example, more details on the configuration of the searched model in appendix E could be useful.

5. **Address Limitations and Challenges in Reproducibility:** If there were any particular challenges faced that could impact reproducibility (e.g., computational resources required, limitations of available frameworks, etc.), discussing these openly could provide valuable context for others attempting to reproduce or build upon this work.

6. **Consider Providing a Docker Container:** To further enhance reproducibility, consider providing a Docker container or similar virtual environment configuration that encapsulates all the software dependencies required to run the code. This step would minimize issues related to software dependencies and versions.",1
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Yes,"All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.","• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ","The answer provided by the authors is consistent with the guidelines and the content of the paper. They state that both the data and the code are open access, with specific mention of the GitHub repository where the code can be found. This should, in theory, enable reproducibility of the main experimental results as required by the guidelines.

For the paper to fully meet the expectations for openness and reproducibility, the GitHub repository mentioned must include:
1. Detailed instructions on how to run the experiments, including environment setup, dependencies, and exact commands.
2. Scripts for data preprocessing to match the formats expected by the codebase.
3. An explanation of any steps required to replicate the dataset or access it, acknowledging that the datasets are publicly available as per the references provided in the appendices.
4. Scripts or instructions to replicate all experimental results reported in the paper, specifying if any result cannot be reproduced with the provided code and the reasons for the same.

While the answer is affirmative and aligns with the guidelines, it's critical for the review to assess if the GitHub repository does indeed contain all necessary elements to ensure reproducibility. For a robust review, one would need to verify the provided repository for completeness in documentation and ease of reproducing results.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Yes,Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. ","Justification for Score:
The authors claim to provide all necessary training and test details, including hyperparameter sampling methods and data splits, mentioning that these details are explained in specific sections and an appendix. However, upon review, there are several areas where more specificity could enhance clarity and reproducibility:

1. **Hyperparameter Exploration:** While the paper provides an overview of hyperparameter exploration, particularly in the context of reducing the hyperparameter space (e.g., Table 2 and associated text), the justification for selecting certain hyperparameters over others could be more detailed. The ranking strategy for hyperparameters is a novel methodological component, but more explanation on how the rankings were utilized to finalize hyperparameter choices would be beneficial.

2. **Optimizer and Learning Rate Specifics:** The paper mentions studying different optimizers and learning rates, detailing their narrowed scopes (Table 2), but lacks deeper discussion on why specific values within the ""Reduced Scope"" were chosen for the final experiments. The impact of hyperparameter choices (e.g., Adam vs. AdamW and different learning rates) on model performance would offer valuable insights.

3. **Dataset Split and Evaluation Metrics:** Appendix B provides dataset statistics, but more information on the test and validation splits and the reasoning behind their proportions would strengthen the experimental setup's transparency. Furthermore, while multiple evaluation metrics (MAE, RMSE) are used, clarity on their choice and any dataset-specific considerations would be useful, especially for non-traffic datasets.

4. **Reproducibility Concerns:** The paper's attempts to provide comprehensive experimental details are commendable, but the inclusion of specific version numbers for software and libraries used, along with more explicit command-line instructions for training and evaluation, would greatly enhance reproducibility.

Actionable Suggestions:
1. **Expand on Hyperparameter Selection Justification:** Include a more detailed rationale for final hyperparameter choices, possibly with comparative results showing the impact of different hyperparameters on model performance.
   
2. **Clarify Dataset Splits:** Explicitly state the proportions used for training, validation, and test splits across datasets and justify these choices to assure readers of the robustness and generalizability of the experiments.

3. **Detail Versioning and Code Specifics:** Ensure that the paper or supplementary material includes exact version numbers for all software and libraries, alongside any relevant code snippets or command-line instructions to facilitate reproduction of results.

4. **Enhance Discussion on Optimizer and Learning Rate Choices:** Provide a more detailed discussion on the choice of optimizer and learning rate, including a brief summary of the performance implications observed during exploration. This would offer readers deeper insights into the modeling decisions.

Improving the specificity and clarity of the experimental details as suggested would significantly enhance the paper's relevance, reproducibility, and contribution to the field, addressing the gaps identified in the response to the checklist question.",0
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Yes,Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.,"• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ","Justification: While the paper does state that main experiments include variance as error bars in the results, the provided excerpts do not offer specific details on how these error bars are calculated, whether the distribution of results is normal, and whether 1-sigma or 2-sigma error bars are used. The guidance requires a detailed explanation of the calculation methods, assumptions made, and an explicit mention of the factors contributing to variability. The paper's content, as provided, lacks these explanations.

Constructive suggestions:
1. **Methodology for Error Bars**: Include a subsection or paragraph that explicitly explains how the error bars were calculated. If a specific statistical test or method was used, such as bootstrap or a library function, mention this and provide references or formulas if necessary.

2. **Assumptions and Variability Factors**: Clearly state any assumptions made during the calculation of statistical significance, such as the presumption of normally distributed errors. Additionally, detail the sources of variability in your experiments, whether those come from different train/test splits, initialization procedures, or any stochastic elements of the models or training process.

3. **Detailed Statistics**: When reporting error bars, specify whether they represent one standard deviation (1-sigma), two standard deviations (2-sigma), standard error of the mean, or another measure. This detail is crucial for readers to accurately interpret the significance and reliability of the results.

4. **Explanation of Symmetry/Asymmetry in Error Bars**: If the distribution of results is not symmetric, consider reporting confidence intervals or median with interquartile ranges instead of mean with standard deviation/error. If symmetric error bars are used, justify this choice based on the distribution of the results.

5. **Reference to Error Bars in Text**: Ensure that there is a clear reference in the text to where error bars can be found (e.g., specific figures or tables), and discuss the statistical significance of the findings in light of these error bars. This discussion can help readers better understand the reliability and potential variability of the results.

Implementing these suggestions would not only align the paper with the checklist's requirements but also enhance the clarity and credibility of the reported results for the readers.",0
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Yes,"In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). Code of Ethics","Justification:
The provided justification does not fully align with the comprehensive guidelines provided for evaluating the sufficiency of disclosed information regarding computational resources used for experiments. While the paper mentions GPU hours in Section 5, which provides some insight into the compute time required, it lacks detailed information on the type of computational resources (e.g., specific GPU models, CPU types), memory, and storage capabilities. Additionally, the paper does not address the use of internal clusters or cloud providers nor does it give a breakdown of compute requirements for individual experimental runs or discuss the compute resources related to preliminary or failed experiments.

Constructive and Actionable Suggestions:
1. **Detailed Compute Resources**: The authors should include specific details about the computer resources used, such as the types and models of GPUs or CPUs. Details about the memory and storage capabilities of these resources should also be provided.
   
2. **Breakdown of Compute Requirements**: It would be beneficial for the authors to offer a more granular breakdown of the compute resources required for each experiment. This breakdown can help researchers with varying levels of computational resources understand the feasibility of reproducing the experiments.

3. **Use of Cloud Providers or Internal Clusters**: If any internal clusters or cloud-based services were utilized for the experiments, the authors should disclose this information, including any relevant configurations and the potential impact on reproducibility for those lacking access to similar resources.

4. **Account for All Experiments**: The authors should consider discussing the compute resources involved in preliminary or failed experiments, if any. This addition would provide a more comprehensive view of the computational expense related to the research project as a whole, which is valuable for readers attempting to gauge the total resource investment required.

5. **Ethical Considerations and Accessibility**: Include a discussion on the ethical considerations of computational resource consumption and the steps taken to ensure the research is as accessible as possible to the broader community, considering the varying levels of resources available to different researchers.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",Yes,We review the code of ethics and our paper conform with it.,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ","The authors have stated their compliance with the NeurIPS Code of Ethics, which is an affirmation of adherence unless evidence suggests otherwise. Given the provided information, there's no basis to doubt their compliance. However, it is crucial to encourage thorough and transparent documentation of ethical considerations, especially in research involving innovative methods like Neural Architecture Search (NAS) and Spatio-Temporal Graph Neural Networks (STGNNs). The potential implications of these technologies on privacy, fairness, and societal impact warrant careful consideration. While the authors have not detailed specific ethical considerations in their justification, the absence of visible ethical issues in the paper content supports their claim of compliance.

Constructive Suggestions:
1. **Ethical Considerations Section**: Consider adding a section dedicated to discussing the ethical implications of your work, even if the conclusion is that there are no significant ethical concerns. This can include considerations around data privacy, potential misuse of the technology, and how the model's predictions might impact real-world decisions.

2. **Bias and Fairness Analysis**: Given the application in traffic prediction and potentially other areas, it's important to discuss if and how the model's predictions could inadvertently favor or discriminate against certain groups or regions. Including an analysis of the model's fairness and biases would improve the paper's adherence to ethical guidelines.

3. **Impact on Stakeholders**: Reflect on how your research might affect various stakeholders, including the public, government agencies, and those involved in traffic management. Highlighting positive impacts, potential risks, and mitigation strategies would demonstrate a thorough ethical review.

4. **Data Privacy and Consent**: If applicable, clarify the origins of the datasets used, ensuring that data was collected and processed in compliance with privacy laws and regulations. Discuss the measures taken to protect individuals' privacy, especially in datasets involving real-world traffic data.

5. **Model Misuse**: Address the potential for misuse of your research findings or technology and propose strategies to prevent such scenarios. This could include recommendations for secure deployment or limitations on the use of technology.

By addressing these suggestions, the paper would not only conform to the NeurIPS Code of Ethics but also set a robust example for ethical considerations in computer science research.",1
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,No,"We does not discuss the broader impacts. • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. Licenses for Existing Assets","Justification: While the authors claim that their work does not discuss broader impacts and thus justify their response with a ""No,"" this review finds the justification not fully consistent with the expectations outlined in the given guidelines. Given the nature of the work on Spatio-Temporal Graph Neural Networks (STGNNs) and their applications in traffic prediction, there are clear pathways to societal impacts, both positive and negative, that warrant discussion.

Constructive and actionable suggestions for the authors include:

1. **Discuss Potential Positive Impacts**: The authors should elaborate on how advancements in STGNNs could contribute to societal benefits, such as improving traffic management, reducing congestion, and thereby decreasing carbon emissions. Enhanced traffic prediction models can also lead to better urban planning and emergency response strategies.

2. **Address Possible Negative Impacts**: The paper should consider discussing potential risks associated with the deployment of advanced traffic prediction models. For instance, the misuse of traffic data and predictions could lead to privacy concerns or the unfair targeting of specific areas or populations. The authors should reflect on how their work could be misused or lead to unintended consequences.

3. **Consider Fairness and Bias**: The authors should assess if their proposed methods could inadvertently perpetuate or exacerbate biases present in traffic prediction data. This includes examining whether the model's predictions are equitable across different geographic areas and demographically diverse populations.

4. **Privacy Considerations**: Given that traffic prediction models often rely on collecting and analyzing large volumes of data, the authors should discuss privacy implications and how they plan to ensure the responsible use and protection of data.

5. **Mitigation Strategies**: If the paper identifies potential negative societal impacts, the authors should propose mitigation strategies. This could include recommendations for policymakers, suggestions for ethical guidelines in deploying such models, or technical solutions to prevent misuse.

6. **Safeguards and Responsible Release**: Although the authors marked NA for safeguards, they should still reflect on any necessary precautions if their models or datasets are shared publicly. This includes considering licenses that emphasize ethical use, privacy protections, and restrictions to prevent misuse.

In summary, the answer does not fully recognize the broad spectrum of societal implications associated with advancements in STGNNs for traffic prediction. The paper would benefit from a more comprehensive discussion of both positive and negative societal impacts, along with considerations for responsible research and application of these technologies.",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Not Applicable,This paper poses no such risks.,"• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. Licenses for Existing Assets","The response given by the authors, marked as NA with the justification that the paper poses no such risks, is consistent with the content presented in the paper. The focus of the research is on developing a framework for understanding and simplifying architecture search in Spatio-Temporal Graph Neural Networks (STGNNs) for applications such as traffic prediction. The paper does not introduce new datasets, nor does it propose models with inherent risks for misuse similar to what might be found in language or image generation models. Furthermore, there is no indication that the models or any compiled dataset involved poses a high risk for dual-use or misuse.

**Justification for Agreement:**
1. The paper's primary contribution is methodological, aimed at improving efficiency and accuracy in neural architecture search for STGNNs.
2. The applications discussed, such as traffic prediction, are generally aimed at solving practical challenges rather than generating content that could be misused.
3. The paper explicitly mentions making their code available, indicating a commitment to transparency, but does not discuss releasing models or datasets that could raise ethical or misuse concerns.

**Constructive and Actionable Suggestions:**
Though the paper is marked correctly as posing no high risks for misuse, the authors could strengthen their work by briefly discussing ethical considerations related to traffic prediction or similar applications of STGNNs. For instance, they could mention:
- The importance of preserving user privacy when dealing with spatio-temporal data.
- Potential biases in traffic prediction models and ways to mitigate them.
- The significance of ethical AI development practices in their research.

This discussion would not only clarify their stance on responsible AI development but also might guide future research in this area towards more ethical and safe practices.",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Yes,"We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. ","Justification:
While the answer indicates that the datasets used in the paper were cited and a URL was provided, it does not fully address the licensing information, terms of use, or copyright concerns as required by the guidelines. It is essential not only to cite the original sources of the datasets but also to clearly mention the licensing terms and confirm that these terms are respected in the paper. The provided answer lacks specific mention of the licensing details for the used datasets, which is critical for compliance with ethical and legal standards in research publications.

Constructive and actionable suggestions:
1. Explicitly state the licensing terms for each dataset used in the study. If the datasets are public and have a specific license (e.g., CC-BY, MIT, GPL), this should be clearly mentioned in the paper.
2. If the datasets' terms of use have specific conditions for acknowledgment or restrictions on use, make sure these are explicitly mentioned and that your use complies with these terms.
3. If the datasets do not have clear licensing information, consider reaching out to the original creators to clarify the terms under which the data can be used and mention this communication in the paper.
4. For completeness and transparency, include a brief statement in the paper about how the data were used in compliance with the original licensing terms. This could be part of the methodology section or an acknowledgment.
5. If your paper releases any code, models, or derived datasets based on these original datasets, specify the license under which your contributions are released, ensuring it is compatible with the original datasets' licenses.
6. Lastly, consider adding a separate subsection or an appendix that consolidates all information related to asset licensing, copyright, and terms of use for the datasets and any software used in the research. This centralized information would make it easier for readers and reviewers to assess compliance with ethical guidelines.",0
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Not Applicable,The paper does not release new assets.,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ","Justification: The justification provided in the answer ""NA"" contradicts the information found in the paper. The paper introduces a novel framework for ""Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks,"" along with conducting comprehensive experiments to derive certain principles for designing STGNN architectures. Moreover, it presents a simplified method, termed ""SimpleSTG,"" for searching better STGNN models. It's mentioned explicitly that the code related to this research is made available on GitHub, which constitutes the release of new assets. Therefore, stating that the paper does not release new assets is not accurate.

Suggestions:
1. The review should acknowledge the introduction of new assets, specifically the ""SimpleSTG"" method and the associated code on GitHub.
2. The review should assess the documentation provided for these assets. Considerations should include whether the GitHub repository contains a README file with details on how to use the code, the requirements (e.g., library versions), example scripts, and any notes on the architecture or the experimental setup that would help in reproducing or building upon the work.
3. If the paper lacks detailed explanation or documentation on the assets' use, applicability, limitations, and how consent was obtained (if applicable), these should be highlighted as areas for improvement. For instance, the paper could be asked to include a discussion on the potential limitations of the proposed ""SimpleSTG"" method or details on the datasets' licenses.
4. Suggest that authors ensure the provided GitHub link is accessible and anonymized if required by the submission guidelines. They should also ensure the repository remains available and maintains anonymity until the review process is concluded.

By addressing these aspects, the review will provide a thorough and critical assessment of the documentation and release of new assets as per the guidelines provided.",0
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ","The response provided by the authors is consistent with the content of the paper. Upon reviewing the paper's abstract, introduction, methodology, experiments, and conclusions sections, there is no mention or indication that crowdsourcing experiments or research involving direct human subjects were conducted. Instead, the focus of the research is on understanding and simplifying the architecture search of Spatio-Temporal Graph Neural Networks (STGNNs) for multivariate spatio-temporal forecasting tasks, such as traffic prediction, which does not inherently require human subject studies or crowdsourcing experiments.

Given this context, marking the question as ""NA"" (Not Applicable) is accurate and correct. Since the paper's methodologies and experiments are centered around computational models and algorithmic development without direct human participation, there is no need for the inclusion of instructions given to participants, screenshots of the experiments involving humans, or details about compensation, as none of these apply to the work presented in the paper.

Suggestions for Improvement:
1. None - The authors' handling of this question is appropriate given the nature of their research. Their work does not involve crowdsourcing or human subjects, so the guidelines for reporting on such experiments do not apply.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The answer provided by the authors is consistent with the paper content, and the justification correctly states that the paper does not involve crowdsourcing nor research with human subjects. Therefore, there's no requirement for Institutional Review Board (IRB) approvals or disclosures about potential risks to study participants. As this work focuses on the evaluation and propositions related to Spatio-Temporal Graph Neural Networks (STGNNs) for traffic prediction and architecture search, human subjects are not directly involved in the experiments or data collection. The research primarily deals with computational models, algorithm development, and analysis of traffic prediction performance, which do not entail ethical concerns related to human subjects.

Constructive and actionable suggestions are not applicable in this case since the question about IRB approvals or equivalent review processes does not pertain to the nature of the study conducted in the paper.",1
