# Overview 

We are using the Codabench platform to provide a self-service checklist verification tool, based on large language models, for the NeurIPS 2024 checklist. This is not a product, this is an experimental service provided as is. See our terms of use.

The academic review process is becoming more congested due to the rapid increase in the number of papers being produced. This surge is influenced by various factors, notably the pressure on researchers to publish for career advancement. While it may not be advisable to promote the creation of substandard research papers by enabling fully automated large-scale review production, easing the burden on reviewers and editors through partial automation of their tasks could enhance the quality of reviews. This would allow them to concentrate on aspects where their expertise is most crucial. An automated review process can also help authors themselves check any problems with their papers. 

In this experimental setting, we aim at helping authors verify their checklist before submitting their paper. We welcome the submission of **genuine submissions** and of **adversarial submissions** (containing errors with: ethics, theory, or experiments, meant to test the LMM’s robustness to bad papers and/or dishonest checklist answers). If you make adversarial submissions, we will ask you to reveal the design of the flaws that you introduced, for research purposes.

### ⬇️ [Download Sample Paper Submission](https://www.codabench.org/datasets/download/6fc884ae-1878-4fcf-8d60-361d8fbec1c4/)