Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","Discrepancies: No discrepancies are observed between the authors' justification and the actual paper content regarding the main claims made in the abstract and introduction. The abstract and introduction accurately summarize the contributions and scope of the work, which are the introduction of Meta-Album as a diverse, large-scale, and extendable image classification meta-dataset, designed to facilitate research in few-shot learning, meta-learning, and cross-domain generalization.

Feedback:
1. **Clarification on Dataset Usability Outside Competition**: The paper mentions that Meta-Album is also intended to serve outside of competition settings, particularly in academic research. It would be beneficial to include more detailed examples or potential research questions that Meta-Album could help address, especially in areas other than few-shot learning and meta-learning, if any.

2. **Extensibility Details**: The paper highlights the extensibility of Meta-Album as a key feature. For further clarity and to encourage contributions from the community, consider expanding on the process for adding new datasets or domains. Specifically, details on the review process for quality checking, any automatic tools or scripts provided for format checking, and examples of successfully added datasets after the initial release could be valuable.

3. **Bias and Fairness**: Given the diverse domains covered by Meta-Album and the potential for learnt models to be applied in sensitive applications, a discussion on bias and fairness within the datasets and their implications for models trained on Meta-Album would enhance the paper. If applicable, suggestions for mitigating dataset and model biases could be offered.

4. **Comparison with Other Benchmarks**: While the paper includes a comprehensive comparison table with other datasets/benchmarks in terms of quantitative and qualitative features, further analysis comparing the performance of models trained on Meta-Album versus those trained on other benchmarks could provide deeper insights into Meta-Album's effectiveness and uniqueness.

5. **Performance Baselines**: The presented baseline performances are a good starting point. For future iterations or extended versions of the paper, including more diverse baseline methods, especially recent advances in few-shot learning and transfer learning, could help establish a more comprehensive benchmark for the community.

6. **Real-world Application Scenarios**: Highlighting specific real-world application scenarios where Meta-Album could have an impact, or where its datasets are particularly relevant, could further demonstrate its value to a wider audience, including industry researchers.

7. **Open Source Code and Reproducibility**: Ensuring that the links to GitHub repositories, tools, and scripts are up-to-date and maintained will be crucial for the long-term impact of Meta-Album. A brief section on the paper about the support or community around Meta-Album could encourage more active engagement and contributions.

By addressing these points, the paper can significantly enhance its appeal to a broader audience, encourage community engagement, and provide a clearer path for the effective use of Meta-Album in advancing machine learning research.

",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,We have clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","Discrepancies: No, there are no discrepancies between the justification provided by the authors and the actual paper content. The authors have discussed the limitations of their work clearly in the mentioned section, as per their claim.

Feedback:
The authors have made commendable efforts in introducing Meta-Album, a highly versatile and extensive meta-dataset aimed at advancing research in few-shot learning, meta-learning, and more broadly, in machine learning. The detailed presentation of the dataset, including its composition, creation, and potential applications, provides a solid foundation for its utilization in empirical research. However, to further enhance the paper and maximize the impact of Meta-Album within the research community, the following recommendations are suggested:

1. **More In-depth Discussion on Limitations and Assumptions**: While the authors mention discussing limitations, expanding this discussion to explicitly address the assumptions made during the meta-dataset creation and their potential impact on the generalizability of findings would strengthen the paper. This could include assumptions made about the sources of data, the preprocessing steps, and their effects on the representativeness of each domain included in Meta-Album.

2. **Scalability and Efficiency**: Although Meta-Album's design is commendable for its inclusivity and extensibility, a more thorough analysis of the computational efficiency and scalability when using this dataset in practical machine learning problems would be valuable. Specific use cases or scenarios where Meta-Album may pose computational challenges can provide researchers with crucial insights when planning experiments.

3. **Diversity and Fairness Considerations**: Given the wide application scope of Meta-Album, elaborating on efforts made (or planned actions) to ensure diversity within the dataset and addressing fairness-related issues would be beneficial. This aspect is particularly pertinent for domains such as human actions and facial recognition, where biases in data can significantly impact the fairness of machine learning models.

4. **Potential for Future Extensions**: While the paper briefly touches on the extendibility of Meta-Album, providing more concrete guidelines or a roadmap for the community to contribute new domains or datasets could foster collaboration and ensure the dataset’s continuous growth. Highlighting specific areas or domains where additional datasets would be particularly valuable could help direct community efforts.

5. **Experimental Validation and Benchmarks**: Finally, incorporating additional experimental results or benchmarks comparing state-of-the-art few-shot learning and meta-learning algorithms on Meta-Album would give readers a clearer understanding of the dataset's challenges and opportunities. While the authors provide a comprehensive overview, more empirical insights into the dataset’s utility across various learning paradigms can further underscore its value.

By addressing these recommendations, the authors can significantly enhance the utility and impact of Meta-Album, providing the machine learning community with a valuable resource for advancing research in few-shot learning and beyond.

",0.5
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","Discrepancies: No discrepancies were found between the authors' justification and the content of the paper regarding theoretical results and proofs, as the submission focuses on presenting a multi-domain meta-dataset for few-shot learning, rather than theoretical contributions.

Feedback: 
The submission of ""Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification"" to NeurIPS effectively addresses an important gap in the domain of few-shot learning by providing a substantial and diversified dataset intended to stimulate further research in this field. The efforts to ensure diversity across domains, provide preprocessed and uniformly formatted datasets, and encourage community involvement for future extensions are all commendable aspects. However, to strengthen the submission and ensure it meets the high standards of NeurIPS, the following points warrant consideration:

1. **Baseline Methods Justification**: While the paper presents experiments with several baseline methods, there's limited discussion on the choice of these specific methods. Providing a rationale for selecting these particular baselines, considering the vast landscape of few-shot learning methods, would add depth to the experimental design. Discussing any limitations or advantages these methods may have in the context of the Meta-Album datasets would be informative.

2. **Dataset Preparation**: The detailed description of the dataset preparation process, including cleaning, cropping, resizing, and formatting, is appreciated. However, insights into challenges faced during these processes, and how they were overcome, would be valuable for future researchers looking to extend Meta-Album or create similar datasets. Specifically, discussions around handling class imbalance and ensuring data quality across such a diverse set of images could enhance the paper's utility.

3. **Extendability**: The paper mentions Meta-Album's design for extendibility by the research community. While guidelines for contributing to the dataset are provided, elaborating on the review process for new contributions, including quality checks and integration approaches, would solidify this aspect. Additionally, providing a framework or criteria for selecting new domains or datasets for inclusion could help maintain the meta-dataset's diversity and quality.

4. **Impact on Few-Shot Learning Research**: The paper successfully introduces Meta-Album and showcases its utility through experimental evaluation. Expanding the discussion on how Meta-Album can address specific open challenges in few-shot learning, such as domain adaptation, class imbalance, and continual learning, would underline its significance. Highlighting potential research directions that Meta-Album uniquely enables could further demonstrate its contribution to advancing the field.

5. **Supplemental Material**: If not already included, considering the addition of supplementary material showcasing additional experimental details, extended results, or even case studies of using Meta-Album in novel few-shot learning approaches could enhance the submission. Such material might also include anonymized data on class distributions, performance variations across domains, or visualizations augmenting the main paper's content.

By addressing these points, the submission would not only provide a valuable resource to the few-shot learning community but also stimulate discussion on methodological approaches suited to this enriched, multi-domain meta-dataset.

",0.5
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","Discrepancies: No discrepancies are noted between the authors' justification and the actual paper content. The authors have promised to provide the code and detailed instructions on their GitHub repository and information on how to access the datasets on their website, aligning with their commitment to ensure reproducibility.

Feedback: 
1. **Code and Data Accessibility**: Ensure that the GitHub repository and dataset access instructions on the website are publicly available and clearly documented. It’s crucial that access links are not restricted and detailed steps for data preprocessing, if any, are provided. Including environment setup (e.g., required libraries and versions) will further aid in reproduction.

2. **Experimental Details**: The paper provides a comprehensive description of the Meta-Album meta-dataset and related experiments. However, to ensure reproducibility, it would be beneficial to include more detailed configurations, such as hyperparameters used for each model, any data augmentation techniques applied, and specific settings for the training procedure. If any part of the experiments or data processing is computationally intensive, suggesting possible workarounds or providing scripts for cloud-based execution can be beneficial.

3. **Baseline Implementations**: The paper mentions the use of various baseline methods. Clarify whether the implementations of these baseline methods are provided as part of the provided codebase. If there are any modifications or optimizations made to these baselines specifically for compatibility with the Meta-Album dataset, please document these changes thoroughly.

4. **Extension Guidelines**: As Meta-Album is designed to be extendable, include a guideline or template for contributors who wish to add new datasets or domains. This should cover the standardized data format, the process to submit new datasets for review, and any quality control measures that contributors need to be aware of.

5. **Benchmark Results**: While the paper discusses the creation and utility of Meta-Album, including benchmark results for several popular few-shot learning algorithms, for a more comprehensive contribution, consider including more extensive benchmarks covering a variety of few-shot learning, meta-learning, and transfer learning techniques. This dataset represents a new challenge for the community; comprehensive baselines across these different areas will underscore its value.

6. **Licensing and Privacy**: The paper does well to mention the licensing for each contributing dataset. Given the diversity of domains and sources, it’s advisable to also comment on any privacy considerations, especially for datasets containing potentially sensitive information.

7. **Community Engagement**: Finally, considering the ambition to make Meta-Album a continually growing resource, outline plans for community engagement, maintenance of the dataset and code repository, and possible regular updates or competitions based on Meta-Album.

Enhancing the paper with these suggestions will significantly aid in ensuring usability, fostering community contributions, and highlighting the value of Meta-Album for a wide range of machine learning research focused on image classification tasks in few-shot and meta-learning settings.

",1
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","Discrepancies: No discrepancies were found between the authors' justification and the actual paper content regarding data and code availability. The authors have clearly stated the availability of code and data in public repositories and their website, which is consistent with the paper content. They also provided details for contacting them for any queries related to the meta-dataset Meta-Album, which enhances the reproducibility of their experimental results.

Feedback: The paper presents a comprehensive introduction of the Meta-Album, a multi-domain meta-dataset designed for few-shot, cross-domain, and meta-learning research. Here are several suggestions to further improve the quality of the paper:

1. **Enhanced Dataset Description**: While the paper provides a detailed description of the Meta-Album, including the diversity of its datasets and the preprocessing involved, it would benefit from a comparison section that highlights the unique advantages of Meta-Album compared to existing benchmarks in few-shot and cross-domain learning. Such a section could help readers better understand the specific niches the dataset fills in current research.

2. **Experimental Evaluation**: The paper showcases the utility of Meta-Album through several experiments and baselines. However, it would strengthen the contribution to include more details on the experimental setup, such as hyperparameters used, for the sake of complete reproducibility. Additionally, discussing the limitations of the proposed dataset and possible future extensions would provide a more balanced view.

3. **Benchmarking Infrastructure**: Considering the dataset's scope and intent to encourage community contributions, elaborating on the infrastructure for benchmarking submitted models (if any) would be useful. For instance, details on a public leaderboard, submission protocols, and evaluation criteria could incentivize more active participation from the research community.

4. **Applications and Use Cases**: While the paper outlines the suitability of Meta-Album for various ML tasks, including few-shot learning and transfer learning, providing more concrete examples or case studies of such applications could illustrate the dataset's practical utility. Highlights of any preliminary work or ongoing research projects using Meta-Album would be particularly insightful.

5. **Ethical Considerations and Bias**: The paper would benefit from a more detailed examination of ethical considerations, especially related to data sourcing, privacy, and consent. Given the diverse domains covered by the dataset, a discussion on potential biases (cultural, geographic, etc.) and measures taken to mitigate them would be valuable.

6. **Community Engagement and Growth**: The paper mentions the potential for Meta-Album to be expanded with new datasets. It would be helpful to outline specific plans or mechanisms for engaging with the community to facilitate this growth. For example, guidelines for submitting new datasets, quality control measures, and a review process for integration could clarify how researchers can contribute.

In summary, the Meta-Album dataset represents a significant contribution to the field of few-shot learning and related areas. Addressing the above points could enhance the paper's clarity, impact, and usability for the broader research community.

",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","Discrepancies: No, there are no discrepancies between the authors' justification and the actual paper content. The paper does provide comprehensive details about the experiments conducted, including the description of the Meta-Album dataset, how it was curated and preprocessed, the settings for both within and cross-domain few-shot learning experiments, hyperparameter settings, and the division of data into different splits for training, validation, and testing. Additional details are promised to be found in the provided GitHub repository, which aligns with the guidance provided to authors on including detailed training and testing specifics.

Feedback:
The submission presents an extensive and well-documented dataset aimed at advancing research in few-shot learning, meta-learning, and domain adaptation. Here are some recommendations to further enhance the paper:

1. **Clarify Selection Criteria**: Elaborate more on the criteria used for selecting the 40 open datasets included in Meta-Album. While the paper mentions diversity in domains and a minimum number of classes and examples per class, providing insights into why specific datasets were chosen over others could offer a better understanding of the dataset composition.

2. **Expanded Baseline Methods**: While the paper provides an excellent initial evaluation using popular meta-learning algorithms, expanding the set of baseline methods to include more recent advancements or domain-specific approaches could offer richer insights into the utility and challenges of the Meta-Album dataset.

3. **Analysis on Data Preparation Artifacts**: The paper mentions using GradCAM to detect potential biases introduced by preprocessing. A deeper dive into these findings and their implications on model training and evaluation can provide valuable information. Specifically, it would be beneficial to discuss any observed artifacts and how they might affect the learning process or skew the evaluation of models trained on Meta-Album.

4. **Detailed Hyperparameter Settings**: To improve reproducibility, include a more detailed exposition of the hyperparameter settings, especially for the baseline models tested. If these are included in the GitHub repository, explicitly mentioning this fact in the paper will guide the reader accordingly.

5. **Data Versioning**: Since Meta-Album is designed to be extendable, introducing a versioning system for the dataset could help researchers track changes, additions, and improvements over time. This would aid in ensuring reproducibility and consistency in research using the dataset across different versions.

6. **Extended Evaluation on Cross-Domain Generalization**: While initial evaluations are provided, a more comprehensive exploration of cross-domain generalization capabilities, potentially through more diverse tasks or challenges, could further highlight the strengths and limitations of current few-shot learning and meta-learning algorithms when dealing with heterogeneous datasets.

7. **Community Contributions**: Given the extendable design of Meta-Album, it might be helpful to outline a clear and structured process for the community to contribute new datasets or algorithms. This could include submission guidelines, quality control steps, and criteria for inclusion to ensure the continued growth and relevance of Meta-Album.

By addressing these recommendations, the paper can significantly contribute to the few-shot learning and meta-learning research community, providing a challenging and versatile benchmark that stimulates advancements in the field.

",1
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","Discrepancies: No discrepancies were found between the authors' justification and the actual paper content regarding the reporting of error bars and statistical significance. The paper includes adequate statistical details such as confidence intervals for the experimental results supporting its main claims, adhering to the guidelines.

Feedback:
The authors have done a commendable job in providing statistical significance details, such as error bars and confidence intervals, for their experimental results, which is crucial for the reproducibility and reliability of research findings. To further enhance the paper's quality and readability, the authors may consider the following suggestions:
1. **Clarify Variability Factors**: Although the paper mentions error bars and confidence intervals, explicitly stating the sources of variability captured (e.g., initialization, data splits) in each experiment would provide clearer insights into what the reported statistics represent. 
2. **Methodology Description**: While it's evident that statistical analyses were conducted, a more detailed description of the methodology used to calculate the error bars or confidence intervals would add value. For example, specifying if bootstrap methods were used or if these are standard errors of the mean would help readers understand the robustness of the conclusions drawn from the data.
3. **Assumptions and Limitations**: Discuss any assumptions made during the statistical analysis, such as the normality of the data distribution. Additionally, including a discussion on the limitations of the current statistical methodology and its impact on the results could provide a more comprehensive view of the research findings.
4. **Asymmetric Distributions**: If there are cases of asymmetric error distributions within the data, considering the presentation of asymmetric confidence intervals or error bars might offer a more accurate representation of the underlying data variability.
5. **Extending Statistical Analysis**: If not already included, incorporating additional statistical tests (e.g., comparing algorithms using paired t-tests or non-parametric alternatives if assumptions of parametric tests are not met) could strengthen the claims made and help identify statistically significant differences between methods.

Adopting these suggestions could improve the manuscript by providing a more detailed statistical foundation for the claims made, enhancing the reader's ability to critically assess and potentially reproduce the work.

",1
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","Discrepancies: No substantial discrepancies were observed between the authors' justification and the content detailed in the main document and appendices concerning the computational resources utilized in their experiments.

Feedback:
The authors have provided detailed information regarding the computational resources used in their experiments, including mentioning the use of GPUs (e.g., PNY GeForce RTX 2080TI and NVIDIA V100), which is commendable. However, to adhere more closely to NeurIPS guidelines and enhance the reproducibility of the experiments, I recommend the authors to include additional details:

1. **Cloud vs. Local Resources**: Clarify whether the experiments were conducted on a local cluster or cloud resources. If cloud resources were used, specify the service provider (e.g., AWS, Google Cloud, Azure).

2. **Memory Specifications**: Include the memory size for the GPUs used in the experiments. While the type of GPUs is mentioned, the memory size is critical for replication, particularly when working with large datasets or complex models.

3. **Execution Time**: Provide more detailed information on the execution time for each experimental setup. Currently, the paper mentions that runs took ""at most 24 hours"" on the GPUs used. For greater clarity, report the average and range of execution times for key experiments, especially the few-shot learning tasks in different settings (within domain, cross-domain, and any-way any-shot).

4. **Resource Usage for Different Phases**: The paper could benefit from a breakdown of compute resources required for different phases of experiments, such as training, validation, and testing. This information would help researchers with limited resources to plan their experiments accordingly.

5. **Batch Size and Memory Constraints**: Discuss how the choice of the batch size was influenced by the memory constraints of the GPUs used. This detail is valuable for readers attempting to reproduce the results on hardware with different memory capacities.

6. **Preliminary or Failed Experiments**: Acknowledge if any preliminary or failed experiments were conducted that required additional compute resources beyond what was reported for the final experiments. This acknowledgment will provide a more comprehensive view of the computational cost associated with the research project.

By addressing these points, the authors can significantly enhance the reproducibility of their work and align with NeurIPS guidelines on reporting computational resources.

",0.5
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We completely comply with the NeurIPS Code of Ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","Discrepancies: No discrepancies found between the justification provided by the authors and the actual paper content. The authors state adherence to the NeurIPS Code of Ethics, and there is no indication in the paper content that contradicts this statement.

Feedback: The authors have presented Meta-Album, an extensive and diversified meta-dataset aimed at advancing research in few-shot learning, meta-learning, and potentially other machine learning domains. To further improve the quality and contribution of the work, consider the following suggestions:

1. **Broader Impact and Ethical Considerations**: Expand the discussion on the potential societal impact of Meta-Album, especially considering its application in few-shot learning. Discuss both the positive and potentially negative implications. Additionally, consider elaborating on the dataset collection process with respect to privacy, consent, and ethical use, especially for datasets obtained from the internet or those that may contain human subjects.

2. **Dataset Diversity and Representation**: The paper effectively highlights the diversity of domains within Meta-Album. However, further discussion on the representativeness of these domains across different geographies, cultures, and demographics could enhance the understanding of the dataset's applicability and limitations in global contexts. If certain areas are underrepresented, acknowledge these limitations and propose future steps to address them.

3. **Benchmarking and Baselines**: While the paper provides a comprehensive analysis and comparison with existing meta-learning algorithms, it would be beneficial to include a wider range of baseline methods, possibly from related domains such as transfer learning or zero-shot learning. This comparison could provide deeper insights into the unique challenges and opportunities presented by Meta-Album.

4. **Dataset Accessibility and Usability**: Ensure that the datasets and the accompanying codebase are easily accessible and well-documented for the broader research community. Providing clear guidelines, examples of usage, and support for common machine learning frameworks can significantly increase the dataset's impact.

5. **Future Directions**: Outline specific research questions or challenges that Meta-Album is uniquely positioned to address. Encourage the community to explore not only algorithmic advances in few-shot learning but also issues like dataset bias, fairness, and robustness across domains. Highlighting these avenues can inspire further innovative uses of the dataset.

By addressing these aspects, the paper can strengthen its contribution to the machine learning community, providing a resource that not only benchmarks the state-of-the-art in few-shot learning but also opens new pathways for research in ethically responsible and socially beneficial ways.

",1
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","Discrepancies: No discrepancies were found between the justification provided by the authors and the actual paper content regarding the societal impacts. The authors correctly noted that the paper does not directly address negative societal impacts but highlighted the positive implications of their work in facilitating research in few-shot learning and meta-learning.

Feedback:
1. **Clarification on Societal Impacts**: While the authors confidently state there are no negative societal impacts and emphasize the positive facets of their work, it would enhance the paper’s comprehensiveness to discuss more explicitly any hypothetical misuse or unintended consequences of their work. For instance, advances in few-shot learning could potentially accelerate the development of AI systems with capabilities that could be misused if not properly governed or regulated. A brief discussion on how the community could preemptively address such concerns, even if they seem far-removed from the current research, would provide a balanced view on societal implications.

2. **Potential for Bias**: Given that the Meta-Album dataset compiles various domains, including those with potential socio-cultural nuances (such as human actions), the paper could benefit from a discussion on the steps taken or recommended to mitigate biases in the dataset compilation and usage. This is particularly pertinent given the wide application scope of few-shot learning in areas like surveillance, content filtering, and personalization, where biases could have significant implications.

3. **Accessibility and Inclusivity**: The paper would be enhanced by discussing how the Meta-Album, through its design and openness for expansion, promotes inclusivity and accessibility within the research community. For instance, stating explicit ways in which the dataset can support underrepresented domains or lower-resource institutions could underscore the paper’s alignment with positive societal impacts.

4. **Data Privacy and Protection Measures**: Although the paper may not directly deal with sensitive or personal data, considering that the dataset aggregates from various sources, including potentially user-generated content, addressing data privacy, and user consent would reinforce the paper’s ethical considerations. This includes any measures or guidelines in place for contributors regarding data privacy and how the dataset ensures compliance with global data protection regulations.

5. **Future Work on Societal Impact**: The paper could outline future research directions, not only in technical advancements but also in studying the societal impact of such datasets and algorithms. For example, exploring how the dataset could be used to foster the development of AI systems that positively contribute to societal challenges could spotlight the broader relevance of the research.

Overall, the paper introduces an important resource for the machine learning community that could foster significant advances in few-shot learning and meta-learning. Addressing the points above could broaden the paper’s perspective on its societal impacts, reinforcing the positive contributions while acknowledging and mitigating potential negative implications.

",0.5
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Yes,"Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","Discrepancies: No discrepancies identified. The paper thoroughly documents its efforts to address responsible release and proper use of its datasets, consistent with the provided justification.

Feedback:
The paper does a commendable job in presenting Meta-Album, a large and diverse meta-dataset aimed at facilitating research in few-shot learning and meta-learning. The efforts to ensure responsible release through proper licensing and recommendations for use are praiseworthy. However, to further align with NeurIPS's high standards, a few enhancements and clarifications could be valuable:

1. **Clarify Dataset Collection and Ethical Considerations**: While the paper mentions licenses and proper use, it could benefit from a more detailed explanation of the dataset collection process, particularly focusing on ethical considerations. For example, explicitly mentioning any steps taken to ensure that the collected data does not contain personally identifiable information or sensitive content would be helpful.

2. **Explain Data Diversity and Inclusion Criteria**: Given the emphasis on cross-domain applications, elaborating on the diversity of the data and your criteria for selecting datasets for inclusion in Meta-Album would make the paper stronger. Discuss any efforts made towards ensuring that the datasets are representative of diverse scenarios and do not perpetuate biases.

3. **Detail Safeguards Against Misuse**: While the paper mentions safeguards for responsible release, providing specific examples of these safeguards in practice would enhance the reader's understanding. For instance, describe any technical or procedural barriers implemented to prevent misuse of the data or models trained on it, especially in sensitive applications.

4. **Engagement with the Wider Community**: The paper could expand on plans for engaging with the broader research and ethical community to continually assess and update the responsible use guidelines. This ongoing engagement is crucial for adapting to emerging ethical standards and addressing unforeseen misuse or ethical issues.

5. **Impact Assessment**: A forward-looking discussion on how the authors plan to monitor and evaluate the impact of Meta-Album on both the research community and broader societal aspects would be highly informative. This could include plans for gathering feedback from users of the dataset and how such feedback might inform future updates or revisions.

By addressing these points, the paper could significantly strengthen its contribution to the field, ensuring that Meta-Album not only advances few-shot learning research but does so with a keen eye on responsible data practices and ethical considerations.

",0.5
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","Discrepancies: None.

Feedback:
The authors have provided a comprehensive response regarding the crediting and licensing of assets used within their paper, which includes data from 40 different datasets amalgamated into the Meta-Album. They have diligently mentioned the licenses of individual datasets in Appendix B, ensuring compliance with the respective terms of use. Furthermore, they have outlined the open-source nature of their contribution to the community, emphasizing the extendibility and open accessibility of their meta-dataset, which is commendable.

However, to further enhance transparency and reproducibility, the authors could consider including the following in their revised version or supplementary materials:

1. **Versioning Details**: While the authors have mentioned the source websites and general license information, detailing the specific version(s) of each dataset used (if applicable) would help in ensuring reproducibility. Given the dynamic nature of some datasets (for example, the Birds dataset which has expanded since its initial release), specifying versions could be crucial for exact replication in future research.

2. **Derivative Works**: The paper indicates that some datasets were preprocessed or reformatted for integration into Meta-Album. It would be beneficial to explicitly mention if the preprocessing modifies the licensing requirements or necessitates additional permissions from the original dataset creators.

3. **Consolidated Licensing Statement**: Given the large number of datasets integrated into Meta-Album, the paper could benefit from a consolidated statement or table summarizing the licensing for quick reference. This summary could highlight any datasets with more restrictive licenses that might limit the use of Meta-Album or require special attention from users.

4. **Guidance on Ethical Use**: Beyond licensing, offering guidance on the ethical use of these datasets, especially regarding potentially sensitive imagery or the need for fairness and bias consideration in downstream applications, would add value to the paper. This is particularly relevant given the broad domain coverage of Meta-Album.

5. **Clarification of Contributions**: The authors should clarify the nature of their contribution regarding the dataset integration process—particularly, how they ensured adherence to the original datasets' intent and ethical use guidelines, and whether any dataset creators have been consulted or involved in this integration effort.

By addressing these points, the paper would not only enhance its adherence to NeurIPS standards but also serve as a valuable resource that encourages responsible use and further innovation in the research community.

",0.5
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","The provided paper presents Meta-Album, a comprehensive and well-documented meta-dataset designed to facilitate research in few-shot learning, meta-learning, and other related machine learning paradigms. The authors have made significant efforts to ensure the new assets introduced are well-documented, as evidenced by the extensive appendix and the provision of a dedicated website and GitHub repository containing all necessary information, data formats, and tools to utilize, reproduce, and potentially contribute to the Meta-Album meta-dataset.

However, while the paper extensively details the creation, structure, and intended usage of Meta-Album, there are areas where the documentation and justification could be further enhanced to meet NeurIPS's high-quality standards fully:

1. **License Information Clarification**: The paper provides a comprehensive table listing the licenses under which each dataset within Meta-Album is released. This is a critical aspect of dataset documentation that facilitates proper and legal usage by the research community. However, the paper could benefit from a centralized summary or discussion about the implications of these varied licenses for users who might wish to use Meta-Album in its entirety or in part. Specifically, it would be helpful to discuss any potential complications or considerations researchers need to keep in mind given the mix of licenses, and how these might impact the usage of Meta-Album for commercial purposes, dataset extension, or derivative work.

2. **Dataset Extensibility and Contribution Guidelines**: While the paper mentions that Meta-Album is designed to be extended and invites the community to contribute more domains and datasets, the provided documentation should clarify the review process for contributed datasets. Detailed guidelines on the criteria used to review and accept new datasets, including any ethical considerations, data quality standards, and domain diversity objectives, would enhance transparency and encourage contributions.

3. **Ethics and Fair Use**: Given the variety of sources from which the datasets have been compiled, a more in-depth discussion on ethical considerations, particularly concerning privacy, bias, and fairness, would be valuable. Though the camera-ready version might anonymize datasets, the final version should ensure that all datasets comply with ethical standards, including consent where applicable, especially in domains involving humans or potentially sensitive information.

4. **Limitations and Biases**: The paper briefly mentions efforts to identify and mitigate biases in the datasets through quality control measures like GradCAM visualizations. However, discussing known limitations and biases in more detail would further strengthen the paper. Acknowledging potential shortcomings in representation within current domains, any systemic biases identified during audits, and how these might affect the generalizability of models trained on Meta-Album would be insightful.

Addressing these recommendations can enrich the paper's contribution and utility for the research community, ensuring that users can effectively leverage Meta-Album while being fully informed of its scope, limitations, and ethical considerations.

",0.5
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This does not apply on our research as we are not doing any crowdsourcing experiments,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","Discrepancies: No discrepancies found between the author's justification and the actual paper content regarding crowdsourcing experiments and research with human subjects. The paper focuses on introducing Meta-Album, a multi-domain meta-dataset for few-shot image classification, and does not involve direct research with human subjects or crowdsourcing for data collection or annotations.

Feedback:
1. **Dataset Description and Accessibility:** Enhance the dataset description section by elaborating on the specific challenges and advantages each domain within Meta-Album provides. While the paper provides a comprehensive overview, offering more detailed examples or case studies from each domain could give readers better insights into the dataset's potential applications.

2. **Baseline Method Explanations:** The explanations for the baseline methods used in the experiments (Prototypical Networks, MAML, etc.) are sufficient, but including more detailed discussions on why these were selected over other potential methods could give readers a more comprehensive understanding of their relevance to the challenges posed by Meta-Album.

3. **Further Experimentation:** While the paper provides a solid foundational analysis using few-shot learning experiments, future work could include more diverse experiments. For example, testing the dataset with the latest few-shot learning methods or incorporating unsupervised or semi-supervised learning approaches could further demonstrate the dataset's utility and robustness.

4. **Extension and Contribution:** The paper mentions Meta-Album's design for extensibility by the community. It would be beneficial to provide a detailed guideline or protocol for contributions, including the quality and diversity standards for new datasets. Highlighting successful community contributions in future iterations could encourage more active participation and growth of the dataset.

5. **Comparison with Similar Datasets:** While the paper compares Meta-Album with other datasets, expanding this analysis to include more direct comparisons in terms of performance impacts on specific few-shot learning methods could be insightful. This would help in understanding Meta-Album's unique position and contributions compared to existing datasets.

6. **Ethical and Privacy Considerations:** Given the dataset amalgamates various domains, including a discussion on ethical considerations, data privacy, and potential biases within the datasets could be beneficial. While not directly related to crowdsourcing or human subject research, these considerations are crucial for datasets used in machine learning research.


The paper introduces an invaluable resource for the machine learning community, especially for those focusing on few-shot learning across diverse domains. Enhancements to the dataset description, baseline method explanations, and the inclusion of more varied experiments and ethical considerations could further bolster the paper's contribution.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This does not concerns us as we are not dealing with human subjects in our research.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","Discrepancies: There are no apparent discrepancies between the justification provided by the authors and the content of the paper regarding the handling of human subjects or crowdsourcing efforts. The paper focuses on introducing a new image classification meta-dataset, Meta-Album, which does not directly involve research with human participants. Therefore, the N/A response to the question of IRB approvals is consistent with the study's nature.

Feedback: The paper introduces Meta-Album, a comprehensive and extensible multi-domain meta-dataset designed for few-shot learning, meta-learning, and other related tasks. This is an ambitious and valuable contribution to the field of machine learning, especially for researchers working on few-shot learning and model generalization across diverse domains. However, there are several areas where the paper could be improved:

1. **Clarity on Dataset Collection and Licensing:** The paper should offer clearer information on the methodology used for collecting the various datasets included in Meta-Album. Specifically, details on the process of verifying licences for each dataset should be provided to ensure that Meta-Album complies with all legal and ethical standards for dataset usage and redistribution.

2. **Extensibility:** While the paper mentions that Meta-Album is designed to be continually extended with new datasets and domains, it would be beneficial to provide a more detailed explanation of how external contributors can add to Meta-Album. Include concrete steps, guidelines, and criteria for dataset inclusion to ensure consistency and quality across the meta-dataset.

3. **Bias and Fairness Considerations:** Though the paper briefly mentions the efforts to avoid introducing bias during the preprocessing steps, it would be advantageous to delve deeper into this topic. Expounding on how Meta-Album addresses issues of bias and fairness, not only in terms of image preprocessing but also in the selection of datasets and their domains, would greatly enhance the paper's contribution to ethical AI research.

4. **Comparative Analysis:** The paper provides a comparison with existing datasets and benchmarks, which is valuable. However, it would be helpful to include more direct comparisons of Meta-Album's performance impact on few-shot learning algorithms versus other benchmarks. Providing empirical evidence on the advantages of using Meta-Album for few-shot learning and meta-learning tasks would strengthen the paper's claims about its utility and significance.

5. **Usage Examples and Baselines:** Expand the section on use cases and baselines by including more examples of how Meta-Album has been used to achieve state-of-the-art results or to uncover new insights in few-shot learning. This could include case studies or detailed experiments that highlight Meta-Album’s unique contribution to advancing the field.

6. **Dataset Quality Control:** More information on the continuous quality control measures for Meta-Album, especially as it grows and evolves with contributions from the community, would be crucial. Discussing how the dataset will be maintained, updated, and curated over time would provide readers with confidence in its long-term viability as a benchmark.

By addressing these points, the authors can significantly enhance the value and impact of the paper, providing the machine learning community with a robust tool for advancing research in few-shot learning and beyond.

",0.5
