<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }
    .container {
        margin: 20px auto;
        padding: 0 20px;
        position: relative;
    }
    .button {
        padding: 10px;
        font-size: 16px;
        text-align: center;
        background-color: #f93361;
        color: #fff;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        text-decoration: none;
        position: absolute;
    }
    .button-top {
        top: 10px;
        right: 20px;
    }
    .button-bottom {
        bottom: 10px;
        right: 20px;
    }
    .button:hover {
        background-color: #bc0530;
    }
    .content {
        padding-top: 60px; /* Adjust according to button height and margin */
        padding-bottom: 40px; /* Add padding instead of margin */
        margin-bottom: 20px;
    }
    h1 {
        margin-top: 0; /* Remove default margin */
        margin-bottom: 30px;
    }
    hr {
        margin-top: 50px;
        margin-bottom: 50px;
    }
    .review {
        margin-bottom: 30px;
        border: 1px solid #ccc;
        padding: 20px;
        border-radius: 5px;
        background-color: #f9f9f9;
    }
    .review h2 {
        margin-top: 0;
    }
    .review p {
        margin: 10px 0;
    }
    .question {
        color: #0033ff;
    }
    .answer {
        color: #28a745;
    }
    .justification {
        color: #de750b;
    }
    .llm_review {
        color: #000;
        background-color: #c3defb;
        border: 1px solid #007bff;
        padding: 20px;
        border-radius: 5px;
    }

    table {
        border-collapse: collapse;
    }
    th, td {
        padding: 8px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }
    th {
        background-color: #f2f2f2;
    }
    .score-label {
        display: inline-block;
        padding: 5px 15px;
        border-radius: 5px;
        text-decoration: none;
    }
    .score-green {
        background-color: #c8e6c9;
        color: #1b5e20;
        border: 1px solid #1b5e20;
    }
    .score-red {
        background-color: #e6c8c8;
        color: #5e1b1b;
        border: 1px solid #5e1b1b;
    }
    .score-blue {
        background-color: #c8d8e6;
        color: #1b455e;
        border: 1px solid #1b455e;
    }
    .scroll-button {
        padding: 10px 20px;
        font-size: 14px;
        color: #212121;
        border: 1px solid #212121;
        border-radius: 5px;
        cursor: pointer;
        text-decoration: none;
    }
    .scroll-button:hover {
        background-color: #212121;
        color: #fff;
    }
    .move-to-top {
        padding: 5px 10px;
        font-size: 12px;
        color: #212121;
        border: 1px solid #212121;
        border-radius: 3px;
        cursor: pointer;
        text-decoration: none;
    }
    .move-to-top:hover {
        background-color: #212121;
        color: #fff;
    }
</style>
</head>
<body>

<div class="container">

    <a class="button button-top" href="https://docs.google.com/forms/d/e/1FAIpQLSfRIDkcXFbsOrR09j4qA1MlG4Rfir2lPD_u9YC4eqKBJ8tHkw/viewform?usp=pp_url&entry.1830873891=TWV0YS1BbGJ1bTogTXVsdGktZG9tYWluIE1ldGEtRGF0YXNldCBmb3JGZXctU2hvdCBJbWFnZSBDbGFzc2lmaWNhdGlvbg==" target="_blank">Click to submit post submission survey</a>

    <div class="content">
        <h1>Meta-Album: Multi-domain Meta-Dataset forFew-Shot Image Classification</h1>

        <hr>

        <h2>Scores</h2>
        <table>
            <tr>
              <td><strong>Correctness Score:</strong></td>
              <td><span class="score-label score-blue">0.73</span></td>
            </tr>
            
            
        </table>

        <hr>

        <h2>Review Summary</h2>
        <table>
            <tr>
              <th>Question</th>
              <th>Score</th>
              <th>Details</th>
            </tr>
            
                
                
                <tr id="summary-Genuine-question-1">
                    <td>1. Claims</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-1" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-2">
                    <td>2. Limitations</td>
                    <td><span class="score-label score-red">0</span></td>
                    <td><a href="#Genuine-question-2" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-3">
                    <td>3. Theoritical assumptions and proofs</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-3" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-4">
                    <td>4. Experiments reproducibility</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-4" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-5">
                    <td>5. Code and data accessibility</td>
                    <td><span class="score-label score-red">0</span></td>
                    <td><a href="#Genuine-question-5" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-6">
                    <td>6. Experimental settings/details</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-6" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-7">
                    <td>7. Error bars</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-7" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-8">
                    <td>8. Compute resources</td>
                    <td><span class="score-label score-red">0</span></td>
                    <td><a href="#Genuine-question-8" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-9">
                    <td>9. NeurIPS code of ethics</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-9" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-10">
                    <td>10. Impacts</td>
                    <td><span class="score-label score-red">0</span></td>
                    <td><a href="#Genuine-question-10" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-11">
                    <td>11. Safeguards</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-11" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-12">
                    <td>12. Credits</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-12" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-13">
                    <td>13. Documentation</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-13" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-14">
                    <td>14. Human subjects</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-14" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
                <tr id="summary-Genuine-question-15">
                    <td>15. Risks</td>
                    <td><span class="score-label score-green">1</span></td>
                    <td><a href="#Genuine-question-15" class="scroll-button">Detailed Review</a></td>
                  </tr>
                
            
        </table>

        <hr>

        <h2>Detailed Review</h2>

        
            
            <p><strong>Correctness Score:</strong> 0.73</p>
        
            
            <div class="review" id="Genuine-question-1">
                <p><strong class="question">Question # 1:</strong> Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> The abstract and introduction clearly state the main contributions and scope of the paper</p>
                <p class="llm_review"><strong>Review:</strong> The given answer matches the content of the paper, and the justification aligns with the guidelines provided. The abstract and introduction of the paper state its primary contributions, namely the introduction of the Meta-Album, a meta-dataset designed to facilitate research in few-shot learning, transfer learning, and meta-learning among others. They accurately describe the scope, including the diversity and scale of the included datasets, the preprocessing and formatting efforts, and the extendability of Meta-Album through community contributions. Furthermore, the paper details the design and utility of Meta-Album, showcasing experimental evaluations that support the claims made early on.<br><br>However, for the sake of thoroughness and in alignment with the critical perspective requested, here are some constructive suggestions:<br><br>1. The paper could benefit from a more detailed discussion of the challenges faced during the development of Meta-Album and how these challenges were addressed. This would give readers a clearer understanding of potential limitations and areas for future improvement.<br><br>2. While the paper does mention that Meta-Album includes various domains like ecology, manufacturing, and human actions, it could elaborate on the selection criteria for these domains and datasets. This would help readers understand why certain domains were included and how they contribute to the broader goals of facilitating cross-domain learning.<br><br>3. The paper mentions the utility of Meta-Album for different machine learning tasks but could further clarify the specific advantages or features that make it particularly well-suited for these tasks compared to other existing datasets or benchmarks.<br><br>4. Finally, as part of discussing the scope and contributions, it would be useful to have a more explicit discussion regarding the limitations of Meta-Album and any known biases in the datasets. This transparency would help researchers in assessing the applicability and relevance of Meta-Album to their specific research questions.<br><br>Overall, the paper does a good job of reflecting its contributions and scope in the abstract and introduction, and these suggestions are meant to further strengthen the presentation and utility of the Meta-Album meta-dataset.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-1">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-2">
                <p><strong class="question">Question # 2:</strong> Does the paper discuss the limitations of the work performed by the authors?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> We have clearly explained the limitations of our work in Section: Discussion and conclusion</p>
                <p class="llm_review"><strong>Review:</strong> Justification Review:<br>The provided justification states that the limitations are clearly explained in a specific section titled "Discussion and Conclusion." However, upon reviewing the extensive details shared in the paper's content summary, there is no explicit mention or dedicated discussion on the limitations of the work undertaken by the authors. The absence of explicit mention of the "limitations" in the content provided makes it difficult to assess the validity of the justification. A well-documented discussion on limitations is crucial for understanding the scope, reliability, and applicability of the research findings.<br><br>Constructive Suggestions:<br>1. Explicit Section on Limitations: If not already included, authors should consider adding a specific subsection within the "Discussion and Conclusion" section that directly addresses the limitations of their study. This subsection should discuss any constraints or areas where the research findings may not apply.<br><br>2. Assumptions and Robustness: Authors should identify any strong assumptions made during the research and discuss how robust their findings are to the violations of these assumptions. This will help in understanding the boundaries within which the research findings hold true.<br><br>3. Scope of Claims: Reflect on the scope of the claims made in the paper. If the research is tested only on a limited number of datasets or scenarios, this should be clearly stated as a limitation.<br><br>4. Performance Influencing Factors: Discuss factors that could potentially influence the performance of the proposed methods or findings. For instance, if the research is context-dependent, the limitations of its applicability in different contexts should be articulated.<br><br>5. Computational Efficiency: If not already discussed, elaborating on the computational efficiency and scalability of the proposed approaches with varying dataset sizes could provide important insights into their practical applicability.<br><br>6. Privacy and Fairness Concerns: If applicable, discuss any potential limitations of the approach in addressing privacy and fairness issues. This is increasingly critical in AI and Machine Learning research domains.<br><br>7. Transparency and Honesty: Encourage transparency by thoroughly acknowledging the limitations of the study. A forthright discussion of the limitations not only adds credibility to the research but also provides valuable insights for future work aiming to address these limitations.<br><br>By addressing these points, the authors would significantly strengthen the quality and integrity of their paper, making it clear to the readers and reviewers the context and framework within which the research findings are most applicable and reliable.<br></p>
                <p><strong>Score:</strong> 0</p>
                <a class="move-to-top" href="#summary-Genuine-question-2">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-3">
                <p><strong class="question">Question # 3:</strong> For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
                <p><strong class="answer">Answer:</strong> NA</p>
                <p><strong class="justification">Justification:</strong> This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset</p>
                <p class="llm_review"><strong>Review:</strong> The authors' justification for not providing theoretical results, proofs, or assumptions is acceptable and consistent with the content and nature of the paper. Since the primary focus of the paper is on the introduction and evaluation of a meta-dataset named Meta-Album, it is inherently practical rather than theoretical. This work involves data curation, dataset description, experimental protocols, and benchmark comparisons rather than theoretical exploration or the derivation of formulas and theorems. Hence, the question about providing the full set of assumptions and complete proofs is not applicable to this study.<br><br>However, for completeness and to ensure clarity for readers, it's important for the paper to clearly state the scope and nature of the research early on, which it does by explicitly mentioning that the work deals with meta-dataset creation and experimental evaluations using this meta-dataset. This clarification helps to set the right expectations for the readers regarding the type of contributions and content they will find in the paper.<br><br>Constructive suggestions for the authors:<br>1. Ensure that the introduction or methodology sections clearly articulate the applied research nature of the work to preemptively address questions related to theoretical aspects.<br>2. When discussing the experimental setups, results, or findings, consider providing detailed explanations or references to the methods used for analysis and evaluation to compensate for the lack of theoretical content. This will add depth to the readers' understanding of how the meta-dataset and its evaluation were conducted.<br>3. Since the paper involves the creation and utilization of a new meta-dataset, it would be beneficial to include a discussion on the implications, potential use cases, and future research directions enabled by Meta-Album. This can help stimulate theoretical investigations by others in the community who might use Meta-Album for developing new algorithms or theories.<br>4. It might also be useful to discuss any limitations or challenges encountered during the dataset creation process and experimental evaluations to inform future work in this area.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-3">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-4">
                <p><strong class="question">Question # 4:</strong> Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> We have publicly provided the code on our github repository and details about how to access the datasets on our website.</p>
                <p class="llm_review"><strong>Review:</strong> The answer provided by the authors suggests they have made efforts to ensure reproducibility by publicly providing the code on their GitHub repository and detailing dataset access on their website. This is a positive step towards meeting the reproducibility criterion as outlined in the guidelines. Given this information, it appears that the authors are compliant with the NeurIPS policy on code and data submission, making the paper’s experimental results potentially reproducible. However, the justification could be improved by specifying whether instructions for environment setup, exact commands for reproducing results, and details on data preparation and access (including raw, preprocessed, and intermediate data) are provided as part of the repository's documentation.<br><br>Suggestions for Improvement:<br>1. Detailed Reproducibility Instructions: Ensure that the GitHub repository contains a README file or documentation that includes detailed instructions for setting up the environment, including any dependencies or specific versions of software/libraries required. It should also provide the exact commands needed to run the experiments and reproduce the results.<br><br>2. Data Access and Preparation: Clarify the process for accessing and preparing the data. This includes instructions on downloading the raw data, steps for preprocessing, and how to generate any intermediate data used in the experiments. If any data cannot be shared due to privacy or license restrictions, explicit instructions on how to simulate or acquire similar datasets should be provided.<br><br>3. Baseline and Proposed Method Scripts: Ensure scripts for both the proposed method and any baseline comparisons are included. Clearly indicate which scripts correspond to which results in the paper. If any experiments or results are not reproducible with the provided scripts, specify this explicitly and explain why.<br><br>4. Environment Details: Provide a requirements.txt file or an environment.yml file for setting up a Python environment with the necessary dependencies. For non-Python environments, provide equivalent documentation. Consider using containerization (e.g., Docker) to further ensure reproducibility across different machines and platforms.<br><br>5. Anonymization for Submission: If the supplementary material or repository contains any information that could compromise the double-blind review process, ensure it is adequately anonymized.<br><br>6. Supplemental Material: If possible, include additional supplemental material that supports the experimental results, such as additional figures, tables, or analysis not included in the main paper due to space constraints.<br><br>Implementing these suggestions can significantly enhance the reproducibility of the paper's main experimental results and align with best practices for scientific reporting.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-4">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-5">
                <p><strong class="question">Question # 5:</strong> Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> We have publicly provided the code on our github repository and details about how to access the datasets on our website.</p>
                <p class="llm_review"><strong>Review:</strong> The given justification mentions the provision of the code on a GitHub repository and details about accessing datasets on a website. However, it lacks specificity regarding the exact steps or commands needed to reproduce the main experimental results, which is a critical aspect as per the guidelines provided for NeurIPS submissions. There is no mention of whether environmental dependencies, data preparation instructions, or scripts for reproducing experimental results are provided, making it difficult to assess the reproducibility of the work based on the provided response alone.<br><br>Constructive and actionable suggestions for improvement:<br><br>1. Detailed Instructions: Provide a step-by-step guide in the supplemental material or the repository README that includes the exact environment setup, dependencies, data preparation steps, and the commands that need to be executed to reproduce the results.<br>2. Environment Configurations: It would be beneficial to include a requirements.txt file or a Dockerfile in the code repository to ensure that the code runs in an appropriate environment with all necessary dependencies installed.<br>3. Script for Experiments: Ensure that there are scripts available that specifically reproduce the figures, tables, and any other key findings reported in the paper. Clearly document which script corresponds to which result in the paper.<br>4. Access and Preparation of Data: Provide clear guidelines on how to access both raw and preprocessed versions of the data used in the experiments. If there are any steps required to preprocess the data further, include scripts or instructions for these steps as well.<br>5. Explain Omissions if Any: If certain experiments or results cannot be reproduced with the provided code or data, explicitly state these limitations and provide a rationale. This adds to the transparency and credibility of the submission.<br>6. Anonymity Preservation: If applicable, ensure that the code and data are shared in a way that preserves anonymity during the review process, following the conference guidelines.<br><br>By addressing these points, the authors can significantly improve the reproducibility of their work, aligning with the NeurIPS submission guidelines and making it easier for peers and reviewers to validate and build upon the research findings.<br></p>
                <p><strong>Score:</strong> 0</p>
                <a class="move-to-top" href="#summary-Genuine-question-5">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-6">
                <p><strong class="question">Question # 6:</strong> Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.</p>
                <p class="llm_review"><strong>Review:</strong> The answer is consistent with the content provided in the paper. The paper mentions the experimental settings in Section 3.2, where the authors give a brief overview of how they conducted their experiments, including references to any-way any-shot settings and cross-domain few-shot learning experiments. The mention of the Meta-Album GitHub repository suggests that additional details, such as data splits, hyperparameters, and the type of optimizer, are available outside the main body of the paper, which aligns with the guidelines provided for answering this question.<br><br>However, to enhance the reproducibility and readability of the paper, the following itemized suggestions are provided:<br><br>1. Explicit Mention of Key Experimental Details: While the paper refers to the additional details being available in the GitHub repository, including explicit mentions of key experimental setup within the paper would improve accessibility for readers not keen or able to dig through external resources. This includes hyperparameters, optimizer types, and data split ratios.<br><br>2. Justification for Choices Made: Providing a brief justification for the choice of hyperparameters, data splits, and the type of optimizer directly in the paper could significantly increase the understanding and trust in the experimental setup.<br><br>3. Version Control and Persistence: Ensure that the GitHub repository is well-maintained and versions of the code and data that were used for the experiments in the paper are easily accessible and clearly marked. This ensures readers can replicate the results even if the project evolves.<br><br>4. Direct Linking: Where possible, include direct links to specific scripts or notebooks in the GitHub repository that correspond to the experiments described in the paper. This direct linkage would greatly ease the process of replication and understanding of the experiments.<br><br>5. Supplementary Material: If space or format constraints in the paper limit the amount of detail that can be shared, consider providing supplementary material or an appendix that goes into greater detail on the experimental setups. This can be a middle ground between the main content of the paper and external repositories.<br><br>Overall, while the answer provided by the authors meets the basic requirement of the question by directing readers to the GitHub repository for more information, incorporating these suggestions could significantly enhance the reproducibility and comprehensibility of the research presented.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-6">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-7">
                <p><strong class="question">Question # 7:</strong> Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars</p>
                <p class="llm_review"><strong>Review:</strong> The justification provided by the authors states that error bars are reported in Figure 2, Figure 3, and Appendices E and F, which is in accordance with the question's requirements for indicating statistical significance or variability in the experimental results. However, the justification lacks specific details about the nature of these error bars (e.g., whether they are standard deviations or standard errors, the confidence interval they represent, or the method used for calculating them), which could be critical for thoroughly evaluating the statistical rigor of the report.<br><br>While the inclusion of error bars in the figures and appendices suggests that the authors have considered variability in their experiments, the lack of detailed information about these error bars means that the reader cannot fully assess the statistical significance of the experiments. The guidelines mentioned in the question highlight the importance of not only including error bars but also thoroughly describing their calculation and the assumptions behind them.<br><br>Constructive suggestions for the authors:<br>1. Clarify the type of error bars used in the experiments (e.g., standard deviation, standard error) and the confidence interval they represent.<br>2. Include a brief explanation of the method used for calculating the error bars, whether it's a closed-form formula, a library function, bootstrap, etc.<br>3. Describe any assumptions made during the calculation of the error bars, such as assuming normally distributed errors.<br>4. Make it clear in the text where the error bars can be found (e.g., by referencing specific figures or tables) and provide a brief interpretation of what these error bars indicate regarding the variability and reliability of the results.<br>5. For asymmetrical distributions, consider the appropriateness of symmetric error bars and provide alternative ways to represent the statistical variability if necessary.<br><br>These suggestions would help improve the transparency and comprehensibility of the statistical analysis, thereby allowing readers to more accurately interpret the significance of the research findings.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-7">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-8">
                <p><strong class="question">Question # 8:</strong> For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning</p>
                <p class="llm_review"><strong>Review:</strong> Justification for Score:<br>The authors of the paper claim to have provided details on the compute resources used in the experiments in Section 3.2 and Appendices E and F. However, after careful review, it is clear that the description of the computational resources is not sufficiently detailed to fully satisfy the guidelines for this question. The paper mentions using a "single PNY GeForce RTX 2080TI GPUs with 11GB of VRAM or a single NVIDIA V100 with 16GB of VRAM" for experiments, and that "Each experimental run took at most 24 hours on the former GPU," which provides some insights but lacks the comprehensive details required for full reproducibility. Specifically, the paper does not elaborate on:<br><br>1. The exact type of compute workers (e.g., CPU specifications) beyond the GPU models for all experiments.<br>2. The memory and storage capacity needed, other than VRAM, which is critical for reproducing the experiments, especially considering the size of the datasets involved.<br>3. An estimate of total compute requirement for the entire set of experiments; only a maximum duration for the experimental runs was provided.<br>4. Details on whether any part of the research required more compute resources than reported for the experiments presented in the paper, such as preliminary or failed experiments.<br><br>Constructive Suggestions:<br>1. Detailed Compute Worker Specifications: Future versions of the paper should specify the CPU model, RAM, and other relevant hardware details for the computational environment used, not just the GPU models. This helps in understanding the computational environment needed for reproducing the experiments.<br><br>2. Memory and Storage Requirements: Provide explicit details on RAM and storage requirements for running the experiments. Considering the size of the datasets and the models employed, this information is crucial for reproducibility.<br><br>3. Total Compute Estimation: Offer a clearer breakdown of the compute requirements for each experiment, including GPU hours and any specifics related to parallel or distributed computing, if applicable. This would also include information on the number of experimental runs conducted, which would help in assessing the overall computational expense.<br><br>4. Comprehensive Disclosure of Compute Use: Address any additional computational resources that were utilized for the research project as a whole, including preliminary studies or experiments that were conducted but not included in the final paper. This provides a more accurate picture of the computational cost of the research.<br><br>By addressing these areas, the paper could significantly improve transparency and aid in the reproducibility of the research findings.<br></p>
                <p><strong>Score:</strong> 0</p>
                <a class="move-to-top" href="#summary-Genuine-question-8">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-9">
                <p><strong class="question">Question # 9:</strong> Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> We completely comply with the NeurIPS Code of Ethics</p>
                <p class="llm_review"><strong>Review:</strong> The authors affirmatively state their compliance with the NeurIPS Code of Ethics, which is a necessary declaration for ensuring that the research adheres to accepted ethical standards. This assertion, as provided, meets the conference's requirement for authors to acknowledge and adhere to these guidelines. However, the justification lacks detail about how they conform to these guidelines specifically.<br><br>Constructive and Actionable Suggestions:<br>1. Elaborate on Ethical Compliance: It would be beneficial for the authors to provide a more detailed justification or examples of how their research complies with the NeurIPS Code of Ethics. This could include discussions on data privacy, consent where applicable, fairness in model training, and mitigation of any potential harmful impacts.<br>2. Address Anonymity Maintenance: The guideline stipulates the importance of preserving anonymity. Even though the authors may not need to discuss anonymity in the context of ethical compliance directly, ensuring it is considered in any public data or shared resources related to the paper should be stated or acknowledged.<br>3. Assess Impact and Risks: Consider including a brief section or statement assessing the potential societal impact and risks associated with the research. This could cover both the immediate implications of the meta-dataset introduction and longer-term effects it might have in developing ML models in various domains.<br>4. Data Ethics and Source Transparency: Given that the Meta-Album compiles datasets from multiple domains, a breakdown of ethical considerations for data collection, licensing, and usage permissions for each of these source datasets should be included. This ensures transparency and adherence to ethical standards across all compiled data.<br>5. Accessibility and Inclusiveness: Reflect on the accessibility of the proposed meta-dataset and the tools or algorithms developed from it. Ensure the resources are inclusive and can be used by a diverse set of researchers while upholding the principles of fairness and equality.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-9">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-10">
                <p><strong class="question">Question # 10:</strong> Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.</p>
                <p class="llm_review"><strong>Review:</strong> The justification provided by the authors suggests that there are no negative societal impacts from their work, focusing instead on the positive impacts related to fostering progress in few-shot learning and meta-learning through their meta-dataset. This approach ignores the conference guidelines that require authors to consider both potential positive and negative societal impacts of their work, even if the research is foundational or not tied to particular applications.<br><br>While it's true that the creation and provision of a large-scale meta-dataset could accelerate advancements in machine learning, specifically in few-shot learning and meta-learning, the authors' failure to address potential negative societal impacts is a significant oversight. For example, advancements in few-shot learning could have implications for privacy (by making it easier to identify individuals from limited data), security (by improving the efficiency of malicious AI systems that require fewer data to learn), or fairness (if the datasets included or the methods developed exacerbate existing biases or create new ones).<br><br>Constructive and actionable suggestions for the authors:<br>1. Broader Impact Discussion: Revisit the section discussing societal impacts, acknowledging that while the primary intention and direct impact of the work may be positive, indirect negative impacts could arise. This might include misuse of the technology, privacy concerns, security implications, and bias and fairness considerations.<br>2. Mitigation Strategies: Offer potential mitigation strategies for any identified risks. This could involve recommendations for responsible use, guidelines for data privacy protection, measures to ensure fairness and prevent bias, or suggestions for future research to address these concerns.<br>3. Examples of Misuse: Provide examples or scenarios where the technology could be misused, even if indirectly related to the core research. Explaining how the meta-dataset or the techniques it enables could be misused helps fulfill the guideline requirements and shows a thorough consideration of the research's societal impact.<br>4. Monitoring and Governance: Discuss potential mechanisms for monitoring the use of the meta-dataset and the technologies developed using it. Suggest governance models or community-driven initiatives to ensure the technology's positive application.<br>5. Engage with Ethical Research: Encourage or conduct further ethical research into the implications of few-shot learning and meta-learning advancements, including how these technologies intersect with societal values, norms, and legal frameworks.<br><br>By addressing these points, the authors can provide a more balanced and comprehensive analysis of the societal impacts of their work, in line with the conference's expectations.<br></p>
                <p><strong>Score:</strong> 0</p>
                <a class="move-to-top" href="#summary-Genuine-question-10">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-11">
                <p><strong class="question">Question # 11:</strong> Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.</p>
                <p class="llm_review"><strong>Review:</strong> The answer provided by the authors is consistent with the content of the paper reviewed. The paper indeed states that all datasets have been carefully licensed, with clear outlines on their intended academic use and restrictions on commercial exploitation. Furthermore, the attention to licensing and responsible release is evident from the detailed appendix that lists the licenses for each dataset included in the Meta-Album. This demonstrates a responsible approach towards data release, acknowledging the importance of ethical considerations in sharing datasets. Additionally, the provision of a website and clear contact information for queries related to data usage further supports the responsible dissemination of these resources.<br><br>However, while the authors mention the responsible release and recommended usage, there could be a more explicit discussion on potential risks associated with misuse and specific safeguards beyond licensing. For scraped datasets or those potentially containing sensitive information, detailing any de-identification processes, consent protocols, or privacy assessments conducted would strengthen the responsible release considerations. Moreover, considering the diversity of data domains covered by Meta-Album, a brief analysis of domain-specific risks and how they have been mitigated would be informative.<br><br>Constructive and actionable suggestions for the authors:<br>1. Consider elaborating on any specific privacy or ethical review processes that datasets underwent before inclusion in Meta-Album, especially for those involving human subjects or potentially sensitive contexts.<br>2. Provide a discussion on any potential misuse scenarios you have considered and how the licensing terms or recommended usages aim to prevent such misuse.<br>3. If applicable, detail any content moderation or safety filters that have been put in place for datasets that were compiled from public sources or via web scraping.<br>4. Encourage users of the Meta-Album to engage in ethical research practices by providing guidelines on obtaining necessary permissions for dataset use beyond the provided licenses, especially in cases where further ethical considerations may be necessary (e.g., datasets involving people, potentially sensitive images, etc.).<br>5. Finally, establish a reporting mechanism for misuse or ethical concerns related to the datasets to proactively address any issues that arise post-release.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-11">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-12">
                <p><strong class="question">Question # 12:</strong> Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets</p>
                <p class="llm_review"><strong>Review:</strong> The answer provided by the authors is consistent with the paper content, and they have taken clear steps to ensure proper credit and adherence to licensing terms for all assets used in their work. Appendix B of the paper explicitly mentions the license information for the Meta-Album datasets, demonstrating the authors' commitment to respecting the original licenses and terms of use of the datasets incorporated into Meta-Album.<br><br>However, while the authors have rightfully addressed the licensing and crediting of the datasets used in their work, it could enhance the paper's integrity and usability for future researchers if the following suggestions are implemented:<br><br>1. Explicit URLs: Provide direct URLs to the source of each dataset within Appendix B, ensuring easy access for readers to verify licensing information and terms of use.<br><br>2. Versioning Information: Clearly state the version of each dataset used, if applicable. Versioning can significantly impact reproducibility and comparison of results.<br><br>3. Code Licensing: If any code was developed as part of the research, explicitly state the license under which it is released, and provide a citation or URL to the repository. This inclusion will help future researchers understand their rights to use, modify, and distribute the code.<br><br>4. Clarification on Mixed Licenses: For datasets that combine multiple sources with different licenses, provide a clear explanation of how these mixed licenses affect the use of the compiled dataset.<br><br>5. Detailed License Terms: For each listed license, briefly describe what it permits, requires, and prohibits, especially for licenses less common or familiar to the research community. This detail helps users understand their obligations without needing to look up each license.<br><br>6. Contact Information: Maintain up-to-date contact information for inquiries about the datasets and code, ensuring that future researchers can reach out with questions about licensing or use.<br><br>By incorporating these suggestions, the authors can further enhance the clarity and utility of their licensing and crediting practices, fostering an environment of transparency and respect for original creators and future users alike.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-12">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-13">
                <p><strong class="question">Question # 13:</strong> Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
                <p><strong class="answer">Answer:</strong> Yes</p>
                <p><strong class="justification">Justification:</strong> We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information</p>
                <p class="llm_review"><strong>Review:</strong> The authors have indeed provided a clear and comprehensive documentation for the newly introduced Meta-Album dataset. This is evident from their detailed explanation in the provided content about the structured templates used for documenting each dataset within Meta-Album, including info.json and labels.csv files that follow a standard data format, ensuring uniformity across different datasets. The appendices further enhance this documentation by offering additional details, like datasheets for each specific dataset, overviewing aspects such as licenses, sources, original creators, and contacts, which aligns well with the guidelines suggesting that details about training, license, limitations, etc., should be communicated.<br><br>Additionally, the authors have developed a dedicated website for Meta-Album, facilitating easy access to the datasets, source code for processing and generating reports (e.g., factsheets), and guidelines for contributing new datasets. This not only fosters transparency but also encourages community involvement in enriching this resource further. The commitment to extensibility and community engagement, as mentioned in the comparison with other datasets, underscores the effort put into documentation and user support.<br><br>However, missing from the answer but expected based on the guidelines, would be explicit statements regarding the consent obtained from individuals whose assets are included (if applicable) and steps taken to anonymize assets, which are critical from an ethical standpoint. Given the nature of the paper content provided, these issues may not apply directly (especially concerning consent, since the paper does not indicate the use of personal or sensitive data), yet mentioning them in light of the guidelines would have provided complete clarity. Nonetheless, the provided information strongly supports the claim of well-documented assets.<br><br>Constructive and actionable suggestions:<br>1. If not already included in the appendices or website, add a dedicated section on ethical considerations, detailing the approach to consent and anonymization for parts of the dataset that may include or require such measures in the future.<br>2. Enhance the documentation with examples or tutorials on how to use the datasets for common machine learning tasks, which could further lower the barrier to entry for new users.<br>3. Regularly update the website and GitHub repository with FAQs or common issues encountered by users, fostering a dynamic support resource that evolves based on community feedback.<br>4. Consider developing a forum or community discussion board linked from the Meta-Album website, encouraging users to share insights, applications of the dataset, and potential improvements or extensions.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-13">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-14">
                <p><strong class="question">Question # 14:</strong> For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
                <p><strong class="answer">Answer:</strong> NA</p>
                <p><strong class="justification">Justification:</strong> This does not apply on our research as we are not doing any crowdsourcing experiments</p>
                <p class="llm_review"><strong>Review:</strong> The answer provided by the authors is correct and consistent with the paper's content as it primarily focuses on introducing a meta-dataset for few-shot learning, meta-learning, and not on experiments or research involving human subjects or crowdsourcing. Therefore, the requirement to include instructions for participants or details about compensation does not apply.<br><br>Constructive and actionable suggestions for improvement are not applicable in this case due to the nature of the inquiry being irrelevant to the study's methodology and focus.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-14">Jump to Summary</a>
            </div>
            
            <div class="review" id="Genuine-question-15">
                <p><strong class="question">Question # 15:</strong> Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
                <p><strong class="answer">Answer:</strong> NA</p>
                <p><strong class="justification">Justification:</strong> This does not concerns us as we are not dealing with human subjects in our research.</p>
                <p class="llm_review"><strong>Review:</strong> The answer provided by the authors is consistent with the content of the paper, which introduces the Meta-Album: a meta-dataset for image classification designed to facilitate research in few-shot learning, meta-learning, transfer learning, and similar tasks. The creation and usage of this dataset do not involve crowdsourcing or direct research with human subjects that would incur potential risks requiring disclosures or Institutional Review Board (IRB) approvals. Therefore, the N/A response to the question about potential risks to study participants, disclosure of such risks, and IRB approvals is appropriate and accurate based on the information provided in the paper.<br><br>However, it's important for the authors to ensure that any future work involving the use or expansion of Meta-Album that might involve human subjects, either through dataset contribution, annotation, or in any other form, fully considers and adheres to ethical guidelines, including obtaining IRB approvals if necessary.<br><br>Constructive and actionable suggestions:<br>1. Documentation and Licensing: Ensure that all datasets included in Meta-Album are accompanied by clear documentation regarding their source, licensing, and any ethical considerations related to their original collection and intended use.<br>2. Ethical Considerations Statement: Consider including a general statement on ethical considerations related to the use of image datasets, acknowledging the origins of the data and any potential issues related to privacy, consent, and data protection that users should be aware of.<br>3. Future Work Protocol: Outline a protocol for future expansions of Meta-Album that involves human subjects or crowdsourcing efforts, indicating how ethical oversight will be ensured, including IRB review if applicable.<br>4. Data Anonymity and Protection: If any future versions of Meta-Album involve creating or utilizing datasets with identifiable human subjects, detail the measures taken to ensure anonymity and data protection to adhere to privacy standards and ethical research conduct.<br></p>
                <p><strong>Score:</strong> 1</p>
                <a class="move-to-top" href="#summary-Genuine-question-15">Jump to Summary</a>
            </div>
            
        
        
    </div>

    <a class="button button-bottom" href="https://docs.google.com/forms/d/e/1FAIpQLSfRIDkcXFbsOrR09j4qA1MlG4Rfir2lPD_u9YC4eqKBJ8tHkw/viewform?usp=pp_url&entry.1830873891=TWV0YS1BbGJ1bTogTXVsdGktZG9tYWluIE1ldGEtRGF0YXNldCBmb3JGZXctU2hvdCBJbWFnZSBDbGFzc2lmaWNhdGlvbg==" target="_blank">Click to submit post submission survey</a>

</div>

</body>
</html>