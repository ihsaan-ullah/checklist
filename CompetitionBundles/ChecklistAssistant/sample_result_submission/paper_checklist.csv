Question,Question_Title,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","The given answer matches the content of the paper, and the justification aligns with the guidelines provided. The abstract and introduction of the paper state its primary contributions, namely the introduction of the Meta-Album, a meta-dataset designed to facilitate research in few-shot learning, transfer learning, and meta-learning among others. They accurately describe the scope, including the diversity and scale of the included datasets, the preprocessing and formatting efforts, and the extendability of Meta-Album through community contributions. Furthermore, the paper details the design and utility of Meta-Album, showcasing experimental evaluations that support the claims made early on.

However, for the sake of thoroughness and in alignment with the critical perspective requested, here are some constructive suggestions:

1. The paper could benefit from a more detailed discussion of the challenges faced during the development of Meta-Album and how these challenges were addressed. This would give readers a clearer understanding of potential limitations and areas for future improvement.

2. While the paper does mention that Meta-Album includes various domains like ecology, manufacturing, and human actions, it could elaborate on the selection criteria for these domains and datasets. This would help readers understand why certain domains were included and how they contribute to the broader goals of facilitating cross-domain learning.

3. The paper mentions the utility of Meta-Album for different machine learning tasks but could further clarify the specific advantages or features that make it particularly well-suited for these tasks compared to other existing datasets or benchmarks.

4. Finally, as part of discussing the scope and contributions, it would be useful to have a more explicit discussion regarding the limitations of Meta-Album and any known biases in the datasets. This transparency would help researchers in assessing the applicability and relevance of Meta-Album to their specific research questions.

Overall, the paper does a good job of reflecting its contributions and scope in the abstract and introduction, and these suggestions are meant to further strengthen the presentation and utility of the Meta-Album meta-dataset.",1
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,We have clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","Justification Review:
The provided justification states that the limitations are clearly explained in a specific section titled ""Discussion and Conclusion."" However, upon reviewing the extensive details shared in the paper's content summary, there is no explicit mention or dedicated discussion on the limitations of the work undertaken by the authors. The absence of explicit mention of the ""limitations"" in the content provided makes it difficult to assess the validity of the justification. A well-documented discussion on limitations is crucial for understanding the scope, reliability, and applicability of the research findings.

Constructive Suggestions:
1. **Explicit Section on Limitations:** If not already included, authors should consider adding a specific subsection within the ""Discussion and Conclusion"" section that directly addresses the limitations of their study. This subsection should discuss any constraints or areas where the research findings may not apply.

2. **Assumptions and Robustness:** Authors should identify any strong assumptions made during the research and discuss how robust their findings are to the violations of these assumptions. This will help in understanding the boundaries within which the research findings hold true.

3. **Scope of Claims:** Reflect on the scope of the claims made in the paper. If the research is tested only on a limited number of datasets or scenarios, this should be clearly stated as a limitation. 

4. **Performance Influencing Factors:** Discuss factors that could potentially influence the performance of the proposed methods or findings. For instance, if the research is context-dependent, the limitations of its applicability in different contexts should be articulated.

5. **Computational Efficiency:** If not already discussed, elaborating on the computational efficiency and scalability of the proposed approaches with varying dataset sizes could provide important insights into their practical applicability.

6. **Privacy and Fairness Concerns:** If applicable, discuss any potential limitations of the approach in addressing privacy and fairness issues. This is increasingly critical in AI and Machine Learning research domains.

7. **Transparency and Honesty:** Encourage transparency by thoroughly acknowledging the limitations of the study. A forthright discussion of the limitations not only adds credibility to the research but also provides valuable insights for future work aiming to address these limitations.

By addressing these points, the authors would significantly strengthen the quality and integrity of their paper, making it clear to the readers and reviewers the context and framework within which the research findings are most applicable and reliable.",0
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The authors' justification for not providing theoretical results, proofs, or assumptions is acceptable and consistent with the content and nature of the paper. Since the primary focus of the paper is on the introduction and evaluation of a meta-dataset named Meta-Album, it is inherently practical rather than theoretical. This work involves data curation, dataset description, experimental protocols, and benchmark comparisons rather than theoretical exploration or the derivation of formulas and theorems. Hence, the question about providing the full set of assumptions and complete proofs is not applicable to this study.

However, for completeness and to ensure clarity for readers, it's important for the paper to clearly state the scope and nature of the research early on, which it does by explicitly mentioning that the work deals with meta-dataset creation and experimental evaluations using this meta-dataset. This clarification helps to set the right expectations for the readers regarding the type of contributions and content they will find in the paper.

Constructive suggestions for the authors:
1. Ensure that the introduction or methodology sections clearly articulate the applied research nature of the work to preemptively address questions related to theoretical aspects.
2. When discussing the experimental setups, results, or findings, consider providing detailed explanations or references to the methods used for analysis and evaluation to compensate for the lack of theoretical content. This will add depth to the readers' understanding of how the meta-dataset and its evaluation were conducted.
3. Since the paper involves the creation and utilization of a new meta-dataset, it would be beneficial to include a discussion on the implications, potential use cases, and future research directions enabled by Meta-Album. This can help stimulate theoretical investigations by others in the community who might use Meta-Album for developing new algorithms or theories.
4. It might also be useful to discuss any limitations or challenges encountered during the dataset creation process and experimental evaluations to inform future work in this area.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The answer provided by the authors suggests they have made efforts to ensure reproducibility by publicly providing the code on their GitHub repository and detailing dataset access on their website. This is a positive step towards meeting the reproducibility criterion as outlined in the guidelines. Given this information, it appears that the authors are compliant with the NeurIPS policy on code and data submission, making the paper’s experimental results potentially reproducible. However, the justification could be improved by specifying whether instructions for environment setup, exact commands for reproducing results, and details on data preparation and access (including raw, preprocessed, and intermediate data) are provided as part of the repository's documentation. 

**Suggestions for Improvement**:
1. **Detailed Reproducibility Instructions**: Ensure that the GitHub repository contains a README file or documentation that includes detailed instructions for setting up the environment, including any dependencies or specific versions of software/libraries required. It should also provide the exact commands needed to run the experiments and reproduce the results.

2. **Data Access and Preparation**: Clarify the process for accessing and preparing the data. This includes instructions on downloading the raw data, steps for preprocessing, and how to generate any intermediate data used in the experiments. If any data cannot be shared due to privacy or license restrictions, explicit instructions on how to simulate or acquire similar datasets should be provided.

3. **Baseline and Proposed Method Scripts**: Ensure scripts for both the proposed method and any baseline comparisons are included. Clearly indicate which scripts correspond to which results in the paper. If any experiments or results are not reproducible with the provided scripts, specify this explicitly and explain why.

4. **Environment Details**: Provide a requirements.txt file or an environment.yml file for setting up a Python environment with the necessary dependencies. For non-Python environments, provide equivalent documentation. Consider using containerization (e.g., Docker) to further ensure reproducibility across different machines and platforms.

5. **Anonymization for Submission**: If the supplementary material or repository contains any information that could compromise the double-blind review process, ensure it is adequately anonymized.

6. **Supplemental Material**: If possible, include additional supplemental material that supports the experimental results, such as additional figures, tables, or analysis not included in the main paper due to space constraints.

Implementing these suggestions can significantly enhance the reproducibility of the paper's main experimental results and align with best practices for scientific reporting.",1
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The given justification mentions the provision of the code on a GitHub repository and details about accessing datasets on a website. However, it lacks specificity regarding the exact steps or commands needed to reproduce the main experimental results, which is a critical aspect as per the guidelines provided for NeurIPS submissions. There is no mention of whether environmental dependencies, data preparation instructions, or scripts for reproducing experimental results are provided, making it difficult to assess the reproducibility of the work based on the provided response alone.

Constructive and actionable suggestions for improvement:

1. **Detailed Instructions:** Provide a step-by-step guide in the supplemental material or the repository README that includes the exact environment setup, dependencies, data preparation steps, and the commands that need to be executed to reproduce the results.
2. **Environment Configurations:** It would be beneficial to include a requirements.txt file or a Dockerfile in the code repository to ensure that the code runs in an appropriate environment with all necessary dependencies installed.
3. **Script for Experiments:** Ensure that there are scripts available that specifically reproduce the figures, tables, and any other key findings reported in the paper. Clearly document which script corresponds to which result in the paper.
4. **Access and Preparation of Data:** Provide clear guidelines on how to access both raw and preprocessed versions of the data used in the experiments. If there are any steps required to preprocess the data further, include scripts or instructions for these steps as well.
5. **Explain Omissions if Any:** If certain experiments or results cannot be reproduced with the provided code or data, explicitly state these limitations and provide a rationale. This adds to the transparency and credibility of the submission.
6. **Anonymity Preservation:** If applicable, ensure that the code and data are shared in a way that preserves anonymity during the review process, following the conference guidelines.

By addressing these points, the authors can significantly improve the reproducibility of their work, aligning with the NeurIPS submission guidelines and making it easier for peers and reviewers to validate and build upon the research findings.",0
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","The answer is consistent with the content provided in the paper. The paper mentions the experimental settings in Section 3.2, where the authors give a brief overview of how they conducted their experiments, including references to any-way any-shot settings and cross-domain few-shot learning experiments. The mention of the Meta-Album GitHub repository suggests that additional details, such as data splits, hyperparameters, and the type of optimizer, are available outside the main body of the paper, which aligns with the guidelines provided for answering this question. 

However, to enhance the reproducibility and readability of the paper, the following itemized suggestions are provided:

1. **Explicit Mention of Key Experimental Details:** While the paper refers to the additional details being available in the GitHub repository, including explicit mentions of key experimental setup within the paper would improve accessibility for readers not keen or able to dig through external resources. This includes hyperparameters, optimizer types, and data split ratios.

2. **Justification for Choices Made:** Providing a brief justification for the choice of hyperparameters, data splits, and the type of optimizer directly in the paper could significantly increase the understanding and trust in the experimental setup. 

3. **Version Control and Persistence**: Ensure that the GitHub repository is well-maintained and versions of the code and data that were used for the experiments in the paper are easily accessible and clearly marked. This ensures readers can replicate the results even if the project evolves.

4. **Direct Linking**: Where possible, include direct links to specific scripts or notebooks in the GitHub repository that correspond to the experiments described in the paper. This direct linkage would greatly ease the process of replication and understanding of the experiments. 

5. **Supplementary Material**: If space or format constraints in the paper limit the amount of detail that can be shared, consider providing supplementary material or an appendix that goes into greater detail on the experimental setups. This can be a middle ground between the main content of the paper and external repositories. 

Overall, while the answer provided by the authors meets the basic requirement of the question by directing readers to the GitHub repository for more information, incorporating these suggestions could significantly enhance the reproducibility and comprehensibility of the research presented.",1
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","The justification provided by the authors states that error bars are reported in Figure 2, Figure 3, and Appendices E and F, which is in accordance with the question's requirements for indicating statistical significance or variability in the experimental results. However, the justification lacks specific details about the nature of these error bars (e.g., whether they are standard deviations or standard errors, the confidence interval they represent, or the method used for calculating them), which could be critical for thoroughly evaluating the statistical rigor of the report.

While the inclusion of error bars in the figures and appendices suggests that the authors have considered variability in their experiments, the lack of detailed information about these error bars means that the reader cannot fully assess the statistical significance of the experiments. The guidelines mentioned in the question highlight the importance of not only including error bars but also thoroughly describing their calculation and the assumptions behind them.

Constructive suggestions for the authors:
1. Clarify the type of error bars used in the experiments (e.g., standard deviation, standard error) and the confidence interval they represent.
2. Include a brief explanation of the method used for calculating the error bars, whether it's a closed-form formula, a library function, bootstrap, etc.
3. Describe any assumptions made during the calculation of the error bars, such as assuming normally distributed errors.
4. Make it clear in the text where the error bars can be found (e.g., by referencing specific figures or tables) and provide a brief interpretation of what these error bars indicate regarding the variability and reliability of the results.
5. For asymmetrical distributions, consider the appropriateness of symmetric error bars and provide alternative ways to represent the statistical variability if necessary.

These suggestions would help improve the transparency and comprehensibility of the statistical analysis, thereby allowing readers to more accurately interpret the significance of the research findings.",1
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","Justification for Score:
The authors of the paper claim to have provided details on the compute resources used in the experiments in Section 3.2 and Appendices E and F. However, after careful review, it is clear that the description of the computational resources is not sufficiently detailed to fully satisfy the guidelines for this question. The paper mentions using a ""single PNY GeForce RTX 2080TI GPUs with 11GB of VRAM or a single NVIDIA V100 with 16GB of VRAM"" for experiments, and that ""Each experimental run took at most 24 hours on the former GPU,"" which provides some insights but lacks the comprehensive details required for full reproducibility. Specifically, the paper does not elaborate on:

1. The exact type of compute workers (e.g., CPU specifications) beyond the GPU models for all experiments.
2. The memory and storage capacity needed, other than VRAM, which is critical for reproducing the experiments, especially considering the size of the datasets involved.
3. An estimate of total compute requirement for the entire set of experiments; only a maximum duration for the experimental runs was provided.
4. Details on whether any part of the research required more compute resources than reported for the experiments presented in the paper, such as preliminary or failed experiments.

Constructive Suggestions:
1. **Detailed Compute Worker Specifications:** Future versions of the paper should specify the CPU model, RAM, and other relevant hardware details for the computational environment used, not just the GPU models. This helps in understanding the computational environment needed for reproducing the experiments.

2. **Memory and Storage Requirements:** Provide explicit details on RAM and storage requirements for running the experiments. Considering the size of the datasets and the models employed, this information is crucial for reproducibility.

3. **Total Compute Estimation:** Offer a clearer breakdown of the compute requirements for each experiment, including GPU hours and any specifics related to parallel or distributed computing, if applicable. This would also include information on the number of experimental runs conducted, which would help in assessing the overall computational expense.

4. **Comprehensive Disclosure of Compute Use:** Address any additional computational resources that were utilized for the research project as a whole, including preliminary studies or experiments that were conducted but not included in the final paper. This provides a more accurate picture of the computational cost of the research.

By addressing these areas, the paper could significantly improve transparency and aid in the reproducibility of the research findings.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We completely comply with the NeurIPS Code of Ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","The authors affirmatively state their compliance with the NeurIPS Code of Ethics, which is a necessary declaration for ensuring that the research adheres to accepted ethical standards. This assertion, as provided, meets the conference's requirement for authors to acknowledge and adhere to these guidelines. However, the justification lacks detail about how they conform to these guidelines specifically.

Constructive and Actionable Suggestions:
1. **Elaborate on Ethical Compliance**: It would be beneficial for the authors to provide a more detailed justification or examples of how their research complies with the NeurIPS Code of Ethics. This could include discussions on data privacy, consent where applicable, fairness in model training, and mitigation of any potential harmful impacts.
2. **Address Anonymity Maintenance**: The guideline stipulates the importance of preserving anonymity. Even though the authors may not need to discuss anonymity in the context of ethical compliance directly, ensuring it is considered in any public data or shared resources related to the paper should be stated or acknowledged.
3. **Assess Impact and Risks**: Consider including a brief section or statement assessing the potential societal impact and risks associated with the research. This could cover both the immediate implications of the meta-dataset introduction and longer-term effects it might have in developing ML models in various domains.
4. **Data Ethics and Source Transparency**: Given that the Meta-Album compiles datasets from multiple domains, a breakdown of ethical considerations for data collection, licensing, and usage permissions for each of these source datasets should be included. This ensures transparency and adherence to ethical standards across all compiled data.
5. **Accessibility and Inclusiveness**: Reflect on the accessibility of the proposed meta-dataset and the tools or algorithms developed from it. Ensure the resources are inclusive and can be used by a diverse set of researchers while upholding the principles of fairness and equality.",1
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","The justification provided by the authors suggests that there are no negative societal impacts from their work, focusing instead on the positive impacts related to fostering progress in few-shot learning and meta-learning through their meta-dataset. This approach ignores the conference guidelines that require authors to consider both potential positive and negative societal impacts of their work, even if the research is foundational or not tied to particular applications.

While it's true that the creation and provision of a large-scale meta-dataset could accelerate advancements in machine learning, specifically in few-shot learning and meta-learning, the authors' failure to address potential negative societal impacts is a significant oversight. For example, advancements in few-shot learning could have implications for privacy (by making it easier to identify individuals from limited data), security (by improving the efficiency of malicious AI systems that require fewer data to learn), or fairness (if the datasets included or the methods developed exacerbate existing biases or create new ones).

Constructive and actionable suggestions for the authors:
1. **Broader Impact Discussion**: Revisit the section discussing societal impacts, acknowledging that while the primary intention and direct impact of the work may be positive, indirect negative impacts could arise. This might include misuse of the technology, privacy concerns, security implications, and bias and fairness considerations.
2. **Mitigation Strategies**: Offer potential mitigation strategies for any identified risks. This could involve recommendations for responsible use, guidelines for data privacy protection, measures to ensure fairness and prevent bias, or suggestions for future research to address these concerns.
3. **Examples of Misuse**: Provide examples or scenarios where the technology could be misused, even if indirectly related to the core research. Explaining how the meta-dataset or the techniques it enables could be misused helps fulfill the guideline requirements and shows a thorough consideration of the research's societal impact.
4. **Monitoring and Governance**: Discuss potential mechanisms for monitoring the use of the meta-dataset and the technologies developed using it. Suggest governance models or community-driven initiatives to ensure the technology's positive application.
5. **Engage with Ethical Research**: Encourage or conduct further ethical research into the implications of few-shot learning and meta-learning advancements, including how these technologies intersect with societal values, norms, and legal frameworks.

By addressing these points, the authors can provide a more balanced and comprehensive analysis of the societal impacts of their work, in line with the conference's expectations.",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Yes,"Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","The answer provided by the authors is consistent with the content of the paper reviewed. The paper indeed states that all datasets have been carefully licensed, with clear outlines on their intended academic use and restrictions on commercial exploitation. Furthermore, the attention to licensing and responsible release is evident from the detailed appendix that lists the licenses for each dataset included in the Meta-Album. This demonstrates a responsible approach towards data release, acknowledging the importance of ethical considerations in sharing datasets. Additionally, the provision of a website and clear contact information for queries related to data usage further supports the responsible dissemination of these resources. 

However, while the authors mention the responsible release and recommended usage, there could be a more explicit discussion on potential risks associated with misuse and specific safeguards beyond licensing. For scraped datasets or those potentially containing sensitive information, detailing any de-identification processes, consent protocols, or privacy assessments conducted would strengthen the responsible release considerations. Moreover, considering the diversity of data domains covered by Meta-Album, a brief analysis of domain-specific risks and how they have been mitigated would be informative.

Constructive and actionable suggestions for the authors:
1. Consider elaborating on any specific privacy or ethical review processes that datasets underwent before inclusion in Meta-Album, especially for those involving human subjects or potentially sensitive contexts.
2. Provide a discussion on any potential misuse scenarios you have considered and how the licensing terms or recommended usages aim to prevent such misuse.
3. If applicable, detail any content moderation or safety filters that have been put in place for datasets that were compiled from public sources or via web scraping.
4. Encourage users of the Meta-Album to engage in ethical research practices by providing guidelines on obtaining necessary permissions for dataset use beyond the provided licenses, especially in cases where further ethical considerations may be necessary (e.g., datasets involving people, potentially sensitive images, etc.).
5. Finally, establish a reporting mechanism for misuse or ethical concerns related to the datasets to proactively address any issues that arise post-release.",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","The answer provided by the authors is consistent with the paper content, and they have taken clear steps to ensure proper credit and adherence to licensing terms for all assets used in their work. Appendix B of the paper explicitly mentions the license information for the Meta-Album datasets, demonstrating the authors' commitment to respecting the original licenses and terms of use of the datasets incorporated into Meta-Album.

However, while the authors have rightfully addressed the licensing and crediting of the datasets used in their work, it could enhance the paper's integrity and usability for future researchers if the following suggestions are implemented:

1. **Explicit URLs**: Provide direct URLs to the source of each dataset within Appendix B, ensuring easy access for readers to verify licensing information and terms of use.

2. **Versioning Information**: Clearly state the version of each dataset used, if applicable. Versioning can significantly impact reproducibility and comparison of results.

3. **Code Licensing**: If any code was developed as part of the research, explicitly state the license under which it is released, and provide a citation or URL to the repository. This inclusion will help future researchers understand their rights to use, modify, and distribute the code.

4. **Clarification on Mixed Licenses**: For datasets that combine multiple sources with different licenses, provide a clear explanation of how these mixed licenses affect the use of the compiled dataset.

5. **Detailed License Terms**: For each listed license, briefly describe what it permits, requires, and prohibits, especially for licenses less common or familiar to the research community. This detail helps users understand their obligations without needing to look up each license.

6. **Contact Information**: Maintain up-to-date contact information for inquiries about the datasets and code, ensuring that future researchers can reach out with questions about licensing or use.

By incorporating these suggestions, the authors can further enhance the clarity and utility of their licensing and crediting practices, fostering an environment of transparency and respect for original creators and future users alike.",1
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","The authors have indeed provided a clear and comprehensive documentation for the newly introduced Meta-Album dataset. This is evident from their detailed explanation in the provided content about the structured templates used for documenting each dataset within Meta-Album, including info.json and labels.csv files that follow a standard data format, ensuring uniformity across different datasets. The appendices further enhance this documentation by offering additional details, like datasheets for each specific dataset, overviewing aspects such as licenses, sources, original creators, and contacts, which aligns well with the guidelines suggesting that details about training, license, limitations, etc., should be communicated.

Additionally, the authors have developed a dedicated website for Meta-Album, facilitating easy access to the datasets, source code for processing and generating reports (e.g., factsheets), and guidelines for contributing new datasets. This not only fosters transparency but also encourages community involvement in enriching this resource further. The commitment to extensibility and community engagement, as mentioned in the comparison with other datasets, underscores the effort put into documentation and user support.

However, missing from the answer but expected based on the guidelines, would be explicit statements regarding the consent obtained from individuals whose assets are included (if applicable) and steps taken to anonymize assets, which are critical from an ethical standpoint. Given the nature of the paper content provided, these issues may not apply directly (especially concerning consent, since the paper does not indicate the use of personal or sensitive data), yet mentioning them in light of the guidelines would have provided complete clarity. Nonetheless, the provided information strongly supports the claim of well-documented assets.

Constructive and actionable suggestions:
1. If not already included in the appendices or website, add a dedicated section on ethical considerations, detailing the approach to consent and anonymization for parts of the dataset that may include or require such measures in the future.
2. Enhance the documentation with examples or tutorials on how to use the datasets for common machine learning tasks, which could further lower the barrier to entry for new users.
3. Regularly update the website and GitHub repository with FAQs or common issues encountered by users, fostering a dynamic support resource that evolves based on community feedback.
4. Consider developing a forum or community discussion board linked from the Meta-Album website, encouraging users to share insights, applications of the dataset, and potential improvements or extensions.",1
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This does not apply on our research as we are not doing any crowdsourcing experiments,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","The answer provided by the authors is correct and consistent with the paper's content as it primarily focuses on introducing a meta-dataset for few-shot learning, meta-learning, and not on experiments or research involving human subjects or crowdsourcing. Therefore, the requirement to include instructions for participants or details about compensation does not apply.

Constructive and actionable suggestions for improvement are not applicable in this case due to the nature of the inquiry being irrelevant to the study's methodology and focus.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This does not concerns us as we are not dealing with human subjects in our research.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The answer provided by the authors is consistent with the content of the paper, which introduces the Meta-Album: a meta-dataset for image classification designed to facilitate research in few-shot learning, meta-learning, transfer learning, and similar tasks. The creation and usage of this dataset do not involve crowdsourcing or direct research with human subjects that would incur potential risks requiring disclosures or Institutional Review Board (IRB) approvals. Therefore, the N/A response to the question about potential risks to study participants, disclosure of such risks, and IRB approvals is appropriate and accurate based on the information provided in the paper.

However, it's important for the authors to ensure that any future work involving the use or expansion of Meta-Album that might involve human subjects, either through dataset contribution, annotation, or in any other form, fully considers and adheres to ethical guidelines, including obtaining IRB approvals if necessary.

Constructive and actionable suggestions:
1. **Documentation and Licensing**: Ensure that all datasets included in Meta-Album are accompanied by clear documentation regarding their source, licensing, and any ethical considerations related to their original collection and intended use.
2. **Ethical Considerations Statement**: Consider including a general statement on ethical considerations related to the use of image datasets, acknowledging the origins of the data and any potential issues related to privacy, consent, and data protection that users should be aware of.
3. **Future Work Protocol**: Outline a protocol for future expansions of Meta-Album that involves human subjects or crowdsourcing efforts, indicating how ethical oversight will be ensured, including IRB review if applicable.
4. **Data Anonymity and Protection**: If any future versions of Meta-Album involve creating or utilizing datasets with identifiable human subjects, detail the measures taken to ensure anonymity and data protection to adhere to privacy standards and ethical research conduct.",1
