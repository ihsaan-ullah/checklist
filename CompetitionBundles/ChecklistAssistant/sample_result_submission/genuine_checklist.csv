Question,Question_Title,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","The response provided by the authors accurately reflects the content and contributions of the paper as presented in the abstract and introduction. The paper introduces Meta-Album, a multi-domain meta-dataset aimed at facilitating research in few-shot learning, meta-learning, and related tasks. The claims made in the abstract and introduction accurately outline the contributions, including the creation of the Meta-Album with 40 image classification datasets from various domains, the availability of the dataset in three versions to accommodate computational resources, and the provision of baseline methods and open-source code for community use. Furthermore, the paper discusses the importance of this work for cross-domain and domain-independent studies, adding to its contributions.

However, to ensure absolute clarity and consistency throughout, the authors could consider the following suggestions for improvement:

1. Emphasize the novelty: While the paper states its contributions, highlighting what makes Meta-Album uniquely beneficial compared to existing datasets and benchmarks could further strengthen the paper's impact. Specifically, elaborating on how Meta-Album addresses the limitations of current datasets in the field could provide clearer motivation for the study.

2. Clarify limitations: While the abstract and introduction set ambitious goals and contributions, briefly acknowledging any limitations or challenges encountered during the development and deployment of Meta-Album within these sections can enhance transparency and set realistic expectations for the readers.

3. Further detail on use cases: The introduction mentions facilitating research in few-shot learning, transfer learning, and meta-learning among other tasks. Expanding slightly on how Meta-Album specifically enables advancements in these areas or providing brief examples could help illustrate the practical implications and applications of the dataset more concretely.

By addressing these points, the authors can ensure that the claims made in the abstract and introduction not only accurately reflect the paper’s contributions but also clearly convey the significance and potential impact of their work to the readers.",1
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,We have clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","The authors claim they have discussed the limitations in the ""Discussion and conclusion"" section of their paper, as per the reviewer's question and the justification provided. A critical review of the provided abstract and sections related to discussions and conclusions indeed shows that the authors have tackled the limitation aspect of their work. They have reflected on the dataset's extensibility, potential biases, and the need for more challenging tasks, indicating a thoughtful consideration of the limitations and scope of their study.

However, to further enhance the paper's clarity and adherence to best practices concerning discussing limitations, the authors could consider the following suggestions:
1. **Explicit Limitations Section**: While discussing limitations within the ""Discussion and conclusion"" section is acceptable, having a dedicated ""Limitations"" section could make these discussions more visible and structured. This could help readers quickly understand the potential challenges and scope of applicability of the proposed methods and datasets.
2. **Detailing Computational Efficiency**: The authors might want to include more information on the computational efficiency of handling the Meta-Album and the scalability of the proposed methods. This is especially relevant since the dataset versions vary significantly in size, and computational resources vary among researchers.
3. **Robustness and Assumptions**: Elaborate further on the assumptions underlying the proposed methods or dataset curation. This includes assumptions about data quality, the representativeness of the classes, and the impact of these assumptions on the generalizability of the findings.
4. **Performance Factors**: While the paper discusses dataset features and preliminary results, further details on factors influencing performance across different domains could be insightful. This can include discussions on why certain domains might pose more challenges and how future methods could address these challenges.
5. **Addressing Bias and Fairness**: The authors hint at potential biases in the dataset preparation process. Expanding on how such biases could affect the learning outcomes and potential mitigation strategies would be informative. This also ties into broader discussions on fairness and ethical considerations in machine learning datasets.
6. **Privacy Considerations**: If applicable, discussions on privacy concerns, especially in datasets involving human subjects (directly or indirectly), could be a valuable addition.

By addressing these suggestions, the authors can provide a more comprehensive overview of their work's limitations, thus contributing to transparency, reproducibility, and the integrity of scientific research in machine learning.",1
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,N/A,This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The authors correctly labeled this section as N/A (Not Applicable) due to the nature of their work. Their research focuses on the creation of a new meta-dataset, ""Meta-Album,"" and conducting experiments using this dataset. The submission is fundamentally empirical and does not delve into theoretical analysis that would necessitate the presentation of theorems, formulas, or proofs. This aligns well with the nature of the submission as a contribution to the field of few-shot learning and meta-learning through dataset development rather than theoretical model derivation or proof.

Given the empirical focus of the paper, the necessity for theoretical proofs or the full set of assumptions for theoretical results is indeed not applicable. The detailed description of the dataset, its preparation, presentation, and the experiments conducted provides the necessary information for the contribution to be evaluated on its empirical merits.

**Actionable Suggestions for Further Clarity (even when N/A is a valid response):**
1. While the section is correctly marked as N/A, the authors might consider adding a sentence to briefly clarify the empirical nature of their work early in the paper. This would set the correct expectations for readers and reviewers regarding the content and contributions of the paper.
2. It could be beneficial for the authors to discuss any assumptions made during the dataset creation process or during the experimental setup. While theoretical proofs are not applicable, understanding the assumptions behind the data collection, preprocessing, and choice of baseline methods could enhance the paper’s clarity and help assess the generalizability and applicability of the dataset.
3. Given the empirical nature of the work, authors are encouraged to thoroughly detail their experimental design, including any datasets comparisons, model training details, and evaluation metrics in the supplement material. This ensures the reproducibility and transparency of the experimental results reported.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","Justification for the Score:
While the authors state that the code is publicly available on their GitHub repository and details about accessing the datasets are on their website, the provided answer lacks specificity regarding the comprehensiveness of this documentation. For reproducing the main experimental results, it's not just the availability of code and datasets that matters; detailed instructions, environment setups, and how to access and prepare the data are equally critical. The answer does not address whether explicit instructions, environmental requirements, or scripts to reproduce experiments are provided. Thus, the answer appears to lack critical details necessary to ensure the reproducibility of the experimental results.

Constructive and Actionable Suggestions:
1. **Detailed Instructions:** Provide a clear and detailed step-by-step guide in the paper or supplementary material on how to reproduce the experiments. This should include the exact commands and environment setup.
   
2. **Environment Details:** Specify the software versions, libraries, and any specific hardware requirements needed to replicate the research. If possible, include a Dockerfile or an environment.yml file that can automatically set up the environment.
   
3. **Data Preparation Scripts:** If the paper involves data preprocessing, ensure scripts for these steps are provided along with instructions. Clearly explain how raw data can be transformed into the format used in the experiments.
   
4. **Results Reproduction:** Ensure that scripts to reproduce all tables, figures, and key results presented in the paper are included. Clearly indicate which scripts correspond to which results in the documentation.
   
5. **Anonymization:** Since the paper is under review, ensure that any external links to code or datasets are anonymized appropriately to maintain the double-blind review process integrity.

By addressing these suggestions, the authors can enhance the reproducibility of their experimental results, which in turn strengthens the credibility and usefulness of their research contributions to the community.",0
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The answer provided by the authors is consistent with the content in the paper. The paper indeed mentions the availability of the code on GitHub and provides information on accessing the datasets through their website. This indicates that the authors have made efforts to ensure the reproducibility of their experimental results.

However, a more thorough review of the provided supplementary materials and guidelines reveals that while the authors have made the code and data accessible, the level of detail necessary for ""faithfully"" reproducing the main experimental results could be further improved. For instance, the paper does not explicitly mention if exact commands and environment settings are provided. Additionally, it is unclear if the authors have included detailed instructions on data access and preparation for all stages of their experiments, including how to access raw data, preprocess data, and generate new datasets.

To strengthen the reproducibility of the experimental results, I recommend the authors to:
1. Ensure that the GitHub repository contains detailed instructions, including the exact command lines and environment needed to reproduce the results.
2. Provide a clear and detailed description of data access and preparation in the supplementary materials or on the project's website. This should include how to handle raw, preprocessed, intermediate, and generated data.
3. Clarify which experimental results are reproducible with the provided scripts. If any experiments are not included in the reproducibility scripts, clearly state which ones and explain why.
4. Consider adding a requirements.txt file or an environment.yml file in the GitHub repository to simplify the setup of the environment needed to run the experiments.
5. Verify that all links to data, code, and supplementary material remain accessible and are correctly anonymized (if submission anonymity is required).

By addressing these suggestions, the authors would enhance the clarity of their reproducibility claims and make it easier for others to verify and build upon their work.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","Explanation: The answer provided by the authors mentions that the experimental settings or details are explained in Section 3.2 and that additional details can be found in the Meta-Album GitHub repository. However, the justification lacks specificity regarding the types of details provided and does not address whether all the necessary pieces of information needed to understand the results thoroughly are present. For example, it doesn't explicitly confirm whether all training and test details, including data splits, hyperparameters, the type of optimizer, and how hyperparameters were chosen, are clearly provided. 

To ensure a comprehensive review, the authors should have detailed whether these elements are explicitly addressed in the paper or supplementary materials. Just mentioning the section and an external repository for further details is insufficient to ascertain the completeness of the experimental setup description.

Constructive and Actionable Suggestions:
1. Provide a brief summary within the paper of all critical experimental setup details, including data splits, hyperparameters, type of optimizer, and the methodology for choosing hyperparameters. This helps readers quickly grasp the setup without needing to consult external sources.
2. If certain information is too voluminous or detailed for the main text, consider including it in an appendix or supplementary material, ensuring it is easily accessible and clearly referenced from the main text.
3. Ensure the GitHub repository mentioned for additional details is well-organized and contains clear documentation on experimental settings to facilitate reproduction of results. Mentioning it is beneficial, but it should not replace the need for including critical details within the paper or its immediate supplements.
4. For transparency and reproducibility, explicitly state within the paper what information can be found in the GitHub repository and provide direct links to specific files or documentation sections relevant to the experimental setup.",0
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","The authors have provided error bars as part of their experimental results, which is in line with the requirements for reporting statistical significance. Specifically, they mention the inclusion of error bars in figures 2 and 3, as well as in Appendices E and F, covering both within-domain and cross-domain few-shot learning experiments. This approach helps to understand the variability and reliability of the reported results.

Justification Evaluation: 
The answer provided by the authors aligns with the content of the paper and the guidelines for reporting statistical significance. They have outlined where in the paper the error bars are reported, ensuring transparency about the variability in their experimental outcomes. The authors' approach satisfies the criteria of detailing the statistical significance of the experiments and adheres to good scientific reporting practices. 

Constructive and Actionable Suggestions:
1. While the authors have mentioned the inclusion of error bars, for enhanced clarity, they could explicitly discuss the calculation method of these error bars in the main sections of the paper, not only in the appendices. This will make it easier for readers to understand the basis of the statistical reporting without needing to refer to supplementary material.
2. If not already done, the authors should clarify whether these error bars represent standard deviations, standard errors of the mean, or confidence intervals. Explicitly stating this information within the main text will improve the interpretability of the results.
3. It would be beneficial if the authors include a brief discussion on any assumptions made during the calculation of error bars, such as the assumption of normally distributed errors. This transparency is crucial for readers to properly evaluate the significance of the reported results.
4. For future work or extensions of this dataset, considering the presentation of results using other statistical significance measures, such as p-values or effect sizes, could further enhance the depth of the analysis. This would provide a more comprehensive understanding of the statistical significance of the differences observed in the experiments.

Overall, the authors have met the basic requirement by reporting error bars, which is essential for assessing the reliability and significance of their experimental results. Incorporating the above suggestions could further strengthen the paper's statistical reporting and overall quality.",1
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","Justification: While the authors mention providing details about compute resources in several sections of the paper, the specifics mentioned in the given answer are not as comprehensive or explicit as needed for complete reproducibility. The provided information in the given answer lacks detail regarding the actual hardware specifications (e.g., model of GPU, CPU details, amount of RAM) and does not mention execution times for experiments except in the context of meta-training, meta-validation, and meta-test phases in Table 17, which still does not offer a complete picture of the total compute requirements.

Constructive and actionable suggestions for the authors:
1. Include specific hardware configurations for all experiments, such as the type of GPU or CPU used, the number of cores, and the amount of RAM available.
2. Provide detailed execution times for each experimental setup mentioned in the paper, not just the meta phases. This would be beneficial for researchers attempting to replicate your results or use your methodology for related work.
3. If cloud resources were utilized, specify the provider (e.g., AWS, Google Cloud, Azure) and the configuration of the virtual machines or services employed.
4. For experiments requiring significant computational resources, discuss potential bottlenecks and suggest alternative configurations that might reduce compute time or resource requirements.
5. Include any software dependencies, along with version numbers, that are crucial for reproducing the experiments. This is especially important if specific libraries or frameworks have a substantial impact on performance.

By addressing these points, future readers and researchers will have a clearer understanding of what is required to replicate the experiments and potentially extend upon your work, enhancing the paper's value and applicability.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeruIPS code of ethics,Yes,We completely comply with the NeurIPS Code of Ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","The authors' confirmation of compliance with the NeurIPS Code of Ethics is succinct and directly addresses the question. Given that there's no detailed explanation required for an affirmative answer (Yes) in this context, the authors' response appears appropriate. 

However, it's important to critically evaluate the provided research and the ethics statement. Based on the detailed presentation of the ""Meta-Album"" research project, including data preparation, dataset descriptions, and intention for community expansion and inclusivity, it seems the authors have made a conscious effort to adhere to ethical guidelines. These include considerations around dataset licensing, the open invitation for community contributions, and clear acknowledgment of data sources, which align with ethical transparency and fairness.

Nonetheless, an actionable suggestion for improvement would be for the authors to provide a brief description of how they ensure ethical compliance, particularly in handling data responsibly, ensuring privacy where applicable (even if indirectly, by adherence to dataset licenses already addressing such concerns), and fostering inclusivity in community contributions. An expanded ethics statement could detail specific practices adopted in the research to ensure fairness, respect, transparency, and accountability in accordance with the NeurIPS Code of Ethics.",1
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","The response from the authors is accurate with the provided content of the paper. The Meta-Album meta-dataset is primarily designed to support research in few-shot learning, meta-learning, and related areas. It provides a resource that could enable advancements in automated machine learning, which is fundamentally beneficial for society by potentially reducing the need for large datasets and extensive computational resources, thereby democratizing access to machine learning capabilities.

Justification:
- The paper clearly outlines the positive societal impacts, particularly in advancing research in few-shot learning and meta-learning. These are areas of machine learning research that have the potential to make learning algorithms more efficient and accessible.
- Considering the nature of the dataset and its applications, direct negative societal impacts are not apparent. The dataset itself is a tool for research rather than an application with direct societal interaction, which limits its immediate negative consequences.

However, a more in-depth analysis or discussion of potential negative impacts could be beneficial, even for foundational research datasets. For foundational research, it can be challenging to predict all future applications and potential misuse. Still, acknowledging this complexity and proposing general considerations or guidelines for ethical use and further development could enhance the paper's discussion on societal impacts.

Constructive and actionable suggestions:
1. **Ethical Usage Guidelines**: The authors could include a brief section discussing ethical considerations or guidelines for using the dataset. This could cover general principles to encourage responsible use.
2. **Future Applications Discussion**: Explore potential future applications of models developed using the dataset, both positive and negative. This can help to identify areas where ethical considerations or mitigation strategies are particularly relevant.
3. **Engagement with Ethical Research**: Suggest ways the research community could engage with ethics researchers to explore the longer-term societal impacts of advancements in few-shot learning and meta-learning. This interdisciplinary engagement could help in identifying potential risks not immediately apparent to those focused on technical development.
4. **Transparency and Reproducibility**: Emphasize the importance of transparency and reproducibility in research using the dataset, as these principles are fundamental to ensuring that potential societal impacts (positive or negative) can be accurately assessed and understood by the broader community.",1
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Yes,"Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","The answer provided by the authors regarding safeguards for responsible release lacks sufficient detail on specific measures implemented to prevent misuse of the data, especially considering the broad range and sensitivity of domains covered by the Meta-Album dataset. While mentioning the release under proper licenses and recommended usage provides a starting point, it does not fully address the potential risks associated with misuse, especially given the variety of data encompassing sensitive areas like human actions and medical images (microscopy domain).

Constructive and Actionable Suggestions:
1. **Provide Detailed Data Curation Process**: Elaborate on the data curation process to ensure sensitive or personally identifiable information is not included in the dataset. This is particularly pertinent for domains involving human subjects.
   
2. **Implement Usage Guidelines**: Develop and include clear guidelines for the ethical use of the dataset to guide researchers on acceptable uses and discourage misuse. These guidelines should be easily accessible and prominently displayed to all potential users.

3. **Controlled Access**: Consider implementing a controlled access model for portions of the dataset that could be more susceptible to misuse. This process could include an application where users outline their intended use of the data and agree to terms of responsible usage.

4. **Ethical Review and Risk Assessment**: Document any ethical reviews or risk assessments conducted during the dataset creation process. Sharing insights from these assessments can help users understand potential ethical considerations and how they were meditated.

5. **Community Engagement**: Engage with the broader research community to continuously assess the dataset's impact. Encourage users to report any ethical concerns or misuse cases and be prepared to act on these reports to mitigate any harmful consequences. 

By addressing these suggestions, the authors can strengthen the safeguards around the responsible release of the Meta-Album dataset, mitigating potential risks and encouraging ethical research practices.",0
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","The authors of the paper have explicitly stated their adherence to proper licensing practices concerning the datasets used in their research, as mentioned in their response. The fact that they have dedicated a section (Appendix B) to license information of Meta-Album datasets demonstrates their commitment to crediting the original owners and respecting the terms of use. This section appears to include details about the licenses, ensuring transparency and proper attribution for the used datasets.

However, a more thorough review could enhance clarity and adherence to best practices:
1. **Explicit Mention of All Licenses**: Ensure every dataset mentioned in the paper is accompanied by a clear statement of its license, both within the main text (when first introduced) and in the dedicated appendix.
2. **Versioning and URLs**: Although the mentioning of licenses is in place, adding specific versions of the datasets and direct URLs where the licenses and datasets can be viewed would improve reproducibility and accountability. This is crucial since datasets often undergo updates that might have licensing implications.
3. **Clarification on Derived Assets**: If any datasets were modified or combined to create new datasets (which seems likely given the nature of Meta-Album), clarify the licensing of these derived assets. It's essential to ensure that the new datasets' creation and distribution do not violate the original terms.
4. **License for Redistribution**: If the authors are redistributing existing datasets, either in their original form or a modified version, it's crucial to ensure that the original licenses permit such redistribution.

In summary, while the authors have taken significant steps to ensure proper credit and adherence to licensing terms, further detailing and clarification could enhance compliance and serve as a robust model for best practices in dataset usage and distribution within the academic community.",1
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","The given answer aligns with the paper content regarding the introduction and documentation of new assets, namely the Meta-Album meta-dataset. The authors have indeed detailed their methodology in documenting the datasets through meta-data files and datasheets as stated, which are presented in the appendices. Additionally, the creation of a dedicated website for accessing the datasets and associated information further supports their claim of thorough documentation. The paper thereby adheres to the guidelines for asset documentation by providing structured information on the datasets included in the Meta-Album, including details on their preparation, quality control measures, and how users can access and utilize these assets.

Suggestions:
1. For further improvement, the authors could include explicit information on the licensing for each dataset included in the Meta-Album, ensuring users understand the permissions and restrictions for usage. This could be added directly in the paper text for quick reference and in the documentation provided on the website.
2. It would be beneficial to discuss any limitations or biases inherent in the datasets included in the Meta-Album meta-dataset. This transparency can aid researchers in properly interpreting their experimental results and guide future work in addressing these limitations.
3. While the provided website is a valuable resource, ensuring it remains updated with the latest information and datasets will be crucial. Adding a section on the site for user feedback or contributions could further enhance the Meta-Album as a community-driven asset.
4. Clarification on how consent was obtained (if applicable) for any data derived from human subjects or where copyright considerations apply would be valuable in aligning with ethical research standards.",1
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,N/A,This does not apply on our research as we are not doing any crowdsourcing experiments,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","The answer provided by the authors is accurate and consistent with the content of the paper. The paper introduces Meta-Album, a meta-dataset aimed at facilitating research in few-shot learning, meta-learning, and similar domains within computer vision, specifically focusing on image classification. The work predominantly involves curating and processing existing datasets rather than conducting crowdsourcing experiments or research with human subjects. Hence, the requirements for including instructions, screenshots, and compensation details for participants do not apply to this study.

Constructive suggestions for improvement could include:

1. Clarify in the methodology or ethical considerations section that the study did not involve primary data collection from humans, underscoring the secondary nature of data use for transparency.

2. While the current answer is sufficient, future versions of related work could benefit from a brief mention or disclaimer regarding the ethical considerations around data use, even if the datasets are publicly available or derived from existing sources. This is especially relevant as ethical guidelines and standards for AI research continue to evolve.

3. For readers less familiar with the distinction between types of data collection and analysis, a simple sentence acknowledging awareness of ethical standards for research involving human subjects (even to confirm that such standards were not applicable in this case) might add value to the presentation of the work.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,N/A,This does not concerns us as we are not dealing with human subjects in our research.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The authors' justification for not addressing potential risks incurred by study participants and not obtaining Institutional Review Board (IRB) approvals based on the pretext of ""not dealing with human subjects"" is inconsistent with the comprehensive nature of preparing a dataset, such as Meta-Album, described in the paper. Even though the direct involvement of human participants in the creation of the Meta-Album dataset might be minimal or not applicable, the downstream applications of such datasets could potentially involve risks related to privacy, bias, fairness, and use in sensitive applications. Therefore, the ethical implications and potential risks associated with the dataset's usage should have been considered, even if traditional IRB approval was not directly required.

Constructive and actionable suggestions:
1. **Ethical Consideration Section**: The authors should consider adding a dedicated section on ethical considerations and potential risks related to the use of their dataset. This could include discussions on privacy, bias, fairness, and the intended as well as unintended uses of the dataset.
2. **Dataset Usage Guidelines**: Offer guidelines or recommendations on how the dataset should be ethically used, including any limitations on its use in certain types of research or applications.
3. **Bias and Fairness Analysis**: Conduct and present an analysis of potential biases within the dataset and their implications for fairness in machine learning applications leveraging the Meta-Album.
4. **Community Engagement**: Encourage and facilitate the engagement of the broader research community in discussions about ethical use and potential risks associated with the dataset. This could be achieved through workshops, forums, or participatory design sessions.
5. **Iterative Ethics Review**: Consider establishing an ongoing review process for ethical considerations tied with the dataset’s evolution. As the dataset grows and diversifies, new ethical considerations may arise that warrant attention and mitigation strategies.",0
