Question,Question_Title,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","The justification provided aligns well with the content of the paper, as the abstract and introduction sections effectively summarize the main contributions and the scope of the paper. The Meta-Album, a multi-domain meta-dataset designed to facilitate research in few-shot learning, cross-domain meta-learning, and other tasks, is introduced as the core contribution. This contribution is accurately reflected and detailed in both the abstract and the introduction, establishing a clear and accurate starting point for the readers about what the paper entails and achieves.

The abstract and introduction also emphasize the diversity and scale of the datasets included in Meta-Album, the pre-processing and formatting steps taken to ensure uniformity across datasets, and the features that make Meta-Album a continually extendable resource for the research community. These claims are not only ambitious but are directly supported by the content in the subsequent sections of the paper, thereby meeting the guidelines for the claims to match theoretical and experimental outcomes. Furthermore, the presentation of challenges and the outline of future plans for extending Meta-Album stress the paper’s contribution towards establishing a rolling benchmark for few-shot learning that is designed to be enhanced and expanded over time.

However, to further improve the transparency and utility of the paper, the authors could consider adding the following:

1. **Explicit Identification of Limitations**: While the paper does a good job of outlining the contributions and scope, explicitly detailing the limitations could provide readers with a more rounded understanding of the potential and limits of Meta-Album. This includes any biases in dataset selection or inherent limitations in the few-shot learning models tested.

2. **Comparison with Related Work**: More detailed comparisons with related benchmarks and datasets could further contextualize the contributions of Meta-Album. Specifically, highlighting not just the scale and diversity but also the unique challenges or learning scenarios enabled by Meta-Album compared to existing resources.

3. **Detailed Use Cases**: While several potential applications and research directions are mentioned, concrete examples or case studies of Meta-Album being utilized to solve novel problems or to advance the state-of-the-art in few-shot learning could enhance the reader's appreciation of its value. 

These suggestions aim to enhance clarity, provide a broader context for the contributions, and underline the practical implications of the work presented.",1
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,We have clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","Justification: The given answer and justification provided by the authors do not fully align with the comprehensive evaluation criteria outlined in the guidelines for discussing limitations. While the authors mention that they have explained the limitations in the ""Discussion and conclusion"" section, it is important to carefully assess whether the discussion adequately covers all aspects outlined in the guidelines, such as assumptions, scope of claims, factors influencing performance, computational efficiency, and considerations of privacy and fairness.

Suggestions:

1. **Assumptions and Robustness**: It would be beneficial for the authors to delve deeper into the assumptions made during their research and explore how potential violations of these assumptions could impact the results. They should consider scenarios where these assumptions may not hold and discuss the expected performance under such circumstances.

2. **Scope of Claims**: The authors should critically assess the scope of their claims, especially considering the diversity of the datasets and the number of runs conducted during their experiments. It would be helpful to discuss how the results might vary across different domains or dataset characteristics.

3. **Factors Influencing Performance**: A more thorough discussion on the factors that directly impact the performance of their approach, such as image resolution, lighting conditions (for image-related tasks), or the presence of technical jargon (for text-related tasks), would add depth to the paper.

4. **Computational Efficiency**: The paper should include a dedicated section evaluating the computational efficiency of the proposed methods, highlighting how the algorithm's performance scales with increasing dataset sizes or complexity of tasks.

5. **Privacy and Fairness**: If relevant to the research, the authors should consider discussing the implications of their work on privacy and fairness, especially if the proposed approach could potentially be applied in sensitive applications.

6. **Transparency and Honesty**: Encourage the authors to maintain transparency and honesty about the limitations of their work. This not only contributes to the integrity of the research community but also provides valuable insights for future work aiming to address these limitations.

By addressing these suggestions, the authors can significantly improve the discussion on the limitations of their work, aligning more closely with the guidelines and providing a well-rounded view of their research contributions and areas for future exploration.",0
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,N/A,This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The answer and justification provided by the authors are consistent with the paper content, given that the primary focus of the paper is on the creation and utilization of the Meta-Album meta-dataset for few-shot image classification tasks, and not on theoretical analysis or results. The nature of the research does not involve the development or proof of mathematical theorems, making the question not applicable.

However, to ensure full compliance and critical review:

1. It's important to verify that the paper does indeed focus exclusively on empirical research and dataset creation without delving into theoretical foundations that would require proofs or formal theorems.
2. Even in empirical research, underlying assumptions about the dataset, the choice of algorithms for baseline comparisons, and the experimental setup should be explicitly stated to ensure the research's transparency and reproducibility.

Actionable suggestions for the authors:
1. Ensure that any assumptions made during the dataset creation, preprocessing, or experimental evaluation are clearly documented within the paper, even though formal proofs are not required in your case.
2. If any part of your research indirectly relies on theoretical foundations or earlier theoretical work (e.g., the algorithms used for comparison), provide references or brief descriptions to guide the reader.
3. Consider including a discussion section that highlights the empirical findings' implications in the context of existing theoretical work, if relevant. This would enrich the paper without needing to provide new theoretical contributions.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The answer provided by the authors is insufficient for a number of reasons. Firstly, while the authors mention that the code is publicly available on their GitHub repository, they do not specify if the repository includes detailed instructions on how to use the code to reproduce the experimental results. The inclusion of command lines to execute, necessary dependencies, and environment setups is crucial for reproducibility but is not addressed in the provided response.

Secondly, merely stating that the datasets are accessible via the website does not ensure reproducibility. Detailed instructions on how to access and prepare the data for experiments are necessary. This includes information on downloading the datasets, any preprocessing required, and how the datasets should be split for training, validation, and testing. The authors did not mention if such instructions are provided.

Lastly, the response does not address whether scripts are available to reproduce all experimental results for the newly proposed method as well as for the baselines. This omission is significant since the ability to reproduce results for both the proposed method and the baselines is fundamental to validate the claims made in the paper.

**Constructive and actionable suggestions:**
1. **Code Repository:** Ensure the GitHub repository includes a detailed README file with step-by-step instructions for reproducing the experiments. This should cover the environment setup, necessary dependencies, and how to run the code. If different settings were used for different experiments, these should be clearly documented.

2. **Data Access and Preparation:** Provide explicit instructions on how to download the datasets from the website. If any preprocessing is needed (e.g., resizing images, splitting datasets), include scripts or detailed guidelines for these steps.

3. **Experiment Reproduction:** Include scripts or detailed instructions to reproduce the results for both the proposed method and the baselines. Specify any parameters or configurations used in the experimental setup.

4. **Supplemental Material:** Consider including a more comprehensive supplemental material section appended to the paper, where you can provide exhaustive details on the experimental setup and any other information that might aid in reproducing your results.

5. **Anonymous GitHub Repository (for double-blind review):** If the review process is double-blinded, ensure the GitHub repository does not reveal the authors' identities. This might involve creating a new, anonymous GitHub account or making sure no personal details are present in the repository history or files.

By addressing these points, the authors can significantly improve the reproducibility of their experimental results and thereby strengthen the paper's contributions.",0
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The answer provided by the authors matches the contents and guidelines outlined in the paper. The authors mention openly providing the code on their GitHub repository and detail how to access the datasets on their website, which aligns with the question's criteria for sufficient instructions to faithfully reproduce the main experimental results.

However, while agreeing with the answer, it's crucial to ensure that the provided GitHub link and website contain comprehensive instructions for reproducing the results. Additionally, it's necessary that the environment setup, dependencies, and any required pre-processing steps are clearly documented to facilitate easy replication of the work.

**Constructive Suggestions:**
1. **Detailed Documentation:** Ensure the GitHub repository contains a README.md file with step-by-step instructions for environment setup, dependencies installation, and running the code. This should include precise commands that can be directly executed.
2. **Code Comments:** Include comments within the code to explain the functionality of complex or critical sections. This aids understanding and facilitates modifications by other researchers.
3. **Requirements File:** Provide a requirements.txt or an environment.yml file specifying all the necessary libraries and their versions. This helps in creating an identical environment to that used in the original experiments.
4. **Data Accessibility:** Clarify the process of accessing both the raw and pre-processed datasets on the website. If there are any access restrictions or requirements for usage agreements, make these clear to the potential users.
5. **Supplemental Material:** If not already included, consider adding a supplemental material section to the paper to include any additional information that could assist in reproducing the results but was too detailed for the main paper body.
6. **Anonymization:** For submissions where anonymity is required, ensure that links to code and data are shared through anonymized platforms or generic file-sharing services that do not reveal the authors' identities.

By addressing these suggestions, the authors can enhance the reproducibility and accessibility of their experimental results, thereby strengthening the paper's contribution to the field.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","Justification for Score:
The justification provided by the authors is insufficient for a positive evaluation of the completeness of experimental setup details. While the authors mention that the experimental settings are explained in a specific section and that additional details can be found in the Meta-Album GitHub repository, this response lacks several key specifics that should be stated directly within the paper. These include the process of data splits (training, validation, and test), hyperparameter selection, optimizer details, and criteria for hyperparameter choices, which are essential for the reproducibility and thorough understanding of the experiments. Relying on external repositories for crucial experimental details can be seen as a limitation, as it requires readers to consult additional resources beyond the paper itself.

Constructive Suggestions for the Authors:
1. **Incorporate Crucial Details in the Paper:** It is advisable to include essential experiment details directly within the paper. While references to external repositories are useful, the paper should stand alone in providing all necessary information to understand and reproduce the results.

2. **Detail the Experimental Setup:** Expand the section on experiments to include specific information about data splits (how data is divided into training, validation, and test sets), the rationale behind the selection of hyperparameters, and the type of optimizer used. If certain choices were made based on empirical evidence or literature, this should be mentioned.

3. **Clarify External Resources:** If additional details are to remain in external repositories (e.g., GitHub), explicitly state what information can be found there, ensuring the reader knows what to expect. Moreover, ensure that these resources are well-maintained and accessible.

4. **Enhance Reproducibility:** Provide a detailed appendix or supplemental material that exhaustively lists all parameters, settings, and any scripts or commands necessary to replicate the experiments. This documentation should be thorough enough to enable other researchers to reproduce the study with minimal additional clarification.

By addressing these aspects, the paper could significantly improve its clarity regarding experimental procedures, thereby enhancing its utility and impact on the community.",0
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","Explanation:
The answer provided by the authors declares that error bars are reported in various figures and sections of the appendices, which suggests an attempt to provide statistical significance for their experiments. However, the justification lacks crucial details on how these error bars were calculated, what they represent (e.g., standard error, standard deviation), and under which assumptions. No specific mention is made of the variability factors captured by these error bars (e.g., initialization, experimental conditions). Furthermore, there is no explicit statement in the justification about whether the distributions of measurements were assumed to be normal, which is a critical factor when interpreting the error bars or confidence intervals. Lastly, the authors do not reference the method used to calculate the error bars (e.g., bootstrap, closed-form formula), which is essential for understanding the robustness of their statistical analysis.

Constructive Suggestions:
1. Clearly define the type of error bars reported (e.g., standard deviation or standard error of the mean) in both the figures where they are used and in the text.
2. Explain the method used to calculate the error bars. For example, if it's a standard error of the mean, detail whether it was derived from repeated experiments, cross-validation, or another method.
3. Clarify the factors of variability captured by these error bars. If the variability comes from different initializations, data splits, or other experimental conditions, explicitly state this in the text.
4. If assumptions about the distribution of the data were made (e.g., assuming normal distribution for the applicability of certain statistical tests), these should be clearly stated along with any tests performed to validate these assumptions.
5. For asymmetric distributions or where the measurement scale prohibits negative values, ensure that error bars do not imply impossible values (e.g., negative accuracy rates) and consider using confidence intervals that reflect the data distribution accurately.
6. Offer a section or a paragraph in the methodology that thoroughly explains the statistical methods used in the analysis of experimental results, providing readers with a clear understanding of how the results' statistical significance was assessed.

These suggestions aim to improve the transparency and robustness of the statistical analysis reported in the paper, thereby enhancing the trustworthiness of the findings.",0
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","The justification provided by the authors is partially accurate but lacks the detailed specificity requested in the guidelines for describing compute resources. While the authors mention that details about compute resources are provided in specific sections and appendices, a thorough review of the paper content shows that the information is somewhat general and may not fully meet the guidelines' detailed requirements.

Specifically, while the authors mention using NVIDIA GPUs (e.g., PNY GeForce RTX 2080TI and NVIDIA V100), they do not consistently provide explicit information about the type of compute workers (CPU or GPU specifics beyond a few instances), memory, and storage for all experiments. Additionally, the descriptions of computational resources are scattered across different sections and appendices, which might make it challenging for readers to find and assess the comprehensiveness of the information.

To fully address the reviewers' expectations and comply with the guidelines, the authors could consider the following constructive suggestions:
1. **Centralize Information:** Create a dedicated section or a more detailed table summarizing the computational resources used for each experiment, including the type and specifications of compute workers (CPUs and GPUs), memory, storage, and the internal cluster or cloud provider details.
2. **Time of Execution:** Explicitly mention the time of execution for each experimental run and provide estimates for total compute time. This information is crucial for researchers attempting to reproduce the experiments.
3. **Additional Compute Disclosure:** Disclose if the full research project required additional compute resources beyond what was reported for the experiments, including preliminary or failed experiments. This transparency will help in assessing the overall resource requirements for replicating the study’s findings.
4. **Uniformity in Description:** Ensure that the description of computational resources is consistent and detailed across all experiments. This includes specifying memory and storage capacities when relevant.
5. **Accessibility Considerations:** Discuss the accessibility of the computational resources used, highlighting any potential barriers for researchers with limited access to similar high-performance computing facilities. This could also include suggestions for adapting experiments to lower resource settings if possible.

Addressing these points will make the paper stronger by providing clearer, more accessible, and comprehensive information regarding the computational resources required, thereby facilitating reproducibility and fostering trust in the experimental results.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We completely comply with the NeurIPS Code of Ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","The justification provided by the authors is insufficient and non-specific. Simply stating compliance with the NeurIPS Code of Ethics without detailing how the research upholds various ethical considerations, such as fairness, transparency, privacy, and potential societal impacts, does not adequately answer the question. In the context of developing and introducing Meta-Album, a meta-dataset for few-shot learning, there are several ethical aspects that need to be addressed explicitly:

1. **Data Source and Privacy**: Given that Meta-Album integrates 40 open datasets from diverse domains, it's crucial to verify that all datasets included have been collected and processed in ways that respect individual privacy and consent. The authors need to clarify the steps taken to ensure that the data does not inadvertently contain sensitive or personal information.

2. **Bias and Fairness**: The authors need to discuss any bias present in the datasets and the potential impact on the fairness of models trained using Meta-Album. Given the variety of domains (e.g., humans, animals, vehicles), bias in terms of representation or annotation can lead to skewed algorithms that do not perform equally well across different groups.

3. **Transparency and Reproducibility**: Though the authors mention open-sourcing the code and making the dataset available, explicit details on accessibility, licensing, and any limitations on the use of the dataset and code are essential for ensuring transparency and enabling reproducibility.

4. **Societal Impact**: The potential applications of few-shot learning are vast and span critical areas such as healthcare, security, and education. The authors should consider discussing the broader implications of their work, including positive impacts (e.g., advancements in medical image diagnosis) and possible negative consequences (e.g., misuse in surveillance).

**Constructive Suggestions**:

- Provide a detailed ethics section addressing the above concerns, particularly focusing on data privacy, bias and fairness, transparency, and societal impact.
- Clarify the sourcing process for each dataset included in Meta-Album to ensure compliance with ethical data collection practices and respect for privacy.
- Discuss any measures taken to identify, mitigate, or document biases within the datasets and the potential impact on algorithm fairness.
- Offer detailed information on the availability of the dataset and code, including any restrictions, licensing information, and guidance for usage to encourage transparency and reproducibility.
- Reflect on the broader societal implications of advancing few-shot learning technologies, outlining both potential benefits and risks, and suggest guidelines for responsible use.",0
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","The answer provided by the authors seems to be in partial compliance with the guidelines, as it only mentions that there are no negative societal impacts and suggests a positive outlook on the development of the meta-dataset. However, it lacks a thorough analysis or discussion regarding both potential positive and negative societal impacts, which is required according to the guidelines provided.

The guidelines clearly state that authors should point out potential malicious or unintended uses, fairness, privacy, and security considerations, even if the work is foundational and not directly tied to specific applications or deployments. By stating that ""there are no negative societal impacts"" without a detailed examination, the response does not fully adhere to the expectations for addressing societal impact in their work. Furthermore, it also misses an opportunity to explore mitigation strategies for potential negative impacts or ways to enhance the positive impacts.

Constructive and actionable suggestions for improvement include:
1. **Analyzing Potential Negative Impacts**: The authors should consider and discuss potential negative societal impacts that could arise from the misuse of the meta-dataset or through the development of algorithms based on it. This could include concerns related to privacy, surveillance, fairness, and misuse in generating deceptive content.

2. **Discussing Mitigation Strategies**: In addition to identifying negative impacts, the paper could benefit from offering ideas or strategies to mitigate these risks. This can involve suggestions for ethical guidelines, monitoring tools, or restrictions on the use of the dataset.

3. **Expanding on Positive Impacts**: While the paper mentions fostering progress in few-shot learning and meta-learning as positive impacts, it could be beneficial to expand on how these improvements could contribute to societal benefits, such as advancing research in healthcare, environmental science, or education.

4. **Broader Considerations**: Encourage the authors to think beyond the immediate scope of their research and consider the broader societal, ethical, and legal contexts in which their work exists. This includes pondering on the long-term implications of their contributions to the field.

By addressing these suggestions, the paper could provide a more balanced and comprehensive view of the societal impacts of their work, aligning better with the conference's expectations for discussions on this topic.",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Yes,"Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","The answer provided by the authors appears consistent with the details presented in their paper regarding responsible release and usage of the datasets included in Meta-Album. They mention specifically verifying the licenses of all datasets to ensure they are free for use in academic research and provide their original licenses, which is a critical step in responsible data release. Moreover, the dataset descriptions and recommended uses, as well as the inclusion of a website with source code and further instructions, demonstrate an effort towards responsible dissemination and use of their resources.

However, while the authors have taken steps towards responsible release by checking licenses and providing usage recommendations, the response could be expanded to detail further safeguards against misuse, especially considering the breadth and variety of the datasets included in Meta-Album. Given the potential for misuse in some domains or with certain types of data, additional safeguards could strengthen this aspect.

**Constructive Suggestions:**
1. **Provide Detailed Usage Guidelines:** While the authors mention recommended uses, providing more detailed guidelines on ethical and appropriate use of the datasets could be beneficial. This could include examples of potential misuse and advice on avoiding such scenarios.

2. **Implement a Request Process:** For datasets with higher risks of misuse, consider implementing a process where researchers have to submit requests explaining their intended use. This could help monitor and control the use of sensitive or potentially harmful data.

3. **Community Feedback and Oversight:** Establish a mechanism for community feedback on data use and concerns about potential misuse. An oversight committee could review reported concerns and recommend actions.

4. **Promote Ethical Research Practices:** Encourage users of the datasets to adhere to ethical research practices, including considerations for privacy, fairness, and non-discrimination in their work using Meta-Album.

5. **Update Licenses if Necessary:** Regularly review and, if necessary, update the licenses and terms of use for the datasets to address emerging ethical concerns and ensure responsible use aligns with the latest standards in the field.

By implementing these suggestions, the authors could enhance the safeguards around the responsible release and use of the Meta-Album datasets, mitigating risks for misuse and reinforcing their commitment to ethical research practices.",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","The answer provided by the authors aligns well with the guidelines given for crediting the original owners or creators of the assets used in the paper and for explicitly mentioning the licenses and terms of use. The authors have confirmed that they release their datasets with proper licensing and have specifically mentioned that the original licenses for the datasets they use are detailed in Appendix B of the paper. This demonstrates an effort to respect and accurately present the terms of use and copyrights associated with the assets they have used, which is crucial for ethical research and reuse of these assets by others in the scientific community.

However, while the response aligns with the requirements, I would recommend the authors to ensure that they have also appropriately cited the original papers or sources of the datasets in the main body of their paper, not only in the appendix, and if possible, to include URLs or DOIs for these assets for easy accessibility. Additionally, when releasing their datasets, they should ensure to provide a clear statement of the license under which their dataset is released and make sure that this license is in accordance with the permissions granted by the original datasets' licenses. 

Constructive and actionable suggestions:
1. Ensure that each used dataset or asset is clearly cited in the paper's reference section, including the citation of the original paper or source.
2. Provide direct URLs or DOIs to the original datasets or assets, when available, to facilitate access for readers and future researchers.
3. Clearly state the license under which the Meta-Album dataset is released and ensure it is compatible with the licenses of the original datasets.
4. If modifications were made to original datasets, describe these modifications in the paper and clarify how these affect the terms of use or licensing.
5. In Appendix B or in a dedicated section of the accompanying website, provide a detailed explanation of how the licensing of the Meta-Album was determined based on the original datasets' licenses, to add transparency to the licensing process.",1
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","The answer provided by the authors aligns with the content and the guidelines provided. They have documented their datasets using meta-data files and provided datasheets for datasets in the appendices, making the assets well-documented. Moreover, they have established a website for easier access to datasets and related information, which is a good practice for asset documentation and dissemination.

Constructive and actionable suggestions for further improvement:
1. The authors could enhance the documentation by including more detailed examples of use cases, showing how researchers might utilize the datasets in their projects.
2. It would be beneficial to specifically mention the openness of their datasets, including any licensing information directly in the main text, not only in the appendices or on the website. This helps readers quickly understand the terms of use.
3. The documentation could be further improved by adding information about potential biases or limitations within the datasets, helping researchers to use them more effectively and ethically.
4. It would also be useful to explicitly address the issue of consent, especially for datasets containing human-related data, to ensure ethical standards are met. While this may not directly apply to the current datasets focusing on image classification, general guidelines on ethical considerations would enhance the documentation.
5. Finally, ensuring that the website and all links to datasets and documentation are maintained and updated regularly will improve the longevity and impact of the assets introduced in the paper.",1
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,N/A,This does not apply on our research as we are not doing any crowdsourcing experiments,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","The response given by the authors is consistent with the content of the paper provided. The paper introduces Meta-Album, a meta-dataset for few-shot learning and other machine learning tasks, focusing on the collection, preprocessing, and formatting of various open datasets for facilitating research in meta-learning, especially in cross-domain settings. It does not involve crowdsourcing experiments or research with human subjects; thus, the inclusion of instructions for participants or details on compensation is not applicable. 

The authors' justification is aligned with the nature of their work and follows the guidelines provided for answering this question. Since their research focuses on the aggregation of existing datasets for machine learning experiments, rather than conducting studies or experiments that include human subjects, the answer of N/A (Not Applicable) is accurate and appropriate.

However, for the purpose of thoroughness in review and feedback, here are some constructive and actionable suggestions for the authors, unrelated to the question asked but valuable for improving the clarity and richness of the paper:

1. **Enhance Dataset Documentation:** While the paper does provide a significant amount of detail about the datasets included in Meta-Album, further documentation, particularly regarding the original sources, the rationale behind dataset selection, and any biases or limitations within these datasets, would enrich the reader's understanding and potential application of Meta-Album.

2. **Future Work:** Discuss potential expansions of Meta-Album, including not just the addition of datasets but also the introduction of challenges or tasks that could utilize the meta-dataset. Insights into how Meta-Album could evolve to better support emerging areas in meta-learning and few-shot learning would be useful.

3. **Clarification on Dataset Permissions:** For further clarity and to ensure adherence to ethical standards, explicitly stating the process of obtaining permissions for using the datasets or any communications with original dataset authors could be helpful. Even though the datasets included are open, outlining this process adds transparency.

4. **Impact on the ML Community:** A deeper discussion on how Meta-Album addresses current gaps in meta-learning research and how it might impact future studies would strengthen the contributions section. Highlighting specific research questions or hypotheses that Meta-Album enables the community to investigate could emphasize its relevance.

Overall, the response to the question regarding crowdsourcing and human subjects is correct, evidenced by a score of 1. However, the suggestions provided aim to enhance the paper's contribution to the scholarly community and its utility as a research resource.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,N/A,This does not concerns us as we are not dealing with human subjects in our research.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The answer given by the authors is indeed correct and in line with the content described in their paper. Based on the paper's abstract and description, the study introduces Meta-Album, an image classification meta-dataset, which is a collection of image datasets from various domains prepared for facilitating research in few-shot learning, meta-learning, and related machine learning tasks. There is no mention or indication that the research involved human subjects or crowdsourcing activities that would necessitate IRB approvals or entail risks to study participants. The research primarily focuses on the assembly and processing of existing image datasets for computational experiments, which falls outside the purview of human subjects research.

Justification:
- The content of the paper revolves around data collection, preprocessing, and the development of a meta-dataset for image classification tasks. These activities do not involve human subjects directly.
- The emphasis is on technical and methodological contributions to the field of machine learning, particularly few-shot learning and meta-learning. Such studies typically do not require IRB approvals unless they involve sensitive data or direct interaction with human participants.
- Given the nature of the research, the N/A response to the question about risks to participants, disclosure of risks, and IRB approvals is accurate and appropriate.

Constructive Suggestions for Improvement:
1. While the response is appropriate, the authors could benefit from explicitly stating in the paper that the study did not involve human subjects or use of sensitive data, for clarity to the readers.
2. It might be helpful to include a brief ethical consideration section discussing the selection and use of the datasets, especially if any of the datasets could potentially contain sensitive images, to preempt any ethical concerns from the readers or reviewers.
3. Consider explicitly stating any measures taken to ensure the ethical use of the datasets included in Meta-Album, such as verifying dataset licenses and ensuring compliance with data protection regulations, which can add to the credibility and ethical considerations of the research.",1
