Question,Question_Title,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","The author's answer is consistent with the expectation set by the question's guidelines. The paper's abstract and introduction indeed accurately reflect its contributions and scope. The abstract introduces ""Meta-Album,"" a novel image classification meta-dataset designed to facilitate research in few-shot learning, meta-learning, and related fields. It highlights the dataset's diversity, size, and extensibility, all of which are central to the paper's contributions. Moreover, the introduction further elaborates on the motivation behind Meta-Album, its composition, and its potential applications, effectively setting the stage for the detailed exploration and validation that follows in the paper.

However, it's important for the review to reflect a critical and detailed evaluation. While the response correctly assesses the alignment between the paper's claims and its content, it could be more informative by detailing aspects of how the claims are supported by the results presented in the paper. For instance, discussing the evaluation of Meta-Album in facilitating cross-domain few-shot learning would strengthen the justification. Moreover, addressing any gaps or areas where the contributions could be more robust or where future work is needed would provide constructive feedback for the authors.

Suggestions for improvement:
1. Include specific examples from the paper that demonstrate how the claims made in the abstract and introduction are supported by theoretical and experimental results. This could involve mentioning the sections where the dataset's evaluation is discussed or highlighting any benchmarks the dataset has set or participated in.
  
2. Address any potential limitations or challenges explicitly mentioned in the paper or those perceived by the reviewer. Doing so will provide a more balanced view and suggest areas for future improvement.

3. Encourage the inclusion of a discussion section in the paper that reflects on the extent to which the goals set out in the abstract and introduction have been achieved, and where further research could lead. This would offer a clearer perspective on the paper's contributions within the broader research context.

Overall, while the response is correct, providing a more detailed critique and constructive suggestions would enhance the utility of the review for both the authors and the conference committee.",1
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,We have clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","The authors' assertion that they have discussed the limitations of their work in the designated section is consistent with the common practice of including such a discussion in academic papers. The ""Discussion and Conclusion"" section typically offers a space to consider the scope, potential biases, computational efficiency, and other relevant factors that could affect the generalizability and applicability of the research findings.

However, for a more detailed and critical review, it would be necessary to closely examine the specific limitations mentioned in the ""Discussion and Conclusion"" section of the paper. This examination would assess how thoroughly the authors have addressed potential weaknesses, assumptions, and the robustness of their solutions. While the authors have affirmed that they have discussed limitations, the depth, and breadth of this discussion play a crucial role in evaluating the transparency and scientific rigor of the research.

**Constructive and Actionable Suggestions:**

1. **Explicit mention of limitations:** Ensure that the limitations discussion is clear and explicit. It should be easy for readers to find and understand the potential limitations and their implications on the research findings.

2. **Consider a range of factors:** Broaden the limitations discussion to include a diverse range of factors such as data biases, model assumptions, the applicability of results to real-world settings, computational efficiency, and scalability issues.

3. **Empirical evaluation scope:** If the empirical evaluations were limited in scope (e.g., limited datasets, specific domains, or conditions), clearly state this and discuss how it might affect the generalizability of the results.

4. **Address computational resources:** Discuss the computational resources required for the proposed methods, especially if they involve intensive computations or require specialized hardware. This helps in assessing the practicality of deploying such methods in various settings.

5. **Potential for future work:** While discussing limitations, it could be useful to point towards potential future research directions that could address these limitations. This not only underscores the honesty and self-awareness of the researchers but also contributes to advancing the field by highlighting areas ripe for further exploration.

6. **Impact on fairness and privacy:** If applicable, explore any possible implications of the proposed methods on privacy and fairness. This is increasingly important in ensuring that technologies do not inadvertently perpetuate biases or violate privacy norms.

By addressing these suggestions, the authors can strengthen the credibility of their work and contribute positively to the ongoing scientific discourse in their field.",1
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,N/A,This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The authors' response is correct and justifiable given the content and nature of their research. Their project focuses on the introduction and use of a multi-domain meta-dataset, Meta-Album, for few-shot learning and other related tasks. The paper is empirical and centered around dataset creation, preprocessing, and experimental evaluation rather than theoretical analysis or derivation of new theoretical results. Therefore, the question regarding the provision of full sets of assumptions and complete proofs for theoretical results does not apply to their work.

The response is consistent with the paper's content, which is oriented towards practical application and experimentation rather than theoretical exposition. 

However, for completeness and to further improve clarity for readers, the authors could consider the following suggestions:
1. Explicitly state in the main text or methodology section that the focus of the paper is on empirical research and the creation of a practical tool/resource for the research community, thus making it clear why theoretical results and proofs are not part of the scope.
2. Though not applicable in this case, providing a section on potential theoretical extensions or implications of utilizing the Meta-Album for few-shot learning could enrich the discussion and encourage future theoretical work in this direction.

These suggestions aim to enhance the paper's transparency regarding its empirical focus and to potentially bridge the gap between practical and theoretical research within the area of few-shot learning and meta-datasets.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The justification provided by the authors for the question about the full disclosure of information needed to reproduce the main experimental results is sparse and lacks specific details that would help in the replication of the results. While mentioning the public availability of the code in a GitHub repository and dataset access via a website is a good start, the authors do not provide any concrete details on the following essential aspects required for reproducibility:

1. **Exact Environment Setup**: The authors did not mention specifics about the software environment, including the versions of the programming language (e.g., Python), libraries (e.g., PyTorch, TensorFlow), and any other dependencies required to run the code. This is critical as different versions of libraries or the language itself can lead to variations in experimental results.

2. **Detailed Instructions and Commands**: There is no mention of specific commands or step-by-step instructions required to reproduce the results. For complete reproducibility, the paper should include the exact commands that need to be executed, possibly including data preprocessing steps, model training, evaluation, and plotting of results.

3. **Data Preparation Details**: Although the authors mention accessing datasets through their website, they do not provide details on data access and preparation. For full reproducibility, the authors should describe how to download the raw data, preprocess it, and format it in a way that is compatible with their code.

4. **Experiment Reproducibility**: The authors did not specify which experiments are reproducible with the provided scripts and which are not, nor did they explain why some experiments might not be included in the reproducibility scripts.

5. **Anonymized Resources**: Since this review is part of the NeurIPS submission process and is supposed to maintain anonymity, the authors should ensure that any provided links or resources are anonymized appropriately.

Constructive suggestions for the authors include:

1. **Environment Details**: Provide a detailed description or a requirements.txt file listing all the software dependencies and their versions.
   
2. **Comprehensive Instructions**: Include a README file or an appendix in the paper with detailed instructions for reproducing the experiments, including commands for data preprocessing, model training, and evaluation.

3. **Data Access and Preparation**: Clarify the steps required for downloading, preprocessing, and formatting the datasets for use with the provided code.

4. **Clarify Reproducibility Scope**: Clearly state which parts of the experiments can be reproduced using the provided scripts and provide a rationale for any omitted experiments.

5. **Ensure Anonymity**: Ensure that all provided links and resources are anonymized in line with NeurIPS submission guidelines.

Addressing these points would significantly improve the reproducibility of the research and align the submission with the expectations set by the guidelines.",0
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The answer provided by the authors is consistent with the expectations outlined in the guidelines given their assertion of open access to code on a GitHub repository and dataset details on their website. Furthermore, the authors have indeed included supplemental material that likely provides instructions for reproducing the main experimental results, adhering to the guidelines for reproducibility.

However, the authors' justification lacks detail regarding the specific parts of the paper's experiments that are reproducible, whether all experimental results can be reproduced using the provided code and data, and if there are any omitted experiments, the reason for their omission. Moreover, the justification does not explicitly mention the availability of instructions on data access and preparation, which is crucial for faithful reproduction of the paper’s results.

**Constructive Suggestions:**
1. **Explicit Clarification:** Clearly state which experimental results can be reproduced with the provided code and data. If any results are not included in the reproducibility scope, explain why.
2. **Detailed Instructions:** Ensure that the supplemental material contains detailed instructions on accessing raw, preprocessed, intermediate, and generated data. This should include the exact commands and environment settings needed to reproduce the results.
3. **Environment Details:** Provide detailed information concerning the computational environment used for the experiments (e.g., software versions, hardware specifications) to minimize variability in reproducing the results.
4. **Verification:** It would be beneficial if the authors could also provide a script or a document verifying the correctness of the reproduced results against the paper’s findings.
5. **Anonymized Materials:** Confirm that any provided URLs or links to code and data are anonymized if necessary during the review process, following the guidelines for preserving anonymity.

Overall, while the authors' response aligns with the requirements, enriching the justification with these details would strengthen the assertion of reproducibility and facilitate the replication of their findings by other researchers in the community.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","Explanation:
The justification provided by the authors for answering ""Yes"" to the question about specifying all training and test details is insufficient and somewhat inconsistent with the rigorous standards expected for reproducibility and understanding of the results. While the authors mention that experimental settings are explained in Section 3.2 and additional details can be found in the Meta-Album GitHub repository, the answer lacks specific information on how certain critical aspects were handled, such as:

- Data splits: How the data were divided into training, validation, and test sets is not detailed in the justification. This information is crucial for understanding the generalization of the models tested.
- Hyperparameters and how they were chosen: The justification does not mention if the paper provides details on the choice of hyperparameters, which plays a significant role in the performance of machine learning models.
- Type of optimizer: There is no mention of the optimizer used in the experiments, which is an essential aspect of training deep learning models.

Constructive and Actionable Suggestions:
1. Provide more detailed information in the main body of the paper or in an appendix about the data splits used for the experiments. Specify the ratio or the exact number of images/instances used for training, validation, and testing.
2. Include a section or a detailed description about hyperparameter tuning. Explain the rationale behind choosing specific hyperparameters and whether any automated search techniques (like grid search or random search) were employed.
3. Clearly state the type of optimizer used for training the models, along with its settings such as learning rate, momentum (if applicable), and any scheduler for learning rate adjustments.
4. If feasible, embed this detailed information directly within the paper to ensure that the experimental setup can be understood without requiring readers to visit external links or repositories.
5. Ensure that the GitHub repository linked for additional details is well-organized and contains clear instructions and documentation for replicating the experiments, including pre-processing steps, model definitions, and evaluation protocols.

By incorporating these suggestions, the paper could significantly improve its reproducibility and allow the scientific community to better appreciate and build upon the presented work.",0
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","The authors' answer states that error bars are reported in figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, indicating that these error bars are indicative of the statistical significance of their experiments. However, the justification provided in the review does not go into detail about how these error bars were calculated, what variability they capture, or any assumptions made during their calculation, as detailed in the guidelines provided.

The guidelines specifically mention several critical points regarding the reporting of statistical significance, including clarity on whether the error bars represent standard deviation or standard error, the method of calculation, and assumptions regarding the distribution of errors. Without such details, it is challenging to assess the statistical rigor behind the reported error bars. Therefore, based solely on the provided information in the review, there seems to be an inconsistency with the content of the paper and the completeness of the justification for this question.

Constructive and actionable suggestions:
1. Provide a detailed explanation in the methodology section on how the error bars were calculated for figure-2, figure-3, Appendix E, and Appendix F. This includes specifying whether they represent standard deviations or standard errors, the calculation method used, and any assumptions about the distribution of errors.
2. Clarify the source of variability that the error bars are capturing (e.g., different train/test splits, initialization seeds, or parameter variability) to provide the reader with a clear understanding of what the error bars represent.
3. If the assumption of normal distribution is made for the calculation of error bars, this should be stated explicitly. If possible, conduct tests to verify this assumption, or consider using alternative methods suitable for asymmetric distributions.
4. Enhance the description in the text referring to the figures and appendices where error bars are shown, detailing their statistical significance and how they support the paper's claims. This will not only improve the clarity of the statistical analysis but also strengthen the overall credibility of the experiments.",0
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","Justification Review:
The justification provided by the authors does not fully comply with the guidelines for disclosing computer resources needed to reproduce experiments. The authors claim that details about compute resources used are provided in Section 3.2 and Appendices E and F. However, upon review, the information given in these sections does not adequately cover all aspects outlined in the guidelines. Specifically:

1. **Type of Compute Workers:** The paper mentions the use of ""single PNY GeForce RTX 2080TI GPUs with 11GB of VRAM or a single NVIDIA V100 with 16GB of VRAM"" in some instances (Appendix E and F), which partially satisfies the guideline requirement. However, it does not comprehensively indicate whether CPUs were used alongside GPUs, or the specific configurations (e.g., number of CPUs/GPUs, model names) for all experiments.

2. **Memory and Storage:** While the paper mentions the VRAM size of the GPUs used, it does not provide information regarding system memory (RAM) or storage requirements, which are crucial for reproducing experiments.

3. **Amount of Compute Required:** The paper lacks detailed breakdowns of the compute required for individual experimental runs. It mentions execution times in some contexts (e.g., ""Each experimental run took at most 24 hours on the former GPU""), but it does not estimate the total compute needed nor does it provide execution times for all experiments.

4. **Additional Compute Disclosure:** There is no disclosure on whether the full research project required more compute than reported for the experiments presented in the paper. It is unclear if preliminary or failed experiments that didn’t make it into the paper consumed additional compute resources.

Constructive and Actionable Suggestions:
1. Clearly specify the types of compute resources used for each experiment, including the make and model of GPUs and CPUs, if applicable.
2. Include detailed information on the memory (RAM) and storage that were essential for conducting the experiments.
3. Provide a more comprehensive breakdown of compute time required for individual experiments and estimate the total compute resources needed for reproducing the experiments in the paper.
4. Disclose if any additional compute resources were utilized for preliminary studies or experiments that were not included in the final paper, to give readers a complete understanding of the compute demands of the research project.

By addressing these points, the authors can enhance the reproducibility of their experiments and adhere more closely to the guidelines for disclosing computer resources.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeruIPS code of ethics,Yes,We completely comply with the NeurIPS Code of Ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","The answer provided by the authors is consistent with the content of the paper, and no evidence or reason was provided to doubt their compliance with the NeurIPS Code of Ethics. The paper introduces Meta-Album, a multi-domain meta-dataset designed for few-shot learning, transfer learning, and meta-learning research. The creation and use of such a dataset inherently align with ethical guidelines focused on promoting research and development in the field of machine learning. Furthermore, the paper describes a transparent and open process for data collection, preparation, and sharing, which adheres to the principles of ethical research.

However, to ensure thorough compliance and provide a more satisfying response for review, the authors could have elaborated on specific aspects of the NeurIPS Code of Ethics, showing how their research and the construction of Meta-Album adhere to these guidelines. This could include discussions on data privacy (especially considering some image datasets might contain identifiable features), consent (where applicable), and fairness (ensuring the dataset's diversity and reducing biases).

Constructive and actionable suggestions for the authors include:
1. Elaborate on the steps taken to ensure that the datasets included in Meta-Album comply with privacy and consent guidelines, considering images are sourced from various domains and publicly available datasets.
2. Discuss any measures implemented to assess and mitigate potential biases in the dataset, which is crucial for ethical machine learning research.
3. Provide information on the licensing of the datasets included in Meta-Album to ensure that they can ethically be used for academic research, as claimed.
4. Considering the Meta-Album's goal of being expandable, outline guidelines or criteria for future dataset contributions to ensure they also comply with the NeurIPS Code of Ethics.",1
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ",Justification for,0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Yes,"Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","The paper presents a responsible approach towards the release of the 'Meta-Album' dataset ensuring legal and ethical considerations. The datasets included in Meta-Album have been carefully curated with respect to their original licenses, as explicitly mentioned in the paper. Furthermore, the paper provides a detailed account of the licenses under which each component dataset is released, demonstrating diligence in ensuring that the data release complies with the terms set by the original creators. This approach aligns well with safeguarding against potential misuse by clearly defining the usage terms and respecting the intent of the original data providers.

However, while the safeguard measures described are primarily legal and ethical, there is room for further elaboration on technical safeguards, particularly for datasets that could potentially be misused if not handled correctly. For instance, data that could potentially train biased AI models or infringe on privacy should be accompanied by guidelines on ethical use and data handling practices to mitigate such risks.

Constructive Suggestions:
1. **Technical Safeguards**: Beyond legal compliance through licenses, the authors could consider adding technical safeguards or best practice recommendations for the ethical use of the 'Meta-Album' datasets. This could include guidelines on privacy preservation, data anonymization techniques, and recommendations to avoid training biased models.

2. **Usage Guidelines and Access Control**: Where applicable, introduce more specific usage guidelines that researchers must agree to before accessing the dataset. This could be particularly relevant for sensitive or potentially harmful datasets, where controlled access might mitigate misuse risks.

3. **Transparency on Content Moderation**: If any content moderation or filtering has been performed (for example, to remove unsafe images), it could be beneficial to describe the process, tools, or criteria used. This enhances transparency and helps users understand the dataset's limitations and considerations.

4. **Community Engagement and Feedback Loop**: Establish a mechanism for community feedback on dataset use and issues encountered, including ethical concerns. This can help in continuously improving the dataset management practices and addressing any unforeseen misuse or ethical issues.

Overall, the 'Meta-Album' paper shows a strong commitment to responsible data release. Expanding on the described safeguard measures with additional technical and community-driven efforts could further strengthen the ethical stance and promote responsible use of the dataset.",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","The authors have properly credited the creators and original owners of the datasets used in the creation of Meta-Album, a comprehensive meta-dataset. The inclusion of Appendix B in the paper that explicitly lists the license information for each dataset within Meta-Album shows due diligence in respecting the terms of use and licenses of the original datasets. This approach aligns well with best practices for using and referencing existing assets in academic work.

However, to further strengthen the paper and address any potential oversights, I recommend the following:
- Ensure that each dataset mentioned in Appendix B indeed corresponds to the licenses specified. A direct link to the licenses or the datasets' respective repository, if available, would provide clearer traceability and credibility.
- For datasets where the original license could not be identified and a default CC BY NC 4.0 license is applied, it might be helpful to document the effort made to determine the original license or any communication with the original creators for clarification.
- Consider including a statement or discussion on any limitations or challenges faced due to licensing restrictions and how they were addressed or could affect the use of Meta-Album.
- If Meta-Album or any of its components are released as part of this work, clearly state the chosen license for Meta-Album itself and ensure it is compatible with the licenses of the included datasets.

These suggestions aim to enhance the transparency and reproducibility of the work with regard to licensing and the use of existing assets.",1
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","The authors' answer that new assets introduced in the paper are well documented, and the documentation is provided alongside the assets, appears consistent with the paper content. The given justification matches the detailed approach described in the paper regarding the handling and documentation of the Meta-Album dataset. Specifically, the paper refers to meta-data files and datasheets for datasets included in the appendices, a dedicated website for accessing datasets and related information, and GitHub repositories for software related to the preparation and evaluation of the datasets.

However, critical review suggests that while the documentation and accessibility have been addressed commendably, the paper could still enhance clarity on a few fronts to further align with the guidelines provided:
1. **License Information:** Although the paper mentions the dataset licenses in the appendix, incorporating explicit statements within the main text on dataset licenses would improve visibility and understanding of usage limitations and ensure users are well-informed about legal considerations.
2. **Consent for Asset Use:** The documentation does not explicitly discuss how consent was obtained for the use of assets contained within the datasets, especially for datasets curated or adapted from existing sources. Clarifying this aspect would reinforce ethical considerations and transparency.
3. **Limitations and Training Details:** While extensive documentation on dataset preparation and structuring is provided, the paper could benefit from a clearer discussion on the limitations of the datasets and detailed training configurations used in the baseline methods' experiments. This would enhance the utility and reproducibility of the datasets and associated benchmarks.
4. **Anonymization of Assets (if applicable):** Given the paper’s context, anonymization might not be directly applicable. However, ensuring that all personal or sensitive data within the datasets is appropriately anonymized or consented for use would address privacy concerns.

**Actionable Suggestions:**
- Include explicit statements in the main text about the licenses of the datasets and directly address the consent obtained for using various assets.
- Elaborate on the limitations of the datasets and potential biases in the documentation to guide users on appropriate uses of the datasets.
- Provide more detailed training configurations and parameters for the baseline experiments to facilitate replication and extension by other researchers.
- Ensure all personal or sensitive information within the datasets is either anonymized or appropriately consented for public use, adhering to ethical guidelines for dataset release.",1
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,N/A,This does not apply on our research as we are not doing any crowdsourcing experiments,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","The authors have correctly indicated that their research does not involve crowdsourcing experiments or research with human subjects, which makes this question not applicable (N/A) to their paper. The justification provided aligns with the content and scope of the paper, which focuses on introducing a meta-dataset for few-shot image classification and does not involve direct experimentation with human participants or crowdsourcing methodologies. Therefore, there's no requirement for including instructions given to participants, screenshots, or compensation details.

Suggestions:
- Although not applicable in this case, for future projects involving human subjects, ensure detailed procedures, instructions, and compensation plans are included as per ethical guidelines.
- Continue to clearly delineate the scope of research and methodologies used in your studies to maintain transparency about the involvement (or lack thereof) of human subjects.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,N/A,This does not concerns us as we are not dealing with human subjects in our research.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","Justification for the Score:
The authors have labeled this question as N/A, indicating that the research does not involve human subjects. However, the response and justification provided by the authors indicating ""This does not concerns us as we are not dealing with human subjects in our research"" do not sufficiently engage with the nuances of their work or the NeurIPS guidelines. The guidelines clearly state that if the work involves crowdsourcing or any form of research with human subjects, details regarding ethical considerations, including IRB approvals, should be disclosed. Since the authors' response does not elaborate on their engagement or lack thereof with human subjects in the context of their datasets and meta-dataset creation, their answer may be seen as overlooking the broader ethical considerations of their work. This is especially pertinent given the growing awareness of the implications of machine learning research on privacy, consent, and ethical data usage, which can extend beyond direct human subject research, for example, in the creation and use of datasets.

Constructive Suggestions:
1. **Clarify Involvement of Human Subjects**: Even though the primary focus might be on datasets and algorithms, if any part of the dataset compilation, preprocessing, or annotation involved human effort (e.g., crowdsourcing for annotations), this should be clarified, and any associated ethical approvals or considerations should be discussed.

2. **Consider Broader Ethical Implications**: It's beneficial to reflect on the broader ethical considerations of the work, including data privacy, consent for data usage, and the impact of the synthesized datasets on these aspects. Even if direct human subject involvement is not present, the ethical sourcing and usage of data are crucial.

3. **Engage with Guidelines More Fully**: Reference the NeurIPS guidelines more explicitly by discussing how the work aligns with ethical considerations, even in cases where IRB approval is not directly applicable. This shows a proactive approach to ethical research practices.

4. **Documentation in Future Submissions**: For future submissions or revisions, consider adding a section on ethical considerations where you detail the steps taken to ensure the ethical use of data and algorithms, even when direct human subject research is not involved.

5. **Anonimity in Initial Submissions**: Remember to keep any institution-specific information anonymous if applicable in initial submissions, adhering to the double-blind review process guidelines. 

By addressing these suggestions, the authors can enhance the clarity and depth of their engagement with ethical considerations, aligning more closely with conference guidelines and broader ethical expectations in the research community.",0
