Question,Question_Title,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","The answer provided regarding the consistency of the main claims with the contributions and scope of the paper is accurate. The abstract and introduction of the paper explicitly outline the creation of the Meta-Album, a multi-domain meta-dataset designed for few-shot learning, transfer learning, and meta-learning. These sections detail the diversity of the dataset, its carefully verified licenses, and its intended utility for the research community, matching the contributions discussed throughout the paper.

However, for the sake of critical assessment, it might be beneficial to ensure that the paper not only states these contributions but also demonstrates them effectively throughout the text. The abstract and introduction set high expectations by mentioning the facilitation of research in cross-domain meta-learning and the practicability of the dataset across varying computational resources. Thus, the body of the paper should contain sufficient evidence to support these claims, such as experimental results, comparisons with existing datasets, and examples of the dataset’s application in research scenarios as mentioned.

Constructive suggestions for improvement include:
1. **Expand on Experiments:** Providing more detailed experimental results demonstrating the utility of Meta-Album in few-shot learning, transfer learning, and meta-learning applications. Highlight specific instances where Meta-Album shows superior performance or unique advantages over existing datasets.
2. **Detail Contributions More Clearly:** While the claims are accurately reflected, providing more explicit linkage in the body of the paper to how each claimed contribution is addressed and validated can strengthen the paper. This can include subsections or discussions specifically dedicated to each main claim made in the abstract and introduction.
3. **Discuss Limitations and Assumptions:** While the paper is clear about its contributions, a more detailed discussion on potential limitations and assumptions behind the design of Meta-Album would provide a balanced view and aid in setting realistic expectations for the readers.
4. **Illustrate with Use Cases:** Offering more real-world use cases or examples where Meta-Album has been or could be employed beneficially would provide tangible evidence of its claimed utility and effectiveness in fostering research in the stated domains.

These suggestions aim at reinforcing the paper's strengths and ensuring that the claims made in the abstract and introduction are not only accurately reflected but are also convincingly demonstrated throughout the document.",1
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,We have clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ",Justification for,0
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,N/A,This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The authors' response that their paper is not theoretical but rather practical in nature, focusing on the creation of a meta-dataset and experimental work using this dataset, is correct and aligns with the content and focus of the paper. Since the paper deals with dataset creation, experimentation, and application aspects rather than theoretical propositions, the requirements for providing the full set of assumptions and complete proofs do not apply in this context. 

**Explanation**: The justification provided by the authors is consistent with the guidelines for questions related to theoretical results. The guidelines specify that the NA answer is appropriate for papers that do not include theoretical results, which is the case for this paper. Therefore, the answer of N/A (Not Applicable) is accurate and justified given the nature of the research work presented in the paper.

**Constructive and Actionable Suggestions**:
1. **Clarity on Practical Contributions**: Although the theoretical aspect does not apply, the authors should ensure that the practical contributions of the paper, such as the design and advantages of the Meta-Album meta-dataset, are clearly articulated. This includes the diverse applications and implications of the dataset for the field of few-shot learning and machine learning benchmarks.

2. **Methodological Detailing**: For the experimental section, it is vital to provide detailed methodologies, including the experimental setup, the algorithms tested, and the rationale behind the selection of these algorithms. This detailed account helps in replicating the experiments and understanding the impact of the meta-dataset.

3. **Result Discussion**: Thoroughly discuss the results obtained from the experiments, highlighting not only the successes but also the limitations and areas where the meta-dataset could be further improved or extended. This honesty in reporting helps in setting a realistic expectation and guides future research directions.

4. **Future Work and Extensions**: Given the practical nature of the work, it would be beneficial to outline potential future extensions of the Meta-Album meta-dataset. This could include adding new domains, increasing the diversity of the dataset, or enhancing its utility for other machine learning tasks beyond few-shot learning.

5. **Engagement with the Community**: Encourage and provide clear guidelines for the community to contribute to the meta-dataset. This collaborative approach can significantly enrich the dataset and foster a community-driven effort towards building a more comprehensive resource for research.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The given answer by the authors that they have publicly provided the code on their GitHub repository and detailed how to access the datasets on their website does indeed satisfy the requirements for disclosing all the information needed to reproduce the main experimental results of the paper.

However, while agreeing with the authors' answer, it's critical to ensure that the provided resources truly enable full reproducibility. It is not entirely clear from the answer if the provided GitHub repository and website instructions cover all aspects mentioned in the guidelines, such as exact command and environment needed to run to reproduce the results, instructions on data access and preparation for raw, preprocessed, and intermediate data, and scripts to reproduce all experimental results including baselines. 

**Constructive and Actionable Suggestions:**

1. **Detailed Reproduction Guide:** Ensure that the GitHub repository includes a detailed README file or a separate reproduction guide that covers the exact steps, commands, and environment settings needed to reproduce the experimental results. This guide should explicitly state any dependencies, versions of software or libraries used, and any hardware requirements if applicable.

2. **Data Access and Preparation Instructions:** Provide clear instructions on how to access both the raw and preprocessed data. If there are any steps required to transform the raw data into the format used for experiments, include scripts or detailed instructions for these steps.

3. **Comprehensive Scripts:** Verify that scripts for reproducing each of the experiments, including those for baseline comparisons, are provided and well documented. It's important that these scripts can be easily located and understood by readers.

4. **Environment Details:** Include a requirements.txt file or an environment.yml file for Python environments, detailing the exact versions of libraries and tools used. For complete reproducibility, consider using Docker or another containerization tool to capture the entire experimental environment.

5. **Supplemental Material:** If applicable, append any supplemental material that further aids in reproducing the results to the paper. Detailed architecture diagrams, additional experimental setups, and results that couldn't be included in the main paper due to space constraints can be added here.

By adhering to these suggestions, the authors will significantly bolster the reproducibility of their work, aligning with the guidelines provided and ensuring that interested researchers can accurately replicate the study's findings.",1
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The answer provided by the authors is in agreement with the expectations outlined in the evaluation guidelines. The authors have explicitly mentioned that they have made the code publicly available on their GitHub repository and detailed how to access the datasets on their website. This indicates an effort towards transparency and reproducibility, which are crucial aspects of sharing scientific work. Additionally, the provided resources, such as code and datasets, are fundamental to reproducing the main experimental results.

However, for a thoroughly critical evaluation, there are some areas where the authors' response could be augmented or clarified to ensure the reproducibility of their experiments more comprehensively:

1. Detailed Instructions: While the authors mention the availability of the code and dataset details on their website and GitHub, it is not specified whether there are step-by-step instructions that include the exact commands, environment setup, and data preparation steps required to reproduce the results. For enhanced reproducibility, it would be beneficial if these instructions were made explicit.

2. Coverage of Experiments: The response does not indicate whether all experimental results presented in the paper can be reproduced using the provided code and datasets or if any experiments are omitted. It would be helpful for the authors to clarify this point.

3. Anonymity in Shared Resources: Given that this review is for a conference submission, it is not mentioned whether the shared code and datasets were anonymized to preserve the authors' anonymity (in line with submission rules). Clarification on this aspect would be beneficial for maintaining the integrity of the review process.

4. Supplemental Material: It is not mentioned if the authors have included any supplemental material appended to the paper that provides additional information aiding in the reproducibility of the results. Providing supplemental material can significantly aid in understanding the nuances of the experiments.

Constructive Suggestions:
- The authors should ensure that detailed, step-by-step instructions are readily available for setting up the environment, processing data, and executing the scripts needed to reproduce the experimental results.
- Clarify whether all experiments reported in the paper can be reproduced with the provided resources. If not, explain which experiments are excluded and why.
- Confirm whether the code and dataset links shared have been anonymized in accordance with conference submission guidelines to preserve anonymity during the review process.
- Consider appending supplemental material to the paper that further aids in the reproducibility of the experiments, detailing any specifics not covered in the GitHub repository or on the website.

By addressing these points, the authors can enhance the reproducibility of their work and adhere closely to the guidelines provided for code and data sharing.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","Explanation: The paper's response to the question about specifying all training and test details necessary for understanding the results is vague and lacking in precision. Although the authors reference Section 3.2 for experimental details and mention that additional details can be found in the Meta-Album GitHub repository, this does not guarantee that all necessary details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) are explicitly provided or readily accessible for replication or thorough understanding of the experiments.

Constructive and actionable suggestions for improvement:

1. **Explicitly Document Hyperparameters and Model Configurations:** In the paper's main body or in an easily accessible appendix, provide a detailed list of all hyperparameters used in the experiments, along with the rationale for choosing these specific values. This includes learning rates, batch sizes, optimizer types, and any regularization techniques applied.

2. **Data Splits and Protocol:** Clearly describe the data splits (training, validation, test) for each dataset utilized in the experiments. This should include the method for splitting (random, stratified, etc.), the proportion of data allocated to each split, and whether any specific protocol was followed to ensure reproducibility.

3. **Optimizer Details:** Detail the type of optimizer used in the experiments, including any specific configurations or modifications. If more than one optimizer was evaluated, explain the criteria for their selection and any comparative results that informed the final choice.

4. **Supplemental Material:** If full experimental details are too extensive for the main paper body, consider including them in an appendix or as supplementary material that is directly linked or very easy to find from the main paper. Ensure this material is organized and detailed enough for readers to replicate the work without needing to refer to external sources.

5. **GitHub Repository:** While referencing a GitHub repository for additional details is helpful, ensure that the repository is well-organized and contains clear documentation of the experimental setup. This should include README files that guide readers through the structure of the repository, how to run the experiments, and where to find specific details related to the training/test configuration.

6. **Rationale for Choices:** Wherever possible, provide a rationale for significant experimental design choices. This could include why specific datasets were chosen, the reasoning behind the allocation of data to training/test splits, or the decision-making process for selecting hyperparameters.

By addressing these suggestions, the authors can significantly improve the transparency and reproducibility of their experimental setup, allowing for a deeper understanding and validation of the results reported in the paper.",0
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","The answer provided by the authors is consistent with the paper content, as it clearly indicates where error bars, confidence intervals, or details regarding statistical significance tests associated with the experiments are provided. This includes references to specific figures and appendices which contain these statistical details. 

However, while the authors state that error bars have been reported, there is a lack of explicit detail in the justification regarding the nature of these error bars (e.g., whether they represent standard deviations, standard errors, or confidence intervals) and the method by which they were calculated. For a reader or reviewer looking to critically assess the statistical rigor behind the reported results, such detail is crucial for understanding the strength and reliability of the findings.

**Constructive Suggestions:**

1. **Clarify the Nature of Error Bars:** In your response and, more importantly, within the paper, explicitly state what the error bars represent (e.g., standard error, 95% confidence interval). This helps in appropriately interpreting the variability and reliability of your results.

2. **Methodology for Error Calculation:** Provide a brief explanation of how the error bars were calculated. If a specific statistical method was used (e.g., bootstrap, standard deviation of multiple runs), mentioning this explicitly would add credibility to the statistical analysis carried out.

3. **Discuss Factors of Variability:** Specifically mention the sources of variability captured by the error bars in the experiments. For instance, if error bars account for variability due to different initializations or data splits, make this explicit.

4. **Reference and Explain in Text:** To enhance the reader's understanding, do not only include these statistical details in figures or appendices but also briefly discuss them in the main text where relevant results are presented. This can be done by referring to the specific figures, tables, or appendices and providing a sentence or two about what the error bars tell us about the data.

5. **Assumptions and Limitations:** Where relevant, discuss any assumptions made in the statistical analysis (e.g., normality of error distribution) and how these might affect the interpretation of the data. Mentioning any limitations to the statistical approach used would also enhance the transparency and reliability of the findings.

By addressing these suggestions, the statistical rigor and clarity of the presented experiments would be significantly enhanced, providing the reader with a comprehensive understanding of the reliability and variability of the reported results.",1
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","Justification Review:
The authors' answer to the question regarding the provision of sufficient information on computer resources to reproduce experiments is ultimately inconsistent with the detailed content provided in the paper. While the authors reference specific sections and appendices (Section 3.2, Appendix E, and Appendix F) asserting comprehensive details about computing resources, an in-depth evaluation reveals that the cited sections only provide partial insights compared to what is recommended for reproducibility. Crucially, the sections mentioned collectively overlook key detailed aspects of the computational environment namely, exact specifications of the compute workers (type of CPUs/GPUs, the exact models, and configurations), memory specifics, detailed execution times for individual experimental runs, and nuances regarding internal cluster or cloud provider specifics if utilized.

Constructive and Actionable Suggestions:
1. **Detailed Compute Resource Description**: Clearly articulate the type and specification of compute workers (CPU/GPU models), memory, and storage capabilities utilized for the experiments. This allows readers to gauge the adequacy of their own computational facilities for reproduction efforts.

2. **Execution Time Breakdown**: Provide a detailed breakdown of execution times for each experiment, including preprocessing, training, validation, and testing phases. Time of execution is critical for evaluating the computational efficiency and practicality of the proposed methods.

3. **Compute Source Disclosure**: If experiments were conducted on an internal cluster or utilized cloud computing resources, specify the relevant details. Disclosing cloud providers or specific cluster configurations can help in accurately reproducing the results under similar conditions.

4. **Comprehensive Compute Overview**: Discuss the total compute required, incorporating not just the experiments presented in the main paper but also preliminary studies, parameter tuning, or failed experiments if applicable. This broader view helps in understanding the compute investment behind the presented results.

5. **Reproducibility Checklist**: Consider including a reproducibility checklist or a dedicated section summarizing all necessary computational details and any other prerequisites for replicating the experiments. This serves as a quick reference for researchers looking to validate or build upon your work.

By addressing these points, the paper can provide a more transparent and detailed account of the computer resources necessary for reproducing the experiments, significantly enhancing its practical utility and impact within the research community.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We completely comply with the NeurIPS Code of Ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","The given justification provided by the authors is too brief and lacks specific details to confidently ascertain full compliance with the NeurIPS Code of Ethics. Simplicity in confirming adherence to ethical guidelines is appreciated; however, this response does not demonstrate a clear understanding or thorough application of these important principles. Moreover, the stated compliance does not address specific aspects of the code, such as ensuring fairness in AI, respecting privacy, and handling data responsibly.

Constructive and Actionable Suggestions:
1. **Detailed Explanation**: The authors should provide a more comprehensive explanation of how their work complies with each section of the NeurIPS Code of Ethics. This includes discussions around data privacy, the potential impact of their work on society, and steps taken to ensure that the research does not perpetuate biases or cause harm.

2. **Ethical Considerations in Data Collection and Use**: Given that this paper introduces a new dataset (Meta-Album), it is crucial to delineate any ethical considerations involved in its collection, use, and distribution. This should involve a discussion on permissions for using existing datasets, privacy considerations for any human subjects involved, and how the data can be used responsibly by others.

3. **Impact Assessment**: The authors should assess the potential societal impact of their work. This includes both positive impacts, such as advancements in few-shot learning and AI accessibility, as well as potential negative impacts, like misuse of the data or algorithmic biases. Strategies to mitigate negative impacts should be identified.

4. **Fairness and Bias**: Given the emphasis on machine learning and dataset creation, a discussion on how the authors have considered and addressed issues of fairness and bias in their work is necessary. This includes measures taken to ensure diversity in the dataset and prevent algorithmic biases.

5. **Anonymity Preservation**: While the response complies with anonymity requirements, ensuring that all external references (like websites and repositories) are anonymized or appropriately shared for a double-blind review process (if applicable for this stage) is also an important ethical consideration.

Addressing these suggestions will not only strengthen the paper's adherence to the NeurIPS Code of Ethics but also enhance its contribution to the field by highlighting the ethical implications of the research.",0
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","Justification:
The authors' response to the question regarding the discussion of both potential positive and negative societal impacts is not consistent with the thorough expectations outlined in the guidelines. While the response acknowledges the meta-dataset's potential positive impact by fostering progress in few-shot learning and meta-learning, it falls short by unequivocally stating that there are ""no negative societal impacts."" This assertion does not align with the directive to critically examine both the direct and indirect paths through which the research could contribute to negative societal outcomes. The guidelines specifically instruct authors to consider not only direct applications but also the broader implications of their work, including misuse and unintentional harms. Furthermore, the response lacks engagement with potential mitigation strategies for any identified negative impacts, as suggested in the guidelines.

Constructive and Actionable Suggestions:

1. **Re-examine the Broader Impacts Section**: Encourage the authors to revisit their broader impacts section to assess both the intended and unintended consequences of their work more thoroughly. They should consider the direct and indirect effects of their meta-dataset on society, including how it might be misused or contribute to negative outcomes.

2. **Engage with Potential Negative Impacts**: The authors should discuss potential negative impacts, even if they believe these impacts to be minor or indirect. For instance, they could consider how advancements in few-shot learning enabled by their meta-dataset could lead to accelerated development of technologies with dual-use concerns (e.g., surveillance, deepfakes).

3. **Discuss Mitigation Strategies**: Encourage the authors to offer suggestions for mitigating any anticipated negative impacts. This could include recommendations for ethical guidelines, governance frameworks, or technical measures to prevent misuse and ensure the responsible use of their contributions.

4. **Include Expert Perspectives**: If applicable, the authors might consider including insights from interdisciplinary consultations (e.g., with ethicists, social scientists) to provide a more rounded perspective on the societal implications of their work.

5. **Enhance Transparency**: The authors could discuss the measures they have taken (or plan to take) to ensure transparency and accountability in the use of their meta-dataset, such as documentation standards, usage monitoring, and reporting mechanisms for misuse.

By addressing these suggestions, the authors can provide a more balanced and thorough consideration of the societal impacts of their work, aligning with the conference's expectations and contributing to the responsible advancement of the field.",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Yes,"Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","The authors have described safeguards for responsible release of data, which aligns with the guidelines provided for handling high-risk content and ethical considerations. Particularly, they mention having verified the licenses for all datasets included in the Meta-Album meta-dataset and outlined the recommended usage, which is crucial for ensuring that the datasets are used responsibly and ethically. This demonstrates a proactive approach to mitigating the risks of misuse and ensuring adherence to ethical standards in data release and utilization.

However, for the sake of providing constructive and actionable feedback, it would strengthen the paper to elaborate further on the specific licenses under which each dataset is released. Clarifying whether these licenses restrict certain types of usage or require attribution would provide potential users with clearer guidance on responsible use. Additionally, outlining any particular ethical considerations or challenges encountered during dataset compilation and how they were addressed would enhance the responsible release framework described. Lastly, encouraging a continuous dialogue within the research community about the evolving nature of data use ethics could further solidify the paper's commitment to responsible data handling.

Suggestions:
1. Provide a detailed description or a table listing the specific licenses for each dataset included in the Meta-Album meta-dataset.
2. Elaborate on any ethical considerations or challenges encountered during the dataset compilation process and the measures taken to address them.
3. Encourage an ongoing discussion within the research community on the ethical use of datasets, potentially through forums, workshops, or follow-up publications.",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","The authors have provided a detailed response regarding the credit and licensing information for the assets used in their paper. They affirmed their commitment to releasing their datasets with proper licenses and explicitly mentioned the original licenses for the datasets utilized in their work, which is located in Appendix B as pointed out in their justification. This approach aligns with the guidelines provided, ensuring that the original creators of the assets are properly credited and that the license and terms of use are respected and clearly articulated.

Justification Critique:
While the authors' response is in compliance with the guidelines, it would be helpful for the review if specific examples of licenses (e.g., CC-BY 4.0) used for their datasets or any external assets were briefly highlighted in the main text or in the justification provided. This could offer immediate clarity on compliance without needing to reference the appendices.

Constructive and Actionable Suggestions:
1. **Explicit License Mentioning**: In future submissions or revisions, consider mentioning specific licenses (e.g., CC-BY 4.0, MIT License) directly in the main content or abstract for immediate visibility. This quick reference can be beneficial.

2. **Version and URL Information**: Include information about the version of each asset used and, if possible, provide URLs to these assets. This aids in reproducibility and ensures that readers can access the exact versions of assets used for the study.

3. **Clarify Derivative Work Licensing**: If any datasets or assets were modified or combined to create new ones, state both the original license and the license of the derived asset directly in the paper. If the derived work's license differs from the original, briefly explain the reason for this change.

4. **Accessibility Links**: While Appendix B is mentioned for license details, providing a direct link (e.g., a DOI or a URL within the text or a footnote) that leads to the appendices or to an online repository with this information could enhance accessibility.

5. **Engagement with Original Creators**: If certain licensing information was ambiguous or required clarification, share any communication efforts with the original creators to ensure proper use and acknowledgment. This transparency demonstrates due diligence in asset utilization.

By addressing these suggestions, the authors can further strengthen their paper's adherence to ethical and transparent usage of existing assets, making their work more accessible and reproducible.",1
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","The answer provided by the authors does not fully align with the expectation set by the guidelines for documenting new assets introduced in the paper. While the authors mention that the datasets are properly documented using meta-data files and datasheets and that a website is designed for users to easily access all datasets and related information, this explanation lacks detail on several fronts outlined in the provided guidelines. 

Justification for the Score:
1. The response does not mention if the structured templates detailing the dataset/code/model information, including training, license, limitations, etc., are provided, which is a key requirement.
2. The answer does not address how consent was obtained from people whose data is used, if applicable. This is a significant aspect of dataset documentation and responsible research practice.
3. There is no mention of efforts to anonymize the assets for review purposes, a requirement at the submission stage.
4. Although a website is mentioned, direct references to the specific sections that contain essential documentation for the datasets would strengthen the response.

Constructive Suggestions:
1. Provide a more detailed explanation of how the datasets are documented, specifically mentioning whether structured templates were used and what information these templates include (e.g., training details, license information, any limitations of the datasets).
2. Clarify the consent process for the datasets used, particularly if they contain personal or sensitive information, to adhere to ethical guidelines.
3. Include information about anonymization efforts for the review process if applicable.
4. Provide direct links or references to where on the website or in the supplementary materials reviewers and future users can find detailed documentation for each dataset, including the structured templates, consent details, and anonymization information if applicable.",0
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,N/A,This does not apply on our research as we are not doing any crowdsourcing experiments,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","The authors accurately identified that this question does not apply to the content of their paper. Given the main focus of the paper is on introducing a new meta-dataset, Meta-Album, designed to facilitate research in cross-domain meta-learning, transfer learning, and few-shot image classification, there are no crowdsourcing experiments or direct research involving human subjects related to the construction or utilization of the dataset as described. Therefore, the presence of instructions for participants, screenshots, or details about compensation is not relevant.

However, to ensure thoroughness in the review and provide constructive feedback:
- It would be beneficial for the authors to explicitly state, possibly in an ethics or methodology section, that their work did not involve human experimentation or crowdsourcing where ethical concerns regarding participant compensation would be relevant. This clarification would preempt any potential confusion about the nature of their research processes.
- Since the paper involves the creation and curation of a meta-dataset, detailing the sources of the included datasets and any ethical considerations related to their use (permissions for re-distribution, any modifications, etc.) would strengthen the transparency and reproducibility of the research.

These suggestions aim to enhance the clarity and ethical rigor of the research without altering the judgment that the original question concerning human subjects and crowdsourcing is not applicable.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,N/A,This does not concerns us as we are not dealing with human subjects in our research.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The answer provided by the authors, ""N/A"", and their justification are consistent with the content of the paper. Based on the detailed description of the Meta-Album project, the primary focus of this work is on the development and evaluation of a meta-dataset for few-shot image classification and does not directly involve human subjects in the form of participants for experiments that would require IRB approval or pose potential risks. The nature of the study revolves around the use of pre-existing datasets, computational algorithms, and software development for enhancing machine learning models' capabilities in image classification across various domains. Given these factors, the concern for IRB approvals or participant risks does not apply to this particular case.

However, to ensure comprehensive and constructive feedback, here are some suggestions for future iterations of the work or for addressing similar questions in future publications:

1. **Clarification on Data Use**: While the primary focus is not on human subjects, clarifying the sourcing, licensing, and ethical use of images within the compiled datasets might add value. This is particularly relevant when datasets involve potential personal data or images from sources where consent and usage rights might be concerns.

2. **Consideration of Indirect Human Impact**: In future work, if the application or implications of the developed models could impact human subjects indirectly (e.g., deployment in settings involving privacy concerns, surveillance, or decision-making based on classification results), it may be worthwhile to discuss these aspects and any ethical considerations.

3. **Documentation Standards**: Continue to maintain high standards in documenting the origins, licenses, and permissions associated with datasets used in machine learning research. This practice supports transparency, reproducibility, and ethical use of data.

4. **Engagement with Ethical Review Processes**: If future research extends to directly involve human subjects (e.g., user studies on interface usability, crowdsourcing annotations, or studies on model implications), ensure to engage with IRB or equivalent ethical review processes and clearly document these engagements and outcomes in the research documentation.

The justification provided aligns with the expectations set out in the guidelines, and no information pertaining to the need for human subject research approvals is needed or missing for this type of computational study.",1
