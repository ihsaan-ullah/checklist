Question,Question_Title,Answer,Justification,Guidelines,Review,Correctness_Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","Justification: Upon reviewing the detailed presentation of the paper's content provided, it appears that the abstract and introduction indeed present the main claims, contributions, and scope of the paper effectively. The paper introduces ""Meta-Album,"" a novel, extensive meta-dataset designed to facilitate research in few-shot learning, meta-learning, and other related areas by providing a diverse collection of image classification tasks from various domains. This is clearly stated in the abstract and further elaborated in the introduction, accurately reflecting the paper's contributions and scope. 

However, the response does not provide a detailed analysis to support the claim that ""The abstract and introduction clearly state the main contributions and scope of the paper."" It lacks critical examination of how effectively the contributions and scope are communicated and whether any aspects or limitations are overlooked or not thoroughly addressed. In research, providing a succinct yet comprehensive overview of the contributions and scope in the abstract and introduction is critical for setting the reader's expectations. While the claim may be true, the justification does not sufficiently critique the alignment of the abstract and introduction with the paper's main contributions and scope.

Constructive and Actionable Suggestions:
1. Enhance the response by including an analysis of specific statements in the abstract and introduction that accurately encapsulate the paper's contributions, such as the introduction of ""Meta-Album,"" the inclusion of 40 datasets from diverse domains, and the facilitation of cross-domain few-shot learning research.
2. Examine if any aspects of the paper's contributions or scope are not adequately covered in the abstract and introduction. For instance, discuss whether the paper's challenges, such as data preprocessing and benchmarking across different domains, are clearly communicated.
3. Consider assessing the clarity of assumptions and limitations related to the ""Meta-Album"" meta-dataset's applicability and generalizability, as outlined in the guidelines. Are these aspects transparently discussed in the paper's introductory sections?
4. Offer insights on how the presentation of the paper's contributions and scope in the abstract and introduction could be further refined for clarity and completeness, ensuring readers gain a full understanding of what the paper achieves and its relevance to the field.",0
Does the paper discuss the limitations of the work performed by the authors?,Limitations,No,We haven't clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","Justification: The response provided by the authors in the NeurIPS Paper Checklist indicates that they did not discuss the limitations of their work, which contradicts best practices for scientific reporting. Understanding and discussing limitations are critical for contextualizing findings and guiding future research. The provided guidelines emphasize the importance of acknowledging limitations related to assumptions, scope of claims, performance factors, computational efficiency, and potential challenges in addressing privacy and fairness. However, the justifications offered by the authors do not reflect an engagement with these considerations.

Constructive and Actionable Suggestions:
1. **Add a Dedicated Limitations Section**: Even if briefly, dedicate a section to discussing the limitations of your work. This will not only improve the paper's integrity but also guide future research efforts to address these limitations.
2. **Reflect on Assumptions**: Consider the assumptions made in your approach, such as data distribution, noise levels, or model specifications. Discuss the potential impact if these assumptions do not hold in real-world applications.
3. **Scope of Claims**: Clarify the scope of your experiments. If your method was tested on a limited number of datasets or task types, acknowledge this and suggest areas for broadened testing in future work.
4. **Performance Factors**: Discuss factors that may affect the performance of your proposed methods, such as data quality, quantity, or specific domain challenges.
5. **Computational Efficiency**: Provide insights into the computational demands of your approaches, especially if they may not scale well to larger datasets or more complex scenarios.
6. **Address Privacy and Fairness**: If your work has implications for privacy and fairness, articulate these concerns and discuss potential mitigating strategies, even if they are not implemented in the current paper.
7. **Engage with Reviewers**: Understand that acknowledging limitations is seen as a strength, not a weakness. Engage transparently with potential criticisms, and use the limitations section to preemptively address concerns that reviewers might have.

By addressing these suggestions, the authors can significantly enhance the transparency and integrity of their work, contributing positively to the scientific community and facilitating more informed and constructive discussions around their findings.",0
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,Our work is not theoretical so this does not apply on our research,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The authors' response is consistent with the content of the paper. Based on the thorough review of the provided excerpt from the paper, it's evident that the focus is on the introduction and development of a comprehensive image classification meta-dataset, named Meta-Album. The primary purpose of this work is to advance research in few-shot learning, transfer learning, and meta-learning by providing a diverse and challenging dataset that covers a wide range of domains and conditions. Given this context, it is clear that the paper is empirical in nature, focusing on the creation and utilization of a dataset for enhancing machine learning models' abilities to generalize across different tasks and domains.

Justification for agreeing with the authors' answer:
- The main contribution of the paper is the creation and presentation of Meta-Album, an extensive meta-dataset designed to facilitate a variety of learning tasks, such as meta-learning and few-shot learning. These contributions are inherently practical and experimental, not theoretical.
- The paper does not claim to present new theoretical results, formulations, theorems, or proofs that would necessitate a detailed exploration of assumptions, mathematical formulations, or proofs.
- The excerpt includes detailed descriptions of the meta-dataset's structure, preparation, versions, and potential use-cases, all of which align with the notion that the paper's core contributions are empirical and focused on data creation and analysis rather than theoretical analysis.

Suggestions for the authors:
While the response is consistent with the paper's content and its empirical nature, it might be helpful for readers if the authors:
1. Explicitly state in the methodology or contributions section that the paper's focus is on providing a resource for empirical research rather than advancing theoretical understanding. This could clarify the paper's intentions for readers from the outset.
2. Briefly discuss potential theoretical implications or future research directions that could stem from the use of Meta-Album. Even though the paper is empirical, a short discussion on how it might inspire or facilitate theoretical advancements could enrich the paper's impact.
3. Consider providing guidelines or examples on how Meta-Album could be used to test or develop theoretical concepts in meta-learning and few-shot learning in future work. This would bridge the practical contributions of the dataset with theoretical research areas.",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,We have made our code and data publicly available on our github repository and on our website,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","Justification: The answer provided by the authors suggests that they have made their code and data publicly available, which is indeed a crucial step towards ensuring reproducibility. However, the provided justification lacks details on specific aspects that are essential for full reproducibility. For instance, it does not mention if the provided code includes scripts for preprocessing data, training models, and evaluating them to reproduce the experimental results presented in the paper. Furthermore, there is no mention of whether the documentation includes the environment setup, dependency requirements, and exact commands needed to run the experiments. These details are vital for other researchers to replicate the study's findings accurately and efficiently.

Constructive Suggestions:
1. **Detailed Documentation**: Include a comprehensive README file or documentation that lists the steps to set up the environment (e.g., required libraries and their versions), preprocess the data, train the models, and evaluate them. This should cover all experiments presented in the paper.

2. **Environment Requirements**: Provide a requirements.txt file or an environment.yml file that lists all the dependencies needed to run the code. Consider using virtual environments or Docker containers to ensure consistency across different setups.

3. **Preprocessing Scripts**: Ensure that scripts for data preprocessing are included and documented. If any data cannot be shared due to privacy concerns, provide synthetic examples or detailed descriptions of the data format expected by the models.

4. **Training and Evaluation Scripts**: Include separate scripts for training the models and evaluating their performance, along with instructions on how to use them. Specify the default parameters and any variation used in the experiments.

5. **Results Reproduction**: Provide a script or a set of scripts that, when executed, replicate the paper's main experimental results. Clearly indicate which results can be reproduced directly and which cannot, and provide reasons for the latter.

6. **Version Control**: Use version control (e.g., Git) to manage the code and track changes over time. An anonymized public repository can be used during the review process to maintain anonymity.

7. **Contact Information**: Include contact information (e.g., an anonymized email address) in the documentation for researchers to reach out in case of questions or issues with reproducing the results.

By addressing these suggestions, the authors can significantly improve the reproducibility of their work and adhere more closely to the guidelines provided for disclosing information needed to reproduce the main experimental results.",0
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,We have made our code and data publicly available on our github repository and on our website,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The answer and justification provided by the authors suggest that they have complied with the NeurIPS guidelines for open access to data and code necessary to reproduce the main experimental results. The reference to a GitHub repository and a website where both the code and data can be accessed is in line with the requirements for making experimental results verifiable and reproducible by other researchers. Furthermore, the mere mention of the availability of these resources implies there are likely instructions for how to use them to replicate the work presented in the paper, although specific details on these instructions were not provided in the answer.

However, for a more comprehensive review and to ensure full compliance with the guidelines, it would have been beneficial if the authors provided more detailed information regarding:
1. The exact content available in the GitHub repository and on the website.
2. Specific instructions for reproducing the results, including the exact commands to run, the environment setup, and any dependencies required.
3. Availability of both raw and preprocessed data for transparency and to allow for full replication of the study.
4. Clarity on which experiments' results can be reproduced with the provided code and data, and if there are any omissions, the reasons for these should be explained.

Actionable suggestions for improvement include:
- Enhance the documentation within the GitHub repository and on the website to include detailed step-by-step instructions for reproducing the experimental results.
- Ensure that all necessary data (raw, preprocessed, intermediate, and generated) are accessible and clearly organized.
- Provide a clear list of dependencies and environmental setup instructions to eliminate any potential hurdles in reproducing the results.
- If not all experimental results are reproducible with the provided resources, clearly state which ones are not included and provide justification.

Given the information provided, it is reasonable to assume compliance with the open access guidelines based on the current justification. However, the feedback above should help strengthen the submission and ensure the reproducibility of the results, enhancing the paper's overall credibility and utility to the community.",1
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,No,We have omitted many experimental details and hyper parameters,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","The justification provided by the authors acknowledges a significant limitation in their reporting of experimental details and hyperparameters. This omission can impede the reproducibility of the research and the ability of peers to fully understand and evaluate the reported results. The acknowledgment aligns with the expectation that a publication should provide detailed experimental settings, including data splits, hyperparameters, the rationale for their selection, the type of optimizer used, etc., to ensure clarity and reproducibility.

Constructive suggestions for the authors:

1. **Detail Experimental Setup**: Consider including a dedicated section or appendix that thoroughly describes the experimental setup. This should cover data preprocessing, train/test/validation splits, and the rationale behind these choices.

2. **Hyperparameters Explanation**: Provide a detailed account of the hyperparameters used in the experiments. If certain hyperparameters were chosen based on previous literature or empirical testing, mention this to help readers understand your decision-making process.

3. **Optimizer Information**: Specify the type of optimizer used and any particular settings or adjustments made to its default parameters. If multiple optimizers were tested, discuss the reasons for selecting one over the others.

4. **Supplementary Material**: If including detailed experimental setups and hyperparameter choices within the paper's main body is challenging due to space constraints, consider adding this information as supplementary material or an online appendix. Providing access to the exact code used in experiments, if possible, can also greatly enhance reproducibility.

5. **Rationalize Choices**: Where appropriate, provide justification for significant experimental decisions. This could include the choice of datasets, the reasoning behind certain hyperparameter values, or the selection of specific model architectures.

By addressing these points, the paper could significantly improve in terms of transparency, enabling others in the field to reproduce the work more accurately and understand the results' context fully.",1
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,No,We haven't reported error bars for our experiments,"• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","The authors' response indicates a misunderstanding or overlook of the guidelines provided for reporting statistical significance in their experiments. The guidelines explicitly state the importance of including error bars, confidence intervals, or statistical significance tests, especially for experiments supporting the main claims of the paper. The justification provided by the authors contradicts the expectation set in the review guidelines, which emphasizes the necessity of appropriately communicating the variability and statistical analysis of the experimental results.

Given the content of the paper, there is a considerable amount of experimental evaluation involving numerous datasets and varying few-shot learning settings. The lack of error bars or any mention of statistical significance tests considerably undermines the reliability and robustness of the reported results. This oversight hinders the reader's ability to assess the generalizability and significance of the findings presented, which are crucial for validating the contributions of the work.

Constructive and actionable suggestions for the authors include:
1. Incorporate error bars or confidence intervals in all result tables and figures to visually represent the variability and reliability of the experimental outcomes. If the paper’s current length constraints prevent adding this information directly to the figures, consider summarizing it in the text or supplementary material.
2. Clearly articulate the method used for calculating error bars or confidence intervals in the experimental section. Specify whether they represent standard deviations, standard errors of the mean, or another measure, and explain the choice of method.
3. Discuss the sources of variability in the experimental evaluation, such as differences across train/test splits, initialization seeds, or the stochastic nature of the algorithms evaluated. This will provide readers with a clearer understanding of what the error bars are capturing.
4. If applicable, perform and report statistical significance tests (e.g., t-tests, ANOVA) to compare the performance of different methods across datasets or learning settings, enhancing the robustness of the comparisons made in the paper.
5. Revisit the results section and revise the narrative to explicitly reference statistical analyses and the implications of these findings on the paper’s claims.

Addressing these suggestions will greatly enhance the scientific rigor of the paper and allow readers to more confidently interpret the significance of the reported results.",0
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,No,We haven't provided details about compute resources used,"• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","Justification:
Contrary to the authors' response, the paper does in fact provide detailed information on the compute resources needed to reproduce the experiments. This is evident in the various sections detailing the experimental setups, particularly in the appendix where running times per technique in different settings are meticulously reported. The document outlines not only the meta-training, meta-validation, and meta-testing phases but also distinguishes between different few-shot learning configurations, including within domain and cross-domain few-shot learning, and the innovative any-way any-shot learning. Such transparency ensures reproducibility and offers clear insights into the computational demands of the experiments.

Constructive Suggestions:
1. **Clarification and Consistency:** Ensure that all sections of the paper consistently acknowledge and describe the compute resources used. If certain sections inadvertently omitted these details, consider revising them for clarity and completeness.

2. **Broader Context:** Besides specifying the resources used, discuss any challenges or limitations encountered due to computational constraints. This insight could be valuable for researchers with varying access to computational resources.

3. **Benchmarking Against Other Datasets:** Since the paper introduces a new dataset and meta-dataset, Meta-Album, it would be beneficial to provide a comparison of computational requirements between Meta-Album and other existing datasets or benchmarks. This comparison can highlight the efficiency or the added computational demand of the proposed dataset.

4. **Failed Experiments:** Address the guideline about disclosing compute used in preliminary or failed experiments that did not make it into the paper. This can provide a more holistic view of the computational demands of the research project and might guide future research efforts in terms of resource allocation.

5. **Detailed Cloud or Hardware Specifications:** If cloud services or specific hardware configurations were employed, consider providing more granular details such as the type of GPU, cloud provider specifications, or any special configurations that were crucial for the experiments. This can aid in more accurately reproducing the experiments and assessing computational costs. 

By addressing these suggestions, the authors can enhance the transparency and reproducibility of their research, potentially broadening its impact within the community.",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,Our research is completely ethical and we conform to the NeurIPS code of ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","Explanation: The justification provided by the authors for the question regarding the NeurIPS Code of Ethics is overly brief and unsubstantiated. Simply stating that their research is ""completely ethical"" does not provide enough information on how the research specifically conforms to the detailed guidelines established in the NeurIPS Code of Ethics. Furthermore, maintaining rigor in ethical standards requires explicit clarification of practices adopted in research to address various ethical concerns, such as data privacy, consent (if applicable), potential biases in datasets, fairness, and impacts of the studied technology or methods.

Constructive and actionable suggestions:
1. **Detailed Ethical Consideration**: Authors should provide a more detailed justification that directly addresses specific points in the NeurIPS Code of Ethics. This includes discussing how they handled data privacy and consent (especially for datasets involving human subjects), efforts to mitigate biases within datasets, and evaluating the potential impact of their developed model or methodology on society.
  
2. **Dataset Ethics**: Since the paper introduces a new meta-dataset (Meta-Album), it would be beneficial for the authors to elaborate on the ethical considerations taken in curating these datasets. This should include information on dataset licensing, consent for use (particularly for sources with human-related data), and any steps taken to avoid inclusion of sensitive or personally identifiable information.

3. **Impact Assessment**: The authors should provide a section discussing the potential positive and negative impacts of their work on society. This includes how their findings may be used responsibly, any potential for misuse, and steps that can be taken to mitigate such risks.

4. **Fairness and Bias**: Given that Meta-Album encompasses a wide range of domains, including datasets from diverse sources, the authors should discuss measures or experiments carried out to investigate and address potential biases within the meta-dataset. This could involve analysis on representation fairness among classes, domains, and sensitivity towards socio-cultural biases in image classification tasks.

5. **Follow-up on Special Circumstances**: If there are legal, cultural, or country-specific regulations that affected how research was conducted, these should be explicitly mentioned with a rationale that clarifies their alignment with the NeurIPS Code of Ethics.

Addressing these suggestions will not only improve the alignment of the paper with the NeurIPS Code of Ethics but also enhance the paper's contribution by demonstrating a comprehensive and ethically aware approach to research.",0
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,No,We haven't discussed the limitations in detail,"• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","Justification: The response provided by the authors does not align with the expectation of discussing both potential positive and negative societal impacts of their work on Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification. The authors' brief mention of not discussing limitations in detail does not fulfill the requirement of assessing societal impact, considering both positive and negative aspects. 

Explanation: The creation of Meta-Album, an extensive and versatile meta-dataset, has clear potential societal impacts. On the positive side, such a dataset can accelerate the development of machine learning models that perform well across a wide range of domains with limited examples, which could have applications in medical imaging, environmental monitoring, and other areas with significant social benefits. On the negative side, the potential for misuse in creating deceptive imagery or contributing to surveillance without adequate privacy protections are concerns that merit discussion. The answer provided does not sufficiently engage with these considerations.

Constructive suggestions:
1. **Positive Impacts**: Expand on the potential benefits of the Meta-Album dataset. This could include its role in advancing few-shot learning research, enabling more efficient and effective machine learning models across diverse domains, and specific applications where it could have a profound societal benefit (e.g., healthcare, disaster response).
 
2. **Negative Impacts**: Consider and discuss potential misuse or unintended consequences of the technology. For example, could the models trained on this dataset be used to create deepfakes, contribute to surveillance, or exacerbate biases present in the data? 

3. **Mitigation Strategies**: Propose strategies to mitigate any identified negative impacts. These could include ethical guidelines for the use of the dataset, technical solutions to prevent misuse, or broader recommendations for the responsible development and deployment of machine learning technologies.

4. **Engagement with the broader community**: Highlight any efforts or plans to engage with stakeholders, including ethicists, policymakers, and impacted communities, to ensure the responsible use of Meta-Album.

By addressing these points, the authors can provide a more comprehensive consideration of the societal impacts of their work, fulfilling the expectations for this aspect of the NeurIPS Paper Checklist.",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,No,We haven't put any safeguards e.g. recommended use of datasets or license for datasets,"• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","The answer provided by the authors, stating that no safeguards have been put in place for the responsible release of data or models, is consistent with the content described in the paper. Their transparency about the absence of such safeguards aligns with the information presented. However, the lack of safeguards for responsible release, especially for a dataset as extensive and potentially impactful as Meta-Album, could pose ethical and misuse risks. Given the diverse nature of the data included in Meta-Album and its potential applications, it is crucial to consider responsible release practices.

**Constructive suggestions for the authors:**
1. **License Clarity:** Ensure all datasets within Meta-Album have clear licensing information that outlines permissible uses. This can prevent misuse by clearly stating do's and don'ts for end-users.
   
2. **Usage Guidelines:** Consider providing guidelines for the ethical use of Meta-Album. This can include best practices for researchers and developers on how to use the dataset responsibly, such as avoiding the creation of biased or harmful AI models.

3. **Data Scrubbing:** Perform a thorough review of the datasets for potentially sensitive or harmful content, and describe any steps taken to mitigate these risks, such as data anonymization or removal of problematic data.

4. **Community Engagement:** Engage with the AI ethics and broader research community to discuss and address potential risks associated with the dataset. This collaborative approach can lead to the development of shared norms and standards for dataset release and use.

5. **Access Control:** If feasible, consider implementing access controls or a request-based system for sensitive parts of the dataset. This can ensure that researchers with legitimate purposes can access the data while minimizing the risk of misuse.

6. **Continuous Monitoring:** Commit to regularly reviewing and updating the dataset and its associated safeguards to respond to emerging risks and ethical concerns.

Taking these suggestions into account would strengthen the responsible release and use of Meta-Album, aligning it with best practices for AI research and development.",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,No,We are releasing the datasets without any proper license,"• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","Justification: The provided answer and justification in the review do not align with the comprehensive details shared in the paper concerning the crediting of asset creators and the explicit mention of license and terms of use. The paper extensively references the sources of datasets and provides detailed license information in Appendix B. It outlines efforts made to ensure that all datasets included are freely available for academic research and mentions the original licenses provided by the dataset owners. Furthermore, the authors mention contacting original dataset owners for clarification on permissions when necessary, showing due diligence in respecting and properly acknowledging the terms of use.

Constructive Suggestions:
1. Clarification: It would be helpful for the authors to clarify or correct the review statement regarding the release of datasets without proper licensing. The paper does contain information about licensing and permissions, which could be highlighted or summarized in the main text to avoid any confusion.
2. Links and References: To further improve transparency and ease of access to licensing information, consider providing direct links to the datasets’ original sources and licenses within the main text or a dedicated section, in addition to the appendix.
3. Licensing Documentation: For the Meta-Album dataset that the authors have curated, they should ensure that a clear and accessible license is attached to the dataset itself, detailing how it can be used, modified, and shared. This license should be mentioned explicitly in the paper.
4. Updated Guidelines Acknowledgment: Acknowledge in the paper that you have followed the guidelines provided for properly crediting assets’ creators and respecting licenses and terms of use.
5. Asset Versioning: If not already included, explicitly state the version of the assets used in the research to ensure reproducibility and proper attribution.
6. Continuous Update: As the Meta-Album dataset intends to be extensible, establish a protocol for continuously updating the licensing and credit information as new datasets are added.

By addressing these suggestions, the authors can enhance the clarity and compliance of their paper regarding the use and crediting of third-party assets.",0
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,No,We haven't provided proper documentation fo our new datasets in the paper,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","Justification Review:
The authors' response in the paper review process indicates that they have not provided proper documentation for their new datasets, Meta-Album. This contradicts the extensive content provided in the paper that outlines the creation, purpose, and use of Meta-Album, including detailed descriptions of the datasets involved, their preprocessing steps, analyses, and how the community can contribute to the dataset. The paper includes sections on the motivations behind the design of Meta-Album, data search and preparation processes, initial release information, and how the datasets can be used for various machine learning tasks. Furthermore, the paper provides URLs to access the datasets and the code, making it easily accessible and usable for the research community. Therefore, the answer stating the lack of proper documentation directly contradicts the content found within the paper itself.

Suggestions for Improvement:

1. Clarify Documentation Availability: Ensure that the answer highlights the sections of the paper where the datasets are documented, including how they are preprocessed, the structure of the data, and how researchers can access and use them.

2. Highlight External Resources: Emphasize the availability of additional resources like the GitHub repository and the competition website, which likely contain more detailed documentation and usage examples for the datasets.

3. Explicitly Mention the Licensing Information: If the datasets come with specific licenses, make sure to mention them directly in the paper or provide links to where the licensing information can be found.

4. Consent and Ethical Considerations: Adhere to the guidelines by discussing consent obtained for the data used in the datasets, if applicable, and outline any ethical considerations taken during the dataset compilation process.

5. Anonymization During Review: If the review process requires anonymizing assets, outline the steps taken to ensure anonymization and how reviewers can access the datasets in an anonymized manner.

The discrepancy between the assessment and the content of the paper necessitates a correction to accurately reflect the extent of documentation provided for the Meta-Album datasets within the paper and through external resources.",0
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,Our work is not related to any crowdsourcing experiments.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","After reviewing the content of the paper titled ""Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,"" I agree with the authors' justification for answering NA to the question regarding crowdsourcing experiments or research with human subjects. The focus of the paper is on introducing a novel meta-dataset designed to assist in few-shot learning, meta-learning, and transfer learning tasks within the computer vision domain. The dataset compilation, preprocessing procedures, and the design of the Meta-Album are mainly computational and do not involve human subject research or crowdsourcing tasks that would require the full text of instructions or details about compensation. Therefore, the authors' response is consistent with the paper content.

Constructive and Actionable Suggestions:
1. While the answer is correct in this context, it would be beneficial for the authors to explicitly mention in the paper that their research did not involve human subjects or crowdsourced data collection to clear any ambiguities for readers.
2. For future work or extensions involving human evaluation, such as annotation quality checks or dataset validation, it would be essential to include detailed methodologies and ethical considerations regarding human participants.
3. The authors could consider a short section or statement discussing the ethical considerations of their dataset creation process, even if it does not involve human subjects directly. This could cover topics like data sourcing, privacy, and the usage of open-source or publicly available images.",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This does not apply to our research,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The authors' response of ""NA"" to the question regarding potential risks to study participants and the need for Institutional Review Board (IRB) approvals is indeed consistent with the content of the paper as reviewed. The paper focuses on the development and introduction of Meta-Album, a meta-dataset for few-shot image classification, which does not directly involve human subjects or personal data that would necessitate ethical concerns such as privacy, consent, and risks typically reviewed by an IRB. The research pertains to computer vision and machine learning methodologies applied to publicly available datasets, and as such, does not include experiments that involve interactions with human participants in a way that would require ethical oversight or IRB approvals.

Suggestions for improvement related to this aspect may not be directly applicable since the research scope does not involve human subjects. However, for completeness and to address potential future extensions of the work that might involve human data or interactions (e.g., user studies to evaluate the utility of the developed dataset in real-world applications), the following itemized recommendations are made:

1. Should future work involve human subjects, such as through user studies or the collection of new datasets involving individuals, the authors should seek IRB approvals or equivalent ethical reviews beforehand and disclose such information in the relevant publications.

2. The authors may consider including a brief section discussing the ethical considerations relevant to the collection and use of existing datasets in their work. This could include acknowledgment of the original sources of the datasets included in Meta-Album, any known ethical reviews those datasets underwent, and how privacy and consent were addressed.

3. While the current work may not involve direct ethical risks to participants, the authors could benefit from discussing any potential unintended consequences of deploying the technology developed from this research, especially concerning fairness, bias, and privacy considerations in automated image classification systems.

By addressing these recommendations, the authors would not only strengthen the current paper but also ensure preparedness for addressing ethical considerations in future research extensions.",1
