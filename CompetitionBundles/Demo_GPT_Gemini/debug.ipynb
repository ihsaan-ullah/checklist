{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import fitz\n",
    "import json\n",
    "import yaml\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from openai import OpenAI\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "root_dir_name = os.path.dirname('./')\n",
    "\n",
    "input_data_dir_name = \"input_data\"\n",
    "output_dir_name = \"sample_result_submission\"\n",
    "program_dir_name = \"ingestion_program\"\n",
    "submission_dir_name = \"sample_code_submission\"\n",
    "\n",
    "# Input data directory to read training and test data from\n",
    "input_dir = os.path.join(root_dir_name, input_data_dir_name)\n",
    "# Output data directory to write predictions to\n",
    "output_dir = os.path.join(root_dir_name, output_dir_name)\n",
    "# Program directory\n",
    "program_dir = os.path.join(root_dir_name, program_dir_name)\n",
    "# Directory to read submitted submissions from\n",
    "submission_dir = os.path.join(root_dir_name, submission_dir_name)\n",
    "\n",
    "sys.path.append(input_dir)\n",
    "sys.path.append(output_dir)\n",
    "sys.path.append(program_dir)\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "from constants import API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(paper):\n",
    "    # clean title\n",
    "    paper[\"title\"] = clean_title(paper[\"title\"])\n",
    "\n",
    "    # clean paper\n",
    "    paper[\"paper\"] = clean_paper(paper[\"paper\"])\n",
    "\n",
    "    # clean checklist\n",
    "    paper[\"checklist\"] = clean_checklist(paper[\"checklist\"])\n",
    "\n",
    "    return paper\n",
    "\n",
    "def clean_title(text):\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\-\\s*\\n', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_paper(text):\n",
    "    text = re.sub(r'\\n\\d+', ' ', text)\n",
    "    text = re.sub(r'\\-\\s*\\n', '', text)\n",
    "    text = re.sub(r'([a-zA-Z]\\.\\d+)\\n', r'\\1 ', text)\n",
    "    text = re.sub(r'([a-zA-Z])\\n', r'\\1 ', text)\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = text.replace(\"\\\\'\", \"'\")\n",
    "    text = text.replace(\"- \", \"\")\n",
    "    processed_text = \"\"\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line.split()) < 6:\n",
    "            processed_text += '\\n'\n",
    "            processed_text += line + '\\n'\n",
    "        else:\n",
    "            processed_text += line\n",
    "            processed_text += ' '\n",
    "    text = processed_text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_checklist(text):\n",
    "    text = re.sub(r'\\n\\d+', ' ', text)\n",
    "    text = re.sub(r'\\-\\s*\\n', '', text)\n",
    "    text = re.sub(r'  . ', '\\n', text)\n",
    "    text = re.sub(r'([a-zA-Z]\\.\\d+)\\n', r'\\1 ', text)\n",
    "    text = re.sub(r'([a-zA-Z])\\n', r'\\1 ', text)\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = text.replace(\"\\\\'\", \"'\")\n",
    "    text = text.replace(\"- \", \"\")\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_guidelines(text):\n",
    "    checklist_titles = [\n",
    "        \"Limitations\",\n",
    "        \"Theory Assumptions and Proofs\",\n",
    "        \"Experimental Result Reproducibility\",\n",
    "        \"Open access to data and code\",\n",
    "        \"Experimental Setting/Details\",\n",
    "        \"Experiment Statistical Significance\",\n",
    "        \"Experiments Compute Resources\",\n",
    "        \"Code Of Ethics\",\n",
    "        \"Broader Impacts\",\n",
    "        \"Safeguards\",\n",
    "        \"Licenses for existing assets\",\n",
    "        \"New Assets\",\n",
    "        \"Crowdsourcing and Research with Human Subjects\",\n",
    "        \"Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\",\n",
    "    ]\n",
    "    for checklist_title in checklist_titles:\n",
    "        text = text.replace(checklist_title, '')\n",
    "    return text\n",
    "\n",
    "def get_paper_chunks(paper_text):\n",
    "\n",
    "    try:\n",
    "        # Identify main paper and appendices\n",
    "        paper_end_index = paper_text.find(\"NeurIPS Paper Checklist\")\n",
    "\n",
    "        if paper_end_index == -1:\n",
    "            raise ValueError(\"[-] Error: NeurIPS Paper Checklist not found\")\n",
    "\n",
    "        paper = paper_text[:paper_end_index]\n",
    "\n",
    "        # Identify checklist section\n",
    "        checklist_start_index = paper_end_index\n",
    "        checklist = paper_text[checklist_start_index:]\n",
    "\n",
    "        # Identify title\n",
    "        title_end_index = paper.find(\"Anonymous Author\")\n",
    "        if title_end_index == -1:\n",
    "            title = paper.split(\"\\n\")[:2]\n",
    "            title = ''.join(title)\n",
    "        else:\n",
    "            title = paper[:title_end_index]\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"paper\": paper,\n",
    "            \"checklist\": checklist\n",
    "        }\n",
    "    except ValueError as ve:\n",
    "        raise ve\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"[-] Error occurred while extracting paper chunks in the {'paper' if not paper else 'checklist'} section: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_file):\n",
    "    pdf_file_path = os.path.join(submission_dir, pdf_file)\n",
    "    paper_text = \"\"\n",
    "    with fitz.open(pdf_file_path) as doc:\n",
    "        for page in doc:\n",
    "            paper_text += page.get_text()\n",
    "    return paper_text\n",
    "\n",
    "def load_yaml(yaml_file):\n",
    "    yaml_file_path = os.path.join(submission_dir, yaml_file)\n",
    "\n",
    "    with open(yaml_file_path, 'r') as file:\n",
    "        yaml_data = yaml.safe_load(file)\n",
    "\n",
    "    if not yaml_data:\n",
    "        raise ValueError(\"[-] The YAML file is empty or invalid.\")\n",
    "\n",
    "    for key, value in yaml_data.items():\n",
    "        # Check if key/question is an integer between 1 and 15\n",
    "        if not isinstance(key, int) or not 1 <= key <= 15:\n",
    "            raise ValueError(\"[-] Invalid key: Keys must be integers between 1 and 15.\")\n",
    "\n",
    "        # Check if value is 0, 0.5, or 1\n",
    "        if value not in [0, 0.5, 1]:\n",
    "            raise ValueError(\"[-] Invalid value: Values must be 0, 0.5, or 1.\")\n",
    "\n",
    "    return yaml_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_checklist(checklist):\n",
    "\n",
    "    checklist_questions = [\n",
    "        \"Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\",\n",
    "        \"Does the paper discuss the limitations of the work performed by the authors?\",\n",
    "        \"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\",\n",
    "        \"Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\",\n",
    "        \"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\",\n",
    "        \"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\",\n",
    "        \"Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\",\n",
    "        \"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\",\n",
    "        \"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\",\n",
    "        \"Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\",\n",
    "        \"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\",\n",
    "        \"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\",\n",
    "        \"Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\",\n",
    "        \"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\",\n",
    "        \"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\"\n",
    "    ]\n",
    "\n",
    "    checklist_question_titles = [\n",
    "        \"Claims\",\n",
    "        \"Limitations\",\n",
    "        \"Theoritical assumptions and proofs\",\n",
    "        \"Experiments reproducibility\",\n",
    "        \"Code and data accessibility\",\n",
    "        \"Experimental settings/details\",\n",
    "        \"Error bars\",\n",
    "        \"Compute resources\",\n",
    "        \"NeurIPS code of ethics\",\n",
    "        \"Impacts\",\n",
    "        \"Safeguards\",\n",
    "        \"Credits\",\n",
    "        \"Documentation\",\n",
    "        \"Human subjects\",\n",
    "        \"Risks\"\n",
    "    ]\n",
    "\n",
    "    checklist_df = pd.DataFrame(columns=['Question', 'Question_Title', 'Answer', 'Justification', 'Guidelines', 'Review', 'Score'])\n",
    "    try:\n",
    "        for question_index, question in enumerate(checklist_questions):\n",
    "            question_regex = re.escape(question)\n",
    "            pattern = re.compile(rf\"Question:\\s+{question_regex}(?:.*?Answer:\\s+\\[(.*?)\\].*?Justification:\\s+(.*?))(?:Guidelines:\\s+(.*?))(?=Question:|\\Z)\", re.DOTALL)\n",
    "\n",
    "            mtch = pattern.search(checklist)\n",
    "            if mtch:\n",
    "                answer = mtch.group(1).strip()\n",
    "                justification = mtch.group(2).strip() if mtch.group(2).strip() else None\n",
    "                guidelines = mtch.group(3).strip() if mtch.group(3).strip() else None\n",
    "                if guidelines:\n",
    "                    guidelines = clean_guidelines(guidelines)\n",
    "\n",
    "                if justification is not None and justification.isdigit():\n",
    "                    justification = None\n",
    "\n",
    "            else:\n",
    "                answer, justification, guidelines = \"Not Found\", \"Not Found\", \"Not Found\"\n",
    "\n",
    "            temp_df = pd.DataFrame([{'Question': question, 'Question_Title': checklist_question_titles[question_index], 'Answer': answer, 'Justification': justification, 'Guidelines': guidelines}])\n",
    "            checklist_df = pd.concat([checklist_df, temp_df], ignore_index=True)\n",
    "\n",
    "        return checklist_df\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"[-] Error in extracting answers and justifications: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper():\n",
    "\n",
    "    # -----\n",
    "    # Load PDF from submissions dir\n",
    "    # -----\n",
    "    print(\"[*] Loading PDF paper\")\n",
    "    # get all files from submissions dir\n",
    "    files = os.listdir(submission_dir)\n",
    "    pdf_file = None\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_file = file\n",
    "            break\n",
    "\n",
    "    if not pdf_file:\n",
    "        raise ValueError(\"[-] No PDF file found in the submission directory!\")\n",
    "    print(f\"[+] PDF file: {pdf_file}\")\n",
    "\n",
    "    ground_truth_file = f\"{pdf_file.split('.pdf')[0]}.yaml\"\n",
    "    if ground_truth_file not in files:\n",
    "        print(f\"[!] Ground Truth YAML file not found!. This may happen if your YAML file is not named as: {ground_truth_file}\")\n",
    "    else:\n",
    "        print(f\"[+] YAML file: {ground_truth_file}\")\n",
    "\n",
    "    print(\"[✔]\")\n",
    "\n",
    "    # -----\n",
    "    # Load text from PDF\n",
    "    # -----\n",
    "    print(\"[*] Loading and converting PDF to Text\")\n",
    "    paper_text = get_pdf_text(pdf_file)\n",
    "    print(\"[✔]\")\n",
    "\n",
    "    # -----\n",
    "    # Get paper chunks\n",
    "    # -----\n",
    "    print(\"[*] Breaking down paper into chunks and cleaning text\")\n",
    "    paper = clean(get_paper_chunks(paper_text))\n",
    "    print(\"[✔]\")\n",
    "\n",
    "    # -----\n",
    "    # Load Ground Truth Scores\n",
    "    # -----\n",
    "    if ground_truth_file:\n",
    "        print(\"[*] Loading Ground Truth YAML\")\n",
    "        paper[\"ground_truth\"] = load_yaml(ground_truth_file)\n",
    "        print(\"[✔]\")\n",
    "    else:\n",
    "        paper[\"ground_truth\"] = None\n",
    "\n",
    "    # -----\n",
    "    # Parse Checklist\n",
    "    # -----\n",
    "    print(\"[*] Parsing checklist from text\")\n",
    "    paper[\"checklist_df\"] = parse_checklist(paper[\"checklist\"])\n",
    "    print(\"[✔]\")\n",
    "    return paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading PDF paper\n",
      "[+] PDF file: genuine_MetaAlbum.pdf\n",
      "[+] YAML file: genuine_MetaAlbum.yaml\n",
      "[✔]\n",
      "[*] Loading and converting PDF to Text\n",
      "[✔]\n",
      "[*] Breaking down paper into chunks and cleaning text\n",
      "[✔]\n",
      "[*] Loading Ground Truth YAML\n",
      "[✔]\n",
      "[*] Parsing checklist from text\n",
      "[✔]\n"
     ]
    }
   ],
   "source": [
    "paper = process_paper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LLM_feedback(paper, checklist_df, ground_truth, question_n, paper_prompt):\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=API_KEY,\n",
    "    )\n",
    "\n",
    "    model = \"gpt-4-turbo-preview\"\n",
    "    max_tokens = 1000\n",
    "    temperature = 1\n",
    "    top_p = 1\n",
    "    n = 1\n",
    "\n",
    "    for index, row in checklist_df.iterrows():\n",
    "\n",
    "        question_number = index + 1\n",
    "        if question_number == question_n:\n",
    "            skip_question = ground_truth is not None and question_number not in ground_truth\n",
    "\n",
    "            if skip_question:\n",
    "                print(f\"[!] Skipping Question # {question_number}\")\n",
    "                continue\n",
    "            q = row[\"Question\"]\n",
    "            a = row[\"Answer\"]\n",
    "            j = row[\"Justification\"]\n",
    "            g = row[\"Guidelines\"]\n",
    "\n",
    "            print(f\"Question: {q}\")\n",
    "            print(f\"Answer: {a}\")\n",
    "            print(f\"Justification: {j}\")\n",
    "\n",
    "            paper_prompt = paper_prompt.replace(\"{paper}\", paper)\n",
    "            paper_prompt = paper_prompt.replace(\"{q}\", q)\n",
    "            paper_prompt = paper_prompt.replace(\"{a}\", a)\n",
    "            paper_prompt = paper_prompt.replace(\"{j}\", j)\n",
    "            paper_prompt = paper_prompt.replace(\"{g}\", g)\n",
    "\n",
    "            user_prompt = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": paper_prompt\n",
    "            }\n",
    "\n",
    "            messages = [user_prompt]\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                n=n\n",
    "            )\n",
    "\n",
    "            gpt_review = chat_completion.choices[0].message.content\n",
    "\n",
    "            score = 99\n",
    "            text = gpt_review\n",
    "            score_pattern1 = r\"Score:\\s*([0-9]+(?:\\.[0-9]+)?)\"\n",
    "            score_pattern2 = r\"\\*\\*Score\\*\\*:\\s*([0-9]+(?:\\.[0-9]+)?)\"\n",
    "\n",
    "            match1 = re.search(score_pattern1, gpt_review)\n",
    "            match2 = re.search(score_pattern2, gpt_review)\n",
    "\n",
    "            if match1:\n",
    "                score = match1.group(1)\n",
    "                text = re.sub(r\"Score:.*(\\n|$)\", \"\", text)\n",
    "            elif match2:\n",
    "                score = match2.group(1)\n",
    "                text = re.sub(r\"**Score**:.*(\\n|$)\", \"\", text)\n",
    "\n",
    "            checklist_df.loc[index, 'Review'] = text\n",
    "            checklist_df.loc[index, 'Score'] = score\n",
    "            print(f\"[+] Question # {question_number}\")\n",
    "\n",
    "            print(f\"\\nScore:\\n{score}\")\n",
    "            print(f\"\\n\\nReview:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_prompt = \"You are provided with a paper to be submitted to the NeurIPS conference. You are assisting the authors in preparing their answer to one “Quality Control Question”. Please examine carefully the “Proposed Author's Answer” and the “Proposed Author's Justification to the Answer” and identify any discrepancies with the actual ”Paper Content”, for this specific question, taking into account the “Guidelines Provided to Authors”. Afterwards, provide detailed, actionable feedback, based on the “Guidelines Provided to Authors”, aiming to improve the paper quality. Conclude your review with a score in a separate line: 1 if there are no significant concerns. 0.5 there is ground for improvement. 0 If critical issues must be addressed, that could lead to paper rejection. Make sure that score is shown in a new line in this format `Score: score_value` and there is no content after the score.\\n\\nQuality Control Question: {q}\\nProposed Author's Answer: {a}\\nProposed Author's Justification to the Answer: {j}\\nGuidelines Provided to Authors: {g}\\nPaper Content: {paper}\"\n",
    "\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    1,\n",
    "    paper_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_prompt = \"You are provided with a paper to be submitted to the NeurIPS conference. You are assisting the authors in preparing their answer to one “Quality Control Question”. Please examine carefully the “Proposed Author's Answer” and the “Proposed Author's Justification to the Answer” and identify any discrepancies with the actual ”Paper Content”, for this specific question, taking into account the “Guidelines Provided to Authors”. Afterwards, provide short, to the point, actionable feedback, based on the “Guidelines Provided to Authors” if there is room for improvement, aiming to improve the paper quality. Conclude your review with a score in a separate line: 1 if there are no significant concerns. 0.5 there is ground for improvement. 0 If critical issues must be addressed, that could lead to paper rejection. Make sure that score is shown in a new line in this format `Score: score_value` and there is no content after the score.\\n\\nQuality Control Question: {q}\\nProposed Author's Answer: {a}\\nProposed Author's Justification to the Answer: {j}\\nGuidelines Provided to Authors: {g}\\nPaper Content: {paper}\"\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    1,\n",
    "    paper_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_prompt = \"You are computer science conference reviewer. You are provided with a paper and you are assisting the authors in evaluating their answer to a “Quality Control Question”. Please examine carefully the “Proposed Author's Answer” and the “Proposed Author's Justification to the Answer” and identify any discrepancies with the actual ”Paper Content”, for this specific question, taking into account the “Guidelines Provided to Authors”. Afterwards, provide short, to the point, actionable feedback only if there is a discrepency. Conclude your review with a score in a separate line: 1 if there are no significant concerns. 0.5 there is ground for improvement. 0 If critical issues must be addressed, that could lead to paper rejection. Make sure that score is shown in a new line in this format `Score: score_value` and there is no content after the score.\\n\\nQuality Control Question: {q}\\nProposed Author's Answer: {a}\\nProposed Author's Justification to the Answer: {j}\\nGuidelines Provided to Authors: {g}\\nPaper Content: {paper}\"\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    1,\n",
    "    paper_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_prompt = \"You are tasked with reviewing a paper set to be submitted to the NeurIPS conference. Your role involves assisting the authors by evaluating their response to a specific 'Quality Control Question'. Carefully review the 'Proposed Author's Answer' and the 'Proposed Author's Justification to the Answer'. Check for any inconsistencies with the 'Paper Content' in light of the 'Guidelines Provided to Authors'. After your evaluation, provide concise, actionable feedback aimed at enhancing the paper's quality based on the guidelines. Conclude your review with a score on a new line as follows, based on your assessment: 1 for no significant concerns, 0.5 for areas needing improvement, and 0 for critical issues that could lead to rejection. Ensure the score is displayed in this format Score: score_value. There should be no content after the score.\\n\\nQuality Control Question: {q}\\nProposed Author's Answer: {a}\\nProposed Author's Justification to the Answer: {j}\\nGuidelines Provided to Authors: {g}\\nPaper Content: {paper}\"\n",
    "\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    1,\n",
    "    paper_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_prompt = \"\"\"\n",
    "You are tasked with reviewing a paper set to be submitted to the NeurIPS conference. \n",
    "Your role is to evaluate the authors response to a specific 'Quality Control Question'. \n",
    "Carefully review the 'Proposed Author's Answer' and the 'Proposed Author's Justification to the Answer'. \n",
    "Check for any inconsistencies with the 'Paper Content' in light of the 'Guidelines Provided to Authors'. \n",
    "After your evaluation, provide concise, actionable feedback aimed at enhancing the paper's quality based on the guidelines. \n",
    "Conclude your review with a score on a new line as follows, based on your assessment: \n",
    "1 for no significant concerns, \n",
    "0.5 for areas needing improvement, \n",
    "and 0 for critical issues that could lead to rejection. \n",
    "Ensure the score is displayed in this format Score: score_value. There should be no content after the score.\n",
    "NOTE: only give score 0.5 when the current state of the paper is not acceptable.\n",
    "\n",
    "Quality Control Question: {q}\n",
    "Proposed Author's Answer: {a}\n",
    "Proposed Author's Justification to the Answer: {j}\n",
    "Guidelines Provided to Authors: {g}\n",
    "Paper Content: {paper}\n",
    "\"\"\"\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    1,\n",
    "    paper_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n",
      "Answer: Yes\n",
      "Justification: The abstract and introduction clearly state the main contributions and scope of the paper\n",
      "[+] Question # 1\n",
      "\n",
      "Score:\n",
      "0.5\n",
      "\n",
      "\n",
      "Review:\n",
      "After analyzing the provided materials, including the \"Proposed Author's Answer\" and \"Proposed Author's Justification to the Answer\" in conjunction with the \"Paper Content\" and the \"Guidelines Provided to Authors,\" some discrepancies and areas for improvement have been identified. Below, these concerns are addressed with actionable feedback aimed at enhancing the submission quality and alignment with the conference standards.\n",
      "\n",
      "1. **Clarity and specificity in claims:**  \n",
      "   The \"Proposed Author's Justification to the Answer\" states that the abstract and introduction clearly state the main contributions and scope of the paper. While this might be true at a high level, the provided excerpts from the paper content illustrate a potentially oversimplified overview in the abstract and introduction sections. For better alignment and clarity, it is advisable to:\n",
      "   \n",
      "   - Explicitly summarize the unique contributions of the Meta-Album project compared to existing datasets or benchmarks in both the abstract and introduction.\n",
      "   - Detail the scope of the paper more distinctly by explaining how Meta-Album facilitates specific aspects of few-shot learning, meta-learning, or transfer learning, with emphasis on novel capabilities or properties not readily available in existing datasets.\n",
      "\n",
      "2. **Explicit mention of limitations and assumptions:**  \n",
      "   The guidelines highlight the importance of discussing important assumptions and limitations. This aspect seems to be underrepresented in the provided sections of the paper. Improvements can include:\n",
      "\n",
      "   - A section or paragraph in the introduction or conclusion, openly discussing potential limitations of the Meta-Album dataset, including but not limited to, biases inherent to the source datasets, potential overfitting issues due to the high variety of domains, and limitations in the scope of problems that Meta-Album can address.\n",
      "   - Acknowledging assumptions made during the preprocessing of datasets and the creation of the Meta-Album, and how they might influence the performance of models trained on this meta-dataset.\n",
      "\n",
      "3. **Harmonization between claims and factual content:**  \n",
      "   Ensure that all claims made in the abstract and introduction are verifiable within the main content of the paper. If experimental results, dataset features, or comparative analysis with other datasets are mentioned, corresponding sections should provide in-depth information supporting these claims. This includes:\n",
      "\n",
      "   - Enhanced tables or figures that directly compare Meta-Album with existing datasets on key characteristics and performance benchmarks.\n",
      "   - Detailed discussion sections that not only present the raw experimental results but also critically analyze Meta-Album’s performance in the context of the claims made early in the paper.\n",
      "\n",
      "4. **Structured and detailed methodology section:**  \n",
      "   Given the complex nature of compiling a meta-dataset like Meta-Album, the methodology section should be very detailed, laying out the step-by-step process. This can assure the reviewers and readers of the rigorous approach taken to dataset selection, preprocessing, and annotation. Improving transparency here can bolster the paper's credibility and utility as a resource for other researchers.\n",
      "\n",
      "Based on the identified discrepancies and areas for improvement, the paper would greatly benefit from addressing the specified concerns. The changes are essential not just for enhancing the paper's quality but for ensuring its contributions are well-articulated and its limitations understood, which is crucial for its acceptance and future citations.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_prompt = \"\"\"\n",
    "You are provided with a “Paper Content” to be submitted to the NeurIPS conference. You are assisting the authors in preparing their answer to one “Quality Control Question”. Please examine carefully the “Proposed Author's Answer” and the “Proposed Author's Justification to the Answer”. Then proceed step by step: If a section is cited in the \"Proposed Author's Justification for the Answer,\" summarize the relevant text referenced from the \"Paper Content\"; otherwise, please fetch by yourself relevant justification text. Identify and list any discrepancies between the author's response to the \"Quality Control Question\" (including both the \"Proposed Author's Answer\" and the \"Proposed Author's Justification for the Answer\") and the \"Paper Content.\"Provide itemized, actionable feedback, based on the “Guidelines Provided to Authors”, aiming to improve the paper quality. Concentrate on a few of the most significant improvements that can be made, and write in terse technical English.\n",
    "\n",
    "Conclude your review with a score in a separate line: \n",
    "1: The paper is acceptable without changes.\n",
    "0.5: Improvements are recommended to enhance the likelihood of acceptance, though no fatal flaws exist.\n",
    "0: Critical issues must be resolved, as they could almost certainly cause rejection if unaddressed.\n",
    "Make sure that score is shown in a new line in this format `Score: score_value` and there is no content after the score.\n",
    "Quality Control Question: {q}\n",
    "Proposed Author's Answer: {a}\n",
    "Proposed Author's Justification to the Answer: {j}\n",
    "Guidelines Provided to Authors: {g}\n",
    "Paper Content: {paper}\n",
    "\n",
    "Let's think step by step. The score should be formatted as \"Score: 0\", \"Score: 0.5\" or \"Score: 1\" in a separate final line.\n",
    "\n",
    "\"\"\"\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    1,\n",
    "    paper_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n",
      "Answer: N/A\n",
      "Justification: This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset\n",
      "[+] Question # 3\n",
      "\n",
      "Score:\n",
      "0.5\n",
      "\n",
      "\n",
      "Review:\n",
      "### Discrepancies Found:\n",
      "1. **Theoretical Results**: The proposed author's justification claims that the paper does not include theoretical results since it focuses on meta-dataset creation and experimental evaluation. However, the guidelines clearly stipulate that any paper, regardless of its primary focus, that includes theorems, formulas, and proofs, must adhere to specific formatting and presentation standards, including numbering, cross-referencing, and stating assumptions. This discrepancy indicates a misunderstanding or misinterpretation of what constitutes \"theoretical results\" within the scope of the guidelines.\n",
      "\n",
      "### Actionable Feedback:\n",
      "1. **Clarify Theoretical Contributions**: If the paper indeed contains any form of theoretical analysis, even if minor or supportive to the experimental work (e.g., formal definitions, propositions, lemmas, or complexity analysis), the authors should identify these parts clearly. They must ensure that these elements are numbered, assumptions are stated explicitly, and proofs (or proof sketches) are either included in the main document or the supplemental materials.\n",
      "\n",
      "2. **Revise Submission Type If Necessary**: If the paper strictly contains experimental research without any theoretical analysis, the authors should ensure that this is clearly communicated in the paper. It might be beneficial to explicitly state the nature of the paper's contributions in both the abstract and introduction to avoid confusion and align with the “N/A” response to theoretical contributions.\n",
      "\n",
      "3. **Provide a Formal Framework**: If the development of the meta-dataset involves novel selection criteria, preprocessing steps, or experimental protocols that could be formalized, doing so would strengthen the paper. Formalization helps in setting clear boundaries of the contribution and could potentially reveal theoretical aspects not initially evident.\n",
      "\n",
      "4. **Review Guidelines for Theoretical Contributions**: Even if the current submission does not include theoretical results, future work or extensions might. The authors should familiarize themselves with the guidelines concerning theoretical contributions to ensure compliance in subsequent research or versions of the paper.\n",
      "\n",
      "5. **Enrich Discussion on the Scope of Theoretical Results**: To bridge the gap between the proposed answer to the quality control question and the guidelines, it would be advisable to include a discussion section. This section could explain the rationale behind focusing on experimental evaluation and dataset creation, detailing why theoretical analysis was not necessary or how it could be incorporated in future work.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_prompt = \"\"\"\n",
    "You are provided with a “Paper Content” to be submitted to the NeurIPS conference. You are assisting the authors in preparing their answer to one “Quality Control Question”. Please examine carefully the “Proposed Author's Answer” and the “Proposed Author's Justification to the Answer”. Then proceed step by step: If a section is cited in the \"Proposed Author's Justification for the Answer,\" summarize the relevant text referenced from the \"Paper Content\"; otherwise, please fetch by yourself relevant justification text. Identify and list any discrepancies between the author's response to the \"Quality Control Question\" (including both the \"Proposed Author's Answer\" and the \"Proposed Author's Justification for the Answer\") and the \"Paper Content.\"Provide itemized, actionable feedback, based on the “Guidelines Provided to Authors”, aiming to improve the paper quality. Concentrate on a few of the most significant improvements that can be made, and write in terse technical English.\n",
    "\n",
    "Conclude your review with a score in a separate line: \n",
    "1: The paper is acceptable without changes.\n",
    "0.5: Improvements are recommended to enhance the likelihood of acceptance, though no fatal flaws exist.\n",
    "0: Critical issues must be resolved, as they could almost certainly cause rejection if unaddressed.\n",
    "Make sure that score is shown in a new line in this format `Score: score_value` and there is no content after the score.\n",
    "Quality Control Question: {q}\n",
    "Proposed Author's Answer: {a}\n",
    "Proposed Author's Justification to the Answer: {j}\n",
    "Guidelines Provided to Authors: {g}\n",
    "Paper Content: {paper}\n",
    "\n",
    "Let's think step by step. The score should be formatted as \"Score: 0\", \"Score: 0.5\" or \"Score: 1\" in a separate final line.\n",
    "\n",
    "\"\"\"\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    3,\n",
    "    paper_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n",
      "Answer: Yes\n",
      "Justification: The abstract and introduction clearly state the main contributions and scope of the paper\n",
      "[+] Question # 1\n",
      "\n",
      "Score:\n",
      "0.5\n",
      "\n",
      "\n",
      "Review:\n",
      "The author's answer to the checklist question regarding the accuracy of the claims made in the abstract and introduction with respect to the paper's contributions and scope is a simple affirmation. However, the justification provided does not specifically address how the claims in the abstract and introduction match the theoretical and experimental results within the paper or discuss the generalizability of these results to other settings. The guidelines emphasize the importance of explicitly stating the paper's contributions, including any important assumptions and limitations, and ensuring that the claims made are reflective of the results obtained and their potential applicability beyond the experimental setup.\n",
      "\n",
      "---\n",
      "**Feedback:**\n",
      "1. **Clarify Contributions**: Ensure the abstract and introduction explicitly outline the main contributions and clearly state how these contributions are demonstrated through the paper. This includes any methodological advancements, the creation of the Meta-Album dataset, or empirical findings from experiments.\n",
      "2. **Discuss Generalizability**: Expand on the claims related to the scope of the paper to discuss the generalizability of the findings. This means specifying under what conditions or domains the discussed methods or approaches are expected to perform well, according to the experimental results.\n",
      "3. **Highlight Limitations and Assumptions**: Incorporate a discussion on any limitations encountered during the study or inherent to the approach proposed. Similarly, explicitly state any assumptions that underpin the research findings. Doing so will not only align with the completion guidelines but also provide readers with a more comprehensive understanding of the context and applicability of the paper's contributions.\n",
      "4. **Match Claims with Evidence**: Ensure that each claim made in the abstract and introduction regarding the paper’s contributions, the novelty of the dataset, or implications of the findings is directly supported by evidence presented within the main body of the paper. This could involve linking claims to specific sections or results to improve clarity and credibility.\n",
      "5. **Detail the Significance of Contributions**: Elaborate on why the contributions are significant within the field of machine learning, especially in relation to few-shot learning, meta-learning, and cross-domain generalization. This could involve a comparison with existing approaches, datasets, or benchmarks to position the paper's contributions more clearly.\n",
      "\n",
      "**Conclusion**: The paper’s alignment with the checklist criteria regarding the accuracy and reflection of the main contributions within the abstract and introduction can be improved by elaborating on the points mentioned above. Addressing these areas will enhance the paper's adherence to the provided guidelines and increase its likelihood of acceptance by offering a more comprehensive and transparent overview of the research and its implications.\n",
      "\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "paper_prompt = \"\"\"\n",
    "You are provided with a “Paper” to be submitted to the NeurIPS conference. You are assisting the authors in preparing their “Answer” to one checklist “Question”. Please examine carefully the proposed author's “Answer” and the proposed author's “Justification” provided, and identify any discrepancies with the actual ”Paper” content, for this specific “Question”, taking into account the “Guidelines” provided to authors. Afterwards, provide itemized, actionable feedback, based on the “Guidelines”, aiming to improve the paper quality. Concentrate on a few of the most significant improvements that can be made, and write in terse technical English.\n",
    "Conclude your review with a score in a separate line: \n",
    "1: The paper is acceptable without changes.\n",
    "0.5: Improvements are recommended to enhance the likelihood of acceptance, though no fatal flaws exist.\n",
    "0: Critical issues must be resolved, as they could almost certainly cause rejection if unaddressed.\n",
    "Make sure that score is shown in a new line in this format “Score: score_value” and there is no content after the score.\n",
    "Question: {q}\n",
    "Answer: {a}\n",
    "Justification: {j}\n",
    "Guidelines: {g}\n",
    "Paper: {paper}\n",
    "\n",
    "\"\"\"\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    1,\n",
    "    paper_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n",
      "Answer: N/A\n",
      "Justification: This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset\n",
      "[+] Question # 3\n",
      "\n",
      "Score:\n",
      "0.5\n",
      "\n",
      "\n",
      "Review:\n",
      "**Feedback on the Proposed Author's Answer and Justification:**\n",
      "\n",
      "The provided \"Paper\" content and the authors' response seem misaligned with the checklist question \"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\" The response indicates that this is not applicable because their work is on meta-dataset creation and experimental execution, which is primarily empirical in nature and does not directly engage with theoretical proofs or assumptions in the manner that a more mathematically inclined paper might.\n",
      "\n",
      "The essential improvements necessary to enhance paper quality are:\n",
      "\n",
      "1. **Clarify the Methodological Scope:** Explicitly state the empirical nature of the research early in the paper, emphasizing that it focuses on dataset creation, preprocessing methodologies, and experimental validation. This would directly address any theoretical expectations by making the practical and empirical focus of the work clear.\n",
      "\n",
      "2. **Extend the Discussion on Dataset Preparation and Experimental Validation:** Given the empirical focus, it's critical to provide detailed descriptions of dataset curation processes, data preprocessing decisions, experimental designs, benchmark comparisons, and how these support the broader research community. Insights into challenges faced during dataset curation and how they were overcome would be particularly valuable.\n",
      "\n",
      "3. **Rationale for N/A on Theoretical Proofs:** While the answer to the checklist question is N/A, a brief rationale within the paper can help clarify why theoretical proofs are not pertinent to the research's objectives. This includes reinforcing the paper's focus on providing a diverse and scalable resource for few-shot learning and the experimental methodologies to validate it.\n",
      "\n",
      "4. **Address the Potential for Future Theoretical Work:** Explore opportunities for theoretical exploration that the Meta-Album could enable, such as theoretical frameworks for understanding cross-domain few-shot learning outcomes or models that could be developed or tested using the dataset. Positioning the work as a foundation for both empirical and theoretical future research could broaden its impact.\n",
      "\n",
      "5. **Enhance Transparency and Reproducibility:** Ensure that all resources related to dataset curation, preprocessing, and experiments (e.g., code, tools for dataset checking and factsheet generation) are accessible, documented, and easy to use. Transparency in these aspects is crucial for the community to reproduce studies, verify results, and build upon the work.\n",
      "\n",
      "6. **Community Engagement and Contribution Process:** Detail the process for community contributions to the Meta-Album, including quality control, review processes, and guidelines for contributions. This fosters a collaborative environment for the dataset's growth and encourages diverse applications and methodological innovations.\n",
      "\n",
      "7. **Limitations and Future Directions:** Explicitly discuss limitations of the current version of Meta-Album and envisioned future expansions or iterations. Highlight areas where theoretical analyses of the datasets included or developed using Meta-Album could be particularly insightful.\n",
      "\n",
      "Overall, while the paper's focus on empirical methodology necessitates the N/A response to the theoretical proofs question, clarifying and expanding on the empirical contributions can significantly enhance the paper’s value to its audience.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_prompt = \"\"\"\n",
    "You are provided with a “Paper” to be submitted to the NeurIPS conference. You are assisting the authors in preparing their “Answer” to one checklist “Question”. Please examine carefully the proposed author's “Answer” and the proposed author's “Justification” provided, and identify any discrepancies with the actual ”Paper” content, for this specific “Question”, taking into account the “Guidelines” provided to authors. Afterwards, provide itemized, actionable feedback, based on the “Guidelines”, aiming to improve the paper quality. Concentrate on a few of the most significant improvements that can be made, and write in terse technical English.\n",
    "Conclude your review with a score in a separate line: \n",
    "1: The paper is acceptable without changes.\n",
    "0.5: Improvements are recommended to enhance the likelihood of acceptance, though no fatal flaws exist.\n",
    "0: Critical issues must be resolved, as they could almost certainly cause rejection if unaddressed.\n",
    "Make sure that score is shown in a new line in this format “Score: score_value” and there is no content after the score.\n",
    "Question: {q}\n",
    "Answer: {a}\n",
    "Justification: {j}\n",
    "Guidelines: {g}\n",
    "Paper: {paper}\n",
    "\n",
    "\"\"\"\n",
    "get_LLM_feedback(\n",
    "    paper[\"paper\"], \n",
    "    paper[\"checklist_df\"], \n",
    "    paper[\"ground_truth\"],\n",
    "    3,\n",
    "    paper_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
