Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the paper's main contributions and scope.,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ","The provided ""Answer"" and ""Justification"" by the authors claim that the abstract and introduction clearly state the paper's main contributions and scope. However, this review has identified discrepancies that must be addressed to ensure the paper's abstract and introduction accurately reflect its contributions and scope, adhering to the ""Guidelines"" for NeurIPS submissions.

### Actionable Feedback

1. **Clarify Contributions:**
   - The abstract mentions the adaptation of ""collaborative filtering"" and an ""active learning version of the recommender algorithm CofiRank algorithm"" as novel contributions. However, the introduction lacks specificity regarding how this approach improves or differs significantly from existing solutions such as AUTOSKLEARN and ALORS, except for brief mentions. Explicitly detailing these differences and improvements in the introduction can reinforce the paper's claims about its originality and contributions.
   
2. **Detail on Scope and Applicability:**
   - While the introduction broadly mentions the application areas (e.g., computer vision to chat bots), it doesn't specify how the proposed method applies to these areas or its limitations. Clarifying the scope of the algorithm recommendation, including types of machine learning tasks (classification, regression, etc.) where it is most effective, and any limitations in applicability, would align better with the guideline expectations for stating scope, assumptions, and limitations.

3. **Match Contribution to Results:**
   - The claims made about the algorithm’s performance on new datasets and its computational cost efficiency are substantial. The paper should include a brief overview of the results supporting these claims in the introduction section, pointing readers towards the sections that provide detailed experimental validation. This adjustment ensures the claims are immediately matched with evidence, as per the guidelines.

4. **Edge Cases and Assumptions:**
   - The introduction briefly mentions the novel ingredient of AUTOSKLEARN without detailed comparison or justification of how the proposed active meta-learning approach might handle similar or different edge cases and assumptions. Expand on this in the context of the proposed approach's assumptions and its implications for generalizability and performance.

5. **Explicit Mention of Limitations:**
   - The guidelines highlight the importance of stating important assumptions and limitations. The paper should explicitly outline any limitations or cases where the proposed method might not perform as expected in both the abstract and introduction to comply fully with the guidelines.

6. **Revision for Clarity and Detail:**
   - Revisit the abstract and introduction for clarity, especially concerning the problem being addressed, the proposed solution, and why it is a significant advancement. Use concise, precise language that avoids general statements lacking in detail.

### Conclusion

Improving the alignment between the paper's content and the claims made in the abstract and introduction regarding contributions and scope is crucial for meeting NeurIPS expectations. The proposed improvements focus on clarity, specificity, and the evidentiary basis for claims, which are essential for a comprehensive and accurately represented submission.

**",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,"The limitations are discussed in the section “Discussion and conclusion"".","• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ","The proposed author's ""Answer"" and ""Justification"" indicate that the paper discusses the limitations of the work in the section “Discussion and conclusion”. However, based on the provided “Guidelines”, a comprehensive review reveals discrepancies and areas for improvement in addressing limitations rigorously and transparently. The following actionable feedback aims at enhancing the paper's quality concerning its reflection on limitations:

1. **Assumptions and Robustness:** The paper does not fully articulate the strong assumptions made, such as the generalizability of performance across datasets not included in the study and the robustness of results to violations of these assumptions. It is recommended that the paper explicitly discuss assumptions related to the collaborative filtering approach, data sparsity, and algorithm performance predictability.
   - **Actionable Feedback:** Add a subsection detailing assumptions inherent to the active meta-learning approach and the collaborative filtering algorithm used. Discuss how the results might vary if these assumptions do not hold in practice.

2. **Scope of Claims:** Although the paper presents results on a variety of datasets, there is a lack of reflection on the scope of the claims, particularly concerning the diversity of datasets (e.g., domain-specific limitations) and the number of runs conducted for each dataset.
   - **Actionable Feedback:** Expand the ""Discussion and conclusion"" section to include a critique of the empirical validation, specifically addressing the datasets' representativeness and the statistical significance of the findings.

3. **Factors Influencing Performance:** The paper briefly mentions factors influencing algorithm performance but does not delve into specific instances where the system may underperform, such as datasets with particular characteristics (e.g., high dimensionality, class imbalance).
   - **Actionable Feedback:** Provide a detailed analysis of dataset characteristics that could influence the performance of the recommended algorithms, supported by references to existing literature or preliminary tests.

4. **Computational Efficiency and Scalability:** The discussion on computational cost is surface-level. There is a need for a deeper examination of the computational efficiency of the proposed algorithm, especially when scaling to larger datasets or more complex model selections.
   - **Actionable Feedback:** Integrate a subsection on computational considerations, comparing the proposed method with baseline methods in terms of time complexity and resources required.

5. **Privacy and Fairness Considerations:** The paper does not address potential limitations regarding privacy and fairness implications of algorithm recommendations.
   - **Actionable Feedback:** Acknowledge and discuss any privacy and fairness issues that may arise from the application of the proposed meta-learning approach, especially when dealing with sensitive data.

6. **Transparency and Honesty:** While the paper mentions some limitations, a more forthright discussion could further improve its integrity and the likelihood of acceptance.
   - **Actionable Feedback:** Clearly articulate the paper's limitations in the context of impacts on research and practice, highlighting areas for future investigation.

The mention of limitations is an essential aspect of scientific rigor and honesty. Providing a detailed discussion of these aspects not only strengthens the paper but also helps set a research agenda for addressing these limitations in future work.

**",0.5
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,This paper does not include theoretical results.,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. ","The proposed author's ""Answer"" states that their paper does not include theoretical results, which they have marked as ""NA."" However, upon review of the ""Paper"" content, particularly the sections regarding the problem setting, algorithms, and the empirical validation of the approach, it's clear that there are theoretical considerations regarding the methodology, algorithms, and their performance analysis. This directly conflicts with the checklist question about providing full sets of assumptions and complete (and correct) proofs for theoretical results, as the paper implicitly contains theoretical constructs within its proposed algorithms and their justification.

**Actionable feedback:**

1. **Clarify Theoretical Contributions**: The authors must revisit their paper to accurately identify and articulate any theoretical contributions or the theoretical foundation of their proposed algorithm. This includes any assumptions made for the algorithm's performance, the conditions under which the algorithms were analyzed, and any theoretical justification for their approach.

2. **Explicitly State Assumptions**: Assumptions that are implicit in the problem setting, algorithms, or during empirical validation, should be explicitly stated. This could involve detailing the assumptions about the data characteristics, algorithm performance metrics, or any model constraints.

3. **Provide or Reference Proofs**: If there are indeed theoretical claims or results derived from the proposed active meta-learning algorithm, the authors should include these proofs in the main paper or at least sketch the proof with a reference to a more detailed exposition in the supplemental material.

4. **Amend the Answer to the Checklist Question**: Adjust the response to the checklist question to reflect the presence of theoretical considerations within the paper. If the authors maintain that their work does not introduce new theoretical results per se but relies on established theories, this should be clarified.

5. **Reference Theorems and Lemmas**: For any theoretical results relied upon in their analysis or argumentation, the authors should ensure these are properly referenced, even if the results are well-known. This facilitates reader understanding and conveys the depth of the theoretical background.

6. **Supplemental Material**: If providing detailed proofs or a full set of assumptions in the main paper is infeasible due to space constraints, these should be included in supplemental material. A direct reference to where these can be found should be added to the paper.

**
",0.5
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public.,"• The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. Open Access to Data and Code","Feedback:

1. **Clarification on Pseudo-code and Algorithm Specifics**: The authors claim that ""All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public."" However, while the paper includes pseudo-code for the ACTIVMETAL procedure and its variants (Algorithms 1-5), it does not explicitly mention the version or specific settings used for the CofiRank algorithm, nor does it provide details on the computational environment necessary to replicate the experiments (e.g., memory requirements, CPU/GPU specifications). To enhance reproducibility, the authors should:
   - Provide a clear description of the CofiRank version used, including any modifications or specific parameter settings.
   - Detail the computational environment needed for replication, including hardware specifications and any necessary software dependencies.

2. **Data Accessibility and Preprocessing Steps**: Although the paper mentions that datasets are from various sources and includes preprocessing steps (such as taking the square root of the StatLog dataset's performances), it lacks detailed instructions on how to access these datasets and execute the preprocessing. For complete reproducibility:
   - Include direct links to where the datasets can be accessed, or provide the datasets as supplementary material.
   - Describe the preprocessing steps in more detail, including any scripts or commands used.

3. **Code Availability and Execution Instructions**: The claim of code being publicly accessible is supported by a reference to a Github repository. However, the paper does not include specific instructions on how to use the provided code to replicate the results. To improve this aspect:
   - Ensure the Github repository contains a comprehensive README file with step-by-step instructions on running the experiments, including any necessary steps to install CofiRank and other dependencies.
   - If applicable, include information on how to use the Docker image mentioned in the Supplemental material section, ensuring that it contains all necessary environments and dependencies pre-installed.

4. **More Detailed Description of Experimental Setup**: The experimental results section could benefit from more detailed information on the methodology. Specifically:
   - Explain the criteria used for algorithm and dataset selection within the experiments.
   - Provide statistical methods used for analysis, including how error bars and significance levels were calculated.

5. **Supplemental Material References**: The paper mentions supplemental material for exploratory data analyses and additional graphs referred to in the text. It is crucial to:
   - Ensure all supplemental material referenced in the paper is included in the Github repository.
   - Provide a clear link or reference within the paper to where this supplemental material can be found, ensuring readers can easily access it for full understanding and replication of the study.

",0.5
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,"The github repo for reproducibility is given, in which you can find the code and dataset.","• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ","The provided answer and justification by the authors claim that both the code and dataset required for reproducing the experimental results are accessible via a GitHub repository. However, for a comprehensive review and feedback, critical information regarding the exactness of instructions, environment setup details, and data access and preparation clarification is missing in the justification.

1. **Detailed Instructions**: The authors mention a GitHub repository for reproducibility but do not specify if detailed instructions, including the exact command lines and environment setup (software versions, dependencies), are provided. This detail is crucial for ensuring that other researchers can faithfully reproduce the reported results.

2. **Data Access and Preparation**: While the authors state the dataset is available, there's no mention of how to access raw data, preprocess it, and obtain intermediate or generated data as per NeurIPS guidelines. Ensuring easy access and understandability of the data and its preparation process is essential for true reproducibility.

3. **Scripts for Reproduction**: The justification lacks confirmation on whether scripts provided cover all experimental results for the proposed method and baselines. Specifically, it should be clear whether all or only a subset of the experiments can be reproduced. If not all experiments are covered, an explanation should be provided.

4. **Anonymization**: Given the submission to a double-blind review process, it's necessary for the authors to confirm the anonymization of the GitHub repository to ensure no direct or indirect hints about the authors' identities. This aspect is not addressed in the provided justification.

5. **Supplemental Information**: The authors should ensure that supplemental material, if any, appended to the paper or provided online, is comprehensive and includes all necessary details to understand and reproduce the experiments completely.

### Actionable Feedback:
- **Provide Detailed Instructions**: Ensure the GitHub repository includes a detailed README file with clear instructions on setting up the environment, including software versions and dependencies, and step-by-step commands to run the experiments.
- **Clarify Data Access and Preparation**: Amend the repository or supplemental material to include detailed instructions on data access, any preprocessing steps taken, and how to obtain or generate any intermediate data used in the experiments.
- **Ensure Scripts Cover All Experiments**: Verify and state explicitly that the provided script(s) can reproduce all experimental results presented in the paper, including those for the proposed method and any baselines. If any experiments are not covered, explain why.
- **Anonymize the Repository**: If not already done, ensure that the GitHub repository is appropriately anonymized to avoid revealing the authors' identities indirectly, complying with the conference's double-blinded review process.
- **Enhance Supplemental Material**: Augment the supplemental material to include any additional details, data, and code not present in the main GitHub repository, ensuring it supports a thorough understanding and reproduction of the research.

### ",0.5
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,"These details are given in the section of “Results"".","• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. ","The proposed author's answer indicates that the necessary training and test details are present in the Results section of the paper. Upon examination of the paper content provided, there are some discrepancies and points that could be improved upon in relation to the guidelines for specifying all training and test details necessary to understand the results:

1. **Clarification of Dataset Splits**: While the paper discusses the datasets used for benchmarking, it does not explicitly outline how these datasets were split into training and testing, or if cross-validation was applied. For comprehensive understanding, clear specification of data splits and the method used for evaluation (e.g., holdout, cross-validation) should be added.

2. **Hyperparameter Details**: The paper does not provide specific details on the hyperparameters used for the algorithms involved, nor does it explain how these hyperparameters were chosen. Information on hyperparameter settings is crucial for reproducibility and understanding the robustness of the results.

3. **Optimizer Information**: There is no mention of the type of optimizer used for the algorithms. Including details about the optimizer (e.g., SGD, Adam) and its settings (e.g., learning rate, decay) would enhance transparency and allow for replication of the study.

4. **Active Meta Learning Mechanism**: The description of the active meta-learning approach and the criteria for algorithm selection provides an overview but lacks the depth necessary for implementation. A more detailed account of the selection process and how the iterative active learning process is conducted would be beneficial.

5. **Additional Experimental Details**: While the paper mentions computational cost considerations in the discussion, it does not provide a systematic reporting of computational resources used or the execution time for algorithms. Including these details could be helpful in assessing the feasibility of the approach for practical applications.

Actionable Feedback:

- **Datasets and Splits**: Clarify the methodology used for splitting datasets into training and testing sets, including any use of cross-validation or holdout methods.
- **Hyperparameters**: Provide a detailed description of the hyperparameters for each algorithm tested, including how these were selected or optimized.
- **Optimizer Details**: Specify the type of optimizers used along with their key settings.
- **Algorithm Selection Process**: Elaborate on the criteria and the process involved in the active learning-based algorithm selection, including any thresholds or decision metrics used.
- **Computational Details**: Include a subsection detailing the computational resources and the execution time for each phase of the experiments, highlighting any optimizations or efficiency measures adopted.

",0.5
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"These details are given in the section of “Results"" and the figures in this section.","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ","The authors claim that the paper reports error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments. The justification focuses on the mentioning of these details in the ""Results"" section and the figures therein. However, upon close examination of the ""Results"" section and associated figures in the provided paper content, several issues become apparent when matched against the provided guidelines for reporting statistical significance:

1. **Lack of Clear Explanation for Error Bars Calculation**: The paper mentions the use of 1-sigma error bars but does not clearly explain the method used for calculating these error bars (e.g., closed form formula, bootstrap). This omission makes it difficult for readers to gauge the robustness of the reported results.

2. **Factors of Variability Not Stated**: The guidelines highlight the importance of stating the factors of variability captured by the error bars. The paper falls short in this regard as it does not specify whether the variability arises from train/test split, initialization, or any other experimental condition.

3. **Assumptions About Error Distribution**: The guidelines stress the necessity of stating assumptions made, such as the normality of errors. This paper does not provide any such clarification, leaving readers uncertain about the statistical methods' appropriateness for the data.

4. **Use of Asymmetric Distributions**: While the paper mentions error bars, it does not address the guidelines' caution against using symmetric error bars for asymmetric distributions, if that is the case.

5. **References to Corresponding Figures or Tables**: Although the paper refers to the ""Results"" section and figures for details on error bars and statistical significance, a direct linkage or explanation in the text about how these were calculated—preferably with references to specific figures or tables—is missing. This makes it harder for readers to follow and assess the validity of the statistical claims.

### Actionable Feedback

To address the discrepancies and adhere closer to the guidelines, the following improvements are suggested:

1. **Explicit Method Description**: Include a subsection or a detailed description in the ""Results"" section explaining the method used to calculate the error bars. Clarify whether it was a closed-form formula, a bootstrap method, or another statistical technique.

2. **State Factors of Variability**: Amend the paper to clearly state the factors of variability accounted for by the error bars, providing specifics on whether they arise from train/test splits, random initializations, or other sources.

3. **Clarify Assumptions**: Explicitly mention any assumptions made about the error distributions, such as assuming normality. If normality has not been verified, adjust the confidence intervals or error bars accordingly and state this adjustment in the text.

4. **Address Symmetry in Error Bars**: If the underlying distributions are asymmetric, provide a rationale for the choice of symmetric error bars or adjust the representation to reflect the actual distribution shape.

5. **Detailed References to Figures/Tables**: Ensure direct references to the figures or tables that contain relevant statistical information (error bars, significance tests) are included in the text where these metrics are discussed. This linkage should be clear and specific to improve readability and understanding.

Given the identified discrepancies and the importance of accurately reporting and justifying statistical significance in experimental results, the paper would greatly benefit from addressing these issues.

### Score
",0.5
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,No,We only discussed the related computational expenses.,"• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). Code of Ethics","The author's answer and justification indicate that they acknowledge a lack of detailed discussion on the computational resources used for their experiments, a critical factor for ensuring the reproducibility of their research as outlined in the Guidelines. However, there are specific and actionable improvements the authors can make to align their paper with the Guidelines for disclosing computational resources in experiments:

1. **Specify the Compute Environment**:
   - Add a section or table detailing the compute environment used for each set of experiments, including CPU/GPU specifications, any use of internal clusters or cloud providers, and relevant memory and storage capacities. This action ensures clarity on the computational requirements and facilitates replication efforts.

2. **Detail Computational Requirements**:
   - For each experimental run, provide the computational requirements such as CPU/GPU hours, memory usage, and any storage requirements. This granularity will help in assessing the feasibility of reproduction attempts under different computational resource constraints.

3. **Provide Execution Time**:
   - Include the execution time for each algorithm or model evaluated in the experiments. Sharing this information helps in understanding the computational efficiency and potential bottlenecks in the proposed methods.

4. **Discuss Computational Cost Beyond Experiments**:
   - Disclose if the research project involved preliminary studies, failed experiments, or any related work that required significant compute but was not included in the paper. This disclosure provides a clearer picture of the overall computational investment and potential areas of computational savings or inefficiency.

5. **Supplemental Material**:
   - Consider adding supplementary material or appendices that include scripts, detailed computational logs, or additional data supporting the computational resource claims made in the paper. This addition could further aid in the reproducibility of the research.

6. **Ethical Considerations**:
   - Reflect upon and discuss any ethical considerations related to the compute resources used, especially if utilizing cloud services or shared computing environments. This discussion could include the environmental impact of the computational work and steps taken to mitigate such effects.

Implementing these recommendations will enhance the paper's alignment with the Guidelines for detailing computational resources, ultimately improving its clarity, transparency, and potential for replication by other researchers.

",0.5
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We reviewed the code of ethics and our paper conforms with it.,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ","Feedback:

1. Clarify the research impact and potential risks:
   - The paper should include a section discussing the potential impact of the algorithm recommendation approach, including positive impacts on various domains that can benefit from improved model selection without extensive computational costs. It should also address potential negative impacts, such as the risk of bias in algorithm recommendations if the training data used for meta-learning reflects historical biases.

2. Address compliance with all aspects of the NeurIPS Code of Ethics:
   - The statement that the paper conforms with the NeurIPS Code of Ethics is too generic. Authors should provide specific examples or scenarios in the paper where ethical considerations were taken into account, such as ensuring that the algorithm does not exacerbate existing inequalities or biases present in the input data.

3. Enhance anonymity for the ethical review process:
   - Although the paper aims to preserve anonymity as per guidelines, ensuring that any discussion around the dataset sourcing, algorithm selection, or any potentially identifiable information is anonymized or generalized would strengthen the submission. This includes avoiding mentioning specific datasets or algorithms that might hint at the authors' identity or affiliations.

4. Comprehensive examination of ethical implications in practical applications:
   - The paper would benefit from a deeper analysis of how the proposed active meta-learning approach could be ethically applied in real-world scenarios. This includes an exploration of scenarios where algorithm recommendations could have significant ethical implications, such as in healthcare or criminal justice applications. 

5. Future work on ethical safeguards:
   - It would be constructive to include a section on future work that focuses on developing ethical safeguards for the active meta-learning approach. This could involve techniques for detecting and mitigating bias in algorithm recommendations, ensuring transparency in how recommendations are generated, and involving stakeholder feedback in the development process.

",0.5
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,No,Our paper doesn't discuss potential positive societal impacts and negative societal impacts of the work performed.,"• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ","The proposed author's answer to the checklist question concerning the discussion of both potential positive societal impacts and negative societal impacts of the work performed is ""No,"" with the justification that their paper does not address these impacts. This response indicates a clear understanding of the question. However, based on the NeurIPS guidelines, even if the work is considered foundational and not directly tied to specific applications or deployments, there is an expectation to at least consider or discuss possible societal impacts if there is a conceivable path from the research to applications that could have societal ramifications.

**Feedback:**

1. **Revisit the Potential for Societal Impact:** Given the nature of algorithm recommendation systems and active meta-learning, the authors should reconsider the broader implications of their work. For example, improving algorithm recommendation efficiency might indirectly enable faster development of technologies with considerable societal impacts, both positive (e.g., medical diagnosis, energy efficiency) and negative (e.g., surveillance, automation leading to job displacement).

2. **Discuss General Impacts:** The paper does not need to provide an exhaustive analysis of societal impacts, especially since the work is foundational. However, a general discussion on how advancements in algorithm recommendation could influence the field of machine learning and its application areas would be beneficial. For instance, could the research lead to more efficient use of computational resources, or might it exacerbate biases in algorithm selection if not carefully implemented?

3. **Consider Ethical, Privacy, and Security Considerations:** Even if the work is technical and foundational, potential ethical concerns, privacy issues, and security implications of deploying improved algorithm recommendation systems in real-world settings should be considered. For example, could these systems be misused to preferentially select algorithms that are more invasive of user privacy?

4. **Future Work and Mitigation Strategies:** The authors might discuss future research directions that could help understand and mitigate any potential negative societal impacts identified. This discussion could also cover how the research community, including themselves, could ensure that benefits are maximized while risks are minimized.

5. **Engage with the Community:** A brief recommendation for ongoing or future engagement with broader societal stakeholders (e.g., ethicists, policymakers) could enrich the paper. This engagement would help ensure that any technology developed from the research is deployed responsibly and beneficially.

Addressing these points would not only improve the paper by aligning it with the NeurIPS guidelines but also enhance the authors' contribution to the field by acknowledging and considering the broader context of their work.

**",0.5
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,Our paper poses no such risks.,"• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. Licenses for Existing Assets","The authors' answer to the checklist question about safeguards for responsible release of data or models is marked as ""NA"" with the justification that their paper poses no such risks. This raises several concerns when evaluating the paper's content and alignment with the provided guidelines:

1. **Lack of detail about data sources and potential risks**: The paper details the use of three real-world datasets (StatLog, OpenML, AutoML) and artificial data, but it does not discuss the origins of these datasets in depth, specifically whether any of them could contain sensitive or personal information. Given the increasing sensitivity around data privacy and ethics, especially related to datasets scraped from the internet, it's important that the authors address these concerns directly.

2. **No discussion on the model's potential for misuse**: While the paper focuses on algorithm recommendation via a collaborative filtering approach and may not directly imply risks of misuse in the same manner as, for example, deepfakes or discriminatory AI models, the authors have overlooked the broader implications of their work. Any AI model, including those for algorithm recommendation, could be used in ways that were not intended by the creators. For instance, improvements in algorithm recommendation might accelerate the deployment of models in sensitive or controversial areas.

3. **Lack of safeguards and discussion on dual use**: The paper does not discuss any safeguards or measures taken to ensure that the model or its outputs are used ethically and responsibly. Moreover, there is no discussion about the dual-use nature (useful for both beneficial and harmful purposes) of AI technologies, which is important to consider for any AI research.

### Actionable Feedback:

- **Data Privacy and Ethics**: Include a subsection discussing the origin of the datasets used, with an emphasis on any potential privacy or ethical risks associated with them. If any datasets contain personally identifiable information (PII) or could potentially lead to privacy violations, it's crucial to state how these risks have been mitigated (e.g., through anonymization, data use agreements).

- **Model Misuse**: Address potential misuse of the algorithm recommendation model. Even if the primary application seems benign, consider and discuss hypothetical scenarios where the model could be repurposed for unintended and possibly unethical uses. This section doesn't have to be lengthy but should demonstrate an awareness of the broader implications of the model.

- **Safeguards for Dual Use**: Propose safeguards that could be implemented to prevent or reduce the risk of misuse. This might include licensing agreements explicitly stating acceptable uses, incorporating ethical use clauses, or setting up a request/approval process for access to the model.

- **Robustness and Bias Testing**: Briefly discuss whether the model has been tested for bias or robustness issues, especially since the data sources are diverse. This is relevant to the responsible deployment of any AI system.

By addressing these key points, the paper will demonstrate a more comprehensive understanding of the ethical dimensions and potential societal impacts of AI research, aligning with the growing emphasis on responsible AI development.

### ",0.5
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"creators or original owners of assets (e.g., code, data, models), used in the paper are cited.","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. ","Actionable Feedback:

1. **Explicit Citation and Versioning:** The paper claims that the creators or original owners of assets are cited. However, it does not explicitly mention the version of the assets used nor does it provide URLs where these assets can be accessed. Given the dynamic nature of datasets and code, versioning is crucial for ensuring reproducibility. The authors should revise the paper to include explicit mention of the versions of the datasets, code, and models used, along with URLs or DOIs where these can be accessed.

2. **License Information:** The paper lacks detailed information about the licenses of the used assets. According to the guidelines, the name of the license under which the code, datasets, and models are distributed should be clearly mentioned. This is particularly important for ensuring that the use complies with the original terms set by the creators. The authors should update the paper to include specific license details for each asset used in the study.

3. **Terms of Use and Procurement Process:** While the paper mentions citing the creators or original owners, it does not provide any information regarding the compliance with terms of use, especially for assets that may have specific restrictions or requirements for use. The authors should include a brief description of how they ensured compliance with the terms of use for each asset, including any communication with original authors or checks against the terms of service.

4. **Treatment of Scraped Data:** If any data were scraped as part of the research, the guidelines require that the paper should detail the copyright and terms of service of the source. There is no mention in the provided summary indicating whether scraped data were used or how their use complies with copyright and terms. If scraped data were used, the authors need to add a section addressing these concerns.

5. **Clear Attribution for Repackaged/Modified Assets:** For assets that were repackaged or modified in any way (if applicable), the paper should describe both the original license and the license of the derived asset. This includes any preprocessing steps applied to datasets or modifications to code. The authors should review the paper to ensure this information is accurately and clearly presented if applicable.

By addressing these points, the paper can better align with the NeurIPS guidelines on asset use and citation, enhancing its clarity, reproducibility, and compliance with ethical standards.

",0.5
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,"This is given in the github repo, whose link is given in the paper.","• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ","The proposed author’s answer indicates that new assets introduced in the paper are well documented and that documentation is available in a GitHub repository linked in the paper. However, there are several discrepancies and omissions based on the content provided and the NeurIPS guidelines for documenting new assets:

1. **Lack of Detailed Documentation**: The paper does not provide structured templates detailing the dataset/code/model, such as training details, limitations, and licenses. Simply pointing to a GitHub repository does not fulfill the requirement for comprehensive documentation within the paper itself.

2. **Consent and Anonymization**: The paper does not discuss how consent was obtained for using the assets, nor does it mention any efforts to anonymize the assets as suggested by the guidelines.

3. **Incorporation of Structured Information**: While the GitHub link is provided, the paper lacks direct incorporation of structured information regarding the assets within the text, as is expected based on the guidelines. 

**Recommendations for Improvements**:

1. **Document Detailed Information in the Paper**: Include a section in the paper outlining the details of the datasets, code, or models introduced. This should cover aspects such as the nature of the data/models, the training process, the intended use cases, known limitations, and licensing information. 

2. **Clarify Consent and Anonymization**: Address the aspects of consent for using the data directly within the paper. If applicable, describe the measures taken to anonymize the data, especially if it contains or could potentially be linked to personal information.

3. **Structured Templates for Asset Description**: Utilize structured templates or tables within the paper to describe the assets clearly. This approach could improve readability and ensure that all relevant information is easily accessible.

4. **Direct Linking and Description of GitHub Repository**: While the GitHub repository is mentioned, the paper should provide a more explicit description of what the reader can find in the repository and how it is organized. This could include mentioning the presence of README files, examples of use, and any documentation relevant to the assets.

5. **Address Computational Aspects and Reproducibility**: Given that assets include code or models, discuss the computational requirements for their use. Providing details on the computational environment or setup that was used can aid in reproducibility.

6. **License Details**: Clearly state the licensing under which the assets are released. This is critical for users to understand how they can use, modify, or distribute the assets.

**
",0.5
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ","The provided answer and justification suggest there is no discrepancy regarding the involvement of human subjects or crowdsourcing in the research. After a comprehensive review of the paper's abstract, introduction, state of the art, problem setting, algorithms, results, discussion, conclusion, and supplemental material, there are no indications or mentions of experiments or data collection methods involving human subjects directly or through crowdsourcing mechanisms. Therefore, the initial assessment of marking this checklist question as ""NA"" seems fully justified based on the content presented.

However, ensuring no issues were missed, actionable feedback to enhance the paper's clarity regarding compliance with the NeurIPS checklist question on human subject research and crowdsourcing could include:

1. Explicitly state in the methodology or experiment section that the data were sourced from publicly available datasets or previous literature, emphasizing no direct human subject involvement or new data collection, if this aspect was not thoroughly clarified.
2. If any dataset indirectly involved human subjects in its original collection process (which does not seem to be the case here), a brief note on ethical considerations or referencing the original study's ethics statement might add to the paper's integrity and transparency, despite the authors themselves not conducting human subjects research.
3. Reiterate in the discussion or conclusion the advantages of using existing datasets regarding ethical considerations and the avoidance of crowdsourcing complications, potentially framing it as a strength of the study in terms of reproducibility and minimizing ethical concerns.

Considering the strict adherence to the NeurIPS guidelines regarding research with human subjects and the thorough justification provided, no discrepancies or critical issues were identified that would necessitate remediation before publication. The paper is focused on algorithm recommendation through active meta-learning, utilizing existing datasets without involving humans in the study design directly or indirectly through crowdsourcing. 

",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","Itemized, actionable feedback:

1. **Clarification on the Use of Human and Non-Human Data**: The justification states that the paper involves no research with human subjects, implying an absence of human data. However, the paper mentions the use of real-world datasets (StatLog, OpenML, AutoML) without explicitly stating the data's nature or origin. It is crucial to specify whether any of these datasets were derived from human subjects, directly or indirectly. If any dataset has ties to human-derived data, the authors must clarify this point and discuss the ethical considerations.
   
   - **Action**: Review each dataset mentioned (StatLog, OpenML, AutoML) and explicitly state whether they involve data derived from human subjects. If so, elaborate on the ethical considerations of using such data.

2. **Discussion on Potential Indirect Risks**: The utilization of algorithm recommendation systems can have indirect consequences on individuals, especially if deployed in sensitive areas such as healthcare or finance. The paper should briefly discuss the broader implications of using the proposed system in real-world applications, including any potential indirect risks to individuals. 

   - **Action**: Add a discussion section on the broader ethical implications of applying the presented active meta-learning approach. Specifically, address any indirect risks that might emerge from its real-world application and suggest measures to mitigate such risks.

3. **Clarification on Data Privacy and Consent**: Given that some datasets are ""gathered from the Internet or the literature,"" the paper should clarify if there were any considerations for data privacy and consent, particularly if any of the datasets indirectly involve human subjects or personal data.

   - **Action**: Clearly state the process of dataset selection concerning data privacy and consent. If all datasets used are publicly available and deemed ethical for use in research without specific consent, explicitly state this to clarify.

4. **Institutional Review Board (IRB) or Equivalent Discussion**: Even if the study does not directly involve human subjects, discussing the role of IRB or an equivalent ethical review, especially in the context of selecting and utilizing existing datasets, could enhance the paper's ethical stance. This is important in clarifying that the research adheres to general ethical standards.

   - **Action**: Add a brief statement about ethical considerations, even if IRB approval was not necessary. This may involve a statement on compliance with general ethical guidelines or considerations in selecting and using pre-existing datasets.

",0.5
