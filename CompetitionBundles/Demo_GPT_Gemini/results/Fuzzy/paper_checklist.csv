Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The Introduction states the claims.,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ","The author’s answer and justification imply alignment with the guidelines, suggesting that the claims in the abstract and introduction accurately reflect the paper's contributions and scope. However, a thorough assessment of the provided excerpts against the guidelines reveals that while the main claims are primarily supported by the content, there are instances where enhancements could strengthen the alignment further.

Actionable feedback includes:

1. **Clarify Contributions and Limitations**: Ensure that the introduction explicitly states all the paper’s contributions, as mentioned in the abstract, including the novel approach to handling fuzzy paraphrases and its impact on learning word embeddings. Adding information about limitations directly in the introduction or abstract can also help set realistic expectations for the readers.

2. **Direct Comparisons with Baseline Methods**: The paper claims superior performance over existing methods. To bolster this claim, include a brief comparison in the introduction, highlighting key advantages of the proposed method over others. This provides clearer context on the paper’s position within existing research.

3. **Experimental Results in Abstract**: The abstract mentions that the experimental results show efficiency and performance improvements over previous works. It’s crucial to ensure these results are not overstated and directly reflect the scope of experiments conducted. Clarifying the experimental conditions and the scope of comparisons can help avoid overgeneralizing the findings.

4. **Assumptions and Theoretical Justification**: The abstract and introduction should briefly mention the fundamental assumptions or theoretical grounding that supports the claims. This adds credibility and helps frame the research contributions more robustly, satisfying the guideline that asks for the inclusion of important assumptions.

",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,In Sections 4.5 and 5 (Conclusion) some limitations are presented.,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ","1. **Limitations Section Clarity and Depth**: The authors claim that limitations are discussed in Sections 4.5 and 5, but the depth and clarity of these discussions seem insufficient. For a robust understanding and transparency, it's crucial to convey the limitations in a dedicated section with detailed explanations. Highlighting only in the conclusion or scattered across sections might not meet the NeurIPS checklist's comprehensive discussion criterion. 

    **Action**: Authors should consolidate and expand on the limitations in a dedicated section, clearly specifying the scope, assumptions, and potential impact on the results. For example, detail how the reliance on large corpus sizes affects scalability and applicability in resource-constrained scenarios.

2. **Assumptions and Robustness Discussion**: The paper's brief mention of handling polysemous words through fuzzy paraphrasing misses articulating underlying assumptions explicitly, such as the quality and coverage of the lexicon used, and the model's robustness to lexicon inaccuracies. 

    **Action**: Elaborate on the assumptions related to the lexicon's comprehensiveness and accuracy. Discuss the model's sensitivity to these factors and any measures taken to mitigate potential biases or errors introduced by lexicon limitations.

3. **Empirical Justification of Claims**: While the paper presents experimental results to support the method's efficiency and improvements over previous works, it lacks a critical reflection on the empirical methodology—specifically, the choice of datasets, their representativeness, and the statistical significance of the results. 

    **Action**: Strengthen the limitations section by incorporating a discussion on the generalizability of the experimental findings. Address the choice of datasets, number of runs, and any variability in the results to provide a clearer picture of the method's performance in varied contexts.

4. **Impact of Limitations on Claims**: The paper successfully discusses performance improvements but does not fully explore how the identified limitations might constrain the applicability or interpretation of these claims. 

    **Action**: Reflect on how each limitation might affect the generalizability of the method's claimed benefits. For instance, if the method performs better with larger corpora, indicate how this might limit its utility for languages or domains where large annotated corpora are not available.

5. **Potential for Bias, Privacy, and Fairness Issues**: There is no discussion on how the approach handles ethical considerations such as bias in the lexicon or privacy concerns when integrating various data sources. 

    **Action**: Provide an analysis of potential biases in the lexicon and their impact on model outputs. If applicable, discuss any steps taken to ensure privacy and fairness in the use of data, especially when dealing with sensitive or personally identifiable information.

",0.5
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,No theoretical results are presented.,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. ","The proposed author's ""Answer"" and ""Justification"" imply that the paper does not contain any theoretical results, which would mean no assumptions or full proofs are needed according to the guidelines provided to authors. However, upon reviewing the content of the paper titled ""Fuzzy paraphrases in learning word representations with a corpus and a lexicon"", it becomes evident that there are implicit theoretical foundations in the proposed method for learning word embeddings using fuzzy paraphrases. Specifically, the methodology for estimating the local membership of paraphrases and using these to train word embeddings involves assumptions about the nature of language, the behavior of polysemantic words, and the applicability of fuzzy logic to semantic analysis in natural language processing. These can be considered theoretical underpinnings that warrant a discussion of assumptions and possibly the derivation or proof of the proposed method’s efficacy.

**Feedback:**

1. **Clarify Theoretical Underpinnings**: Even though the paper may not present theorems in a traditional mathematical sense, the novel approach of using fuzzy paraphrases to learn word embeddings is based on theoretical assumptions. These assumptions should be explicitly stated. For example, assumptions about the distribution of polysemantic words in language corpora, the behavior of fuzzy sets in semantic analysis, or assumptions made in the design of the membership estimation function.

2. **Provide Proofs or Justifications**: While full mathematical proofs may not be applicable, a rigorous justification or derivation of the key methodological components should be included. This would involve explaining why the method for estimating paraphrase membership and its subsequent use in training embeddings is expected to be effective. A theoretical rationale, possibly supplemented by preliminary empirical results, could serve this purpose.

3. **Detail the Methodological Framework**: Enhance transparency by detailing the methodological framework underlying the fuzzy paraphrase approach. This should include a discussion on how this method compares to or builds upon existing theories or models in the domain of word embeddings and natural language processing. 

4. **Reference Relied Upon Theories or Lemmas**: If the proposed method relies upon established theoretical results, such as properties of fuzzy sets or principles from distributional semantics, these should be properly cited. Even if these theories form the backdrop rather than the core of the paper, acknowledging them strengthens the paper's theoretical grounding.

5. **Include Discussion on Limitations and Implications**: A thorough discussion on the theoretical limitations and implications of the proposed method would add depth. This includes any constraints on the applicability of the method, potential pitfalls in its theoretical assumptions, and avenues for future theoretical exploration.

**",0.5
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,The experimental section provides a corpus and baselines.,"• The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ","1. **Information Disclosure for Reproducibility**:
   - **Discrepancy Identified**: The ""Answer"" suggests that the experimental section of the paper provides sufficient detail for reproducibility, mentioning ""a corpus and baselines"". However, the ""Justification"" and the detailed review of the ""Paper"" content reveal several gaps in information pivotal for replicating the experimental results accurately.
   - **Specific Gaps**:
     1. **Parameters Detail**: Although the paper outlines the experimental setup, it does not provide exhaustive details regarding hyperparameters used in the model training, such as learning rates, batch sizes, or optimization algorithms, which are crucial for the replication of results.
     2. **Preprocessing Steps**: The detailed preprocessing steps of the corpus (e.g., tokenization, handling of special characters, or processing of polysemantic words and utterances) are not thoroughly detailed, leaving ambiguity in the dataset preparation phase.
     3. **Model Architecture and Implementation Details**: The description of the proposed model’s architecture, specifically the fuzzy paraphrase handling mechanism, lacks depth. Implementation details such as software versions, library dependencies, or pseudocode are absent, making the reproduction of the architecture challenging.
     4. **Evaluation Metrics and Experimental Design**: Detailed information on the evaluation metrics, baselines for comparison, and statistical significance testing methods is sparse. This omission hinders the understanding of the experimental design's robustness and the context of the claimed improvements over baselines.

2. **Actionable Feedback for Improvement**:
   - **Provide Detailed Hyperparameters Information**: Enumerate all hyperparameters and their values used during the experiments. If any parameter tuning process was involved, describe the methodology and rationale behind chosen values.
   - **Elaborate on Preprocessing Steps**: Clearly outline every preprocessing step applied to the corpus before model training. Include specific examples where applicable to illustrate the handling of complex cases.
   - **Enhance Model Architecture Description**: Deepen the explanation of the proposed model’s architecture. Include diagrams where possible and provide pseudocode or snippets demonstrating critical aspects of the implementation. Consider openly sharing the implementation codebase.
   - **Expand on Evaluation Design**: Detail the evaluation metrics used, along with their computation methodology. Describe the baseline models and any state-of-the-art comparisons in greater detail. If possible, include a discussion on the statistical significance of the results obtained.

**",0.5
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,The corpus is open and the code is provided on Github.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ","The proposed author's answer claims that the paper provides open access to both the data and code with sufficient instructions for reproducing the main experimental results. However, upon closer examination of the provided detailed justification and guidelines, several discrepancies and areas lacking clarity or completeness emerge:

1. **Instructions for Reproducing Results:** While the author states that the code is available on GitHub, there is no explicit mention of the thoroughness of the instructions provided for reproducing the results. According to the guidelines, instructions should contain the exact command and environment needed to run to reproduce the results, which includes data access and preparation instructions. This area appears to be inadequately addressed or validated in the provided justification.

2. **Data Access and Preparation:** The justification mentions that the corpus is open but does not detail the process of accessing, preparing, or utilizing the data. The guidelines specifically require authors to provide comprehensive instructions on data access and preparation, including accessing raw data, preprocessing, intermediate data, and generated data. This critical information seems to be missing or underexplained.

3. **Reproducibility of All Experimental Results:** The answer lacks confirmation regarding the scripts provided to reproduce all experimental results for the new proposed method and baselines. According to the guidelines, if only a subset of experiments is reproducible, the authors should state which experiments are omitted from the script and why. This aspect is not discussed in the justification provided.

4. **Anonymity Preservation:** The NeurIPS guidelines suggest that, to preserve anonymity at submission time, the authors should release anonymized versions of the code and data, if applicable. There is no indication in the author's justification that such measures were considered or implemented.

### Feedback

Based on these observations, the following actionable feedback is recommended to improve the paper quality:

- **Provide Detailed Instructions:** Enhance the supplemental material or GitHub repository with detailed instructions for reproducing the experiments, including the exact commands, software, and hardware environment specifics. Ensure these instructions are tested by someone not involved in the project to confirm their clarity and completeness.

- **Clarify Data Access and Preparation:** Offer a meticulous description of the process for accessing, preparing, and utilizing the dataset(s) used in the experiments. This should include step-by-step guidance and any necessary scripts or commands.

- **Ensure Comprehensive Reproducibility:** Confirm that scripts for reproducing all experimental results, including those of baselines and the newly proposed method, are provided. Clearly specify any experiments that cannot be reproduced with the scripts and explain the reasons for these limitations.

- **Adhere to Anonymity Requirements:** If not already done, make sure that anonymized versions of the code and data are made available for the review process, following the guidelines to maintain submission anonymity.

### Conclusion

Given these concerns and the importance of reproducibility for the integrity and impact of the work, improvements in these areas are necessary. 

**",0.5
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,All details are given in the experiment section.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. ","The author's Answer states that all necessary training and test details are specified, but a thorough review of the Paper content reveals several discrepancies between the assertion and the provided experiment details. Key issues identified include:

1. **Data Splits Not Clearly Defined**: The Paper lacks explicit mention of how the data is split between training, validation, and testing sets. For reproducibility and understanding of the model's generalization, details on the data split ratio or method are crucial.

2. **Hyperparameter Details Are Incomplete**: While some hyperparameters like context window size, number of negative samples, and total number of iterations are mentioned, there is insufficient detail on how these hyperparameters were chosen. Information on whether these were empirically determined, derived from previous literature, or optimized through a specific process (e.g., grid search, random search) is missing.

3. **Optimizer Type Not Specified**: The type of optimizer used for training the model is not mentioned in the Paper. Since the choice of optimizer (e.g., Adam, SGD, RMSprop) can significantly affect the training dynamics and final performance, this omission is significant.

4. **Lack of Detail on Membership Estimation and Model Training Procedure**: While the Paper introduces a novel method of using fuzzy paraphrases and mentions using negative sampling for training, a detailed explanation of the training algorithm, particularly how the membership function is integrated into the training process, is lacking. It is crucial for readers to understand these processes to replicate the study and appreciate the novelty and efficacy of the proposed method.

To improve the Paper quality, the following actionable feedback is provided:

- **Clarify Data Split Methodology**: Include a subsection detailing how the dataset was divided or if cross-validation was used. Provide reasons for chosen methods or ratios to enable replication and offer insight into potential biases or limitations.
- **Elaborate on Hyperparameter Selection**: Enhance the methodology section by describing the process through which hyperparameters were selected. If applicable, include a discussion on any hyperparameter tuning or optimization methods employed.
- **Specify Optimizer Details**: Clearly state the type of optimizer used, along with its specific parameters such as learning rate, momentum (if applicable), and any learning rate schedules or decay mechanisms employed during training.
- **Expand on Membership Estimation and Training Procedures**: Include a more thorough explanation or a pseudo-code of the training algorithm, especially how the fuzzy membership function is applied during training. This will illuminate the method's innovation and operational details, contributing to reproducibility.
- **Supplemental Material or Code**: Considering the limited space, including a link to supplemental material or the source code repository can provide the community with full experimental setups, additional hyperparameter values, and training details. This practice aligns with the NeurIPS guidelines for enhancing transparency and reproducibility.

",0
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,The result tables include error bars.,"• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ","The proposed author's answer and justification seem to indicate that statistical significance measures, specifically error bars, are reported for the experimental results. However, a careful review of the provided paper sections and particularly the experimental results and method sections reveals a discrepancy. The paper describes a novel approach to learning word embeddings using fuzzy paraphrases and provides a detailed account of the experiments conducted, including baselines, datasets, and overall performance improvements achieved by the proposed method over existing methods. However, despite mentioning the inclusion of error bars in the result tables in the justification, the provided paper excerpts do not detail the statistical analysis methods employed, such as the calculation of error bars, the statistical significance tests used (if any), the factors of variability they account for, or assumptions made regarding data distribution. 

**Actionable Feedback:**

1. **Clearly describe the statistical methods used:** Authors should explicitly detail the method used to calculate error bars, including whether they represent standard deviations, standard errors, or confidence intervals. If statistical tests are used to establish significance (e.g., t-tests, ANOVA), these should be specified.

2. **Clarify factors of variability:** It is essential to define what the error bars represent. For instance, do they account for variability across different training/test splits, initializations, or both? Providing this clarity would help in understanding the robustness of the findings.

3. **Address assumptions regarding data distribution:** If the error bars are calculated based on assumptions of normal distribution, this should be stated clearly. Alternatively, if data distributions are not normal, an explanation of how error bars or confidence intervals were adjusted should be provided.

4. **Reference the figures or tables with error bars in the text:** Ensure that the text explicitly refers to the figures or tables where error bars are presented, guiding the reader directly to the evidence supporting statistical claims.

5. **Consider discussing the practical significance:** Beyond statistical significance, discussing the practical implications of the observed differences can provide readers with insights into the relevance of the results in real-world applications.

6. **Include error analysis for smaller corpora:** Given the mention of weaker performance on smaller corpora, it would be informative to include statistical analysis for these cases to understand the variability and confidence levels in those scenarios.

Implementing these recommendations would not only enhance the transparency and reproducibility of the results but also provide the reader with a clearer understanding of the performance and robustness of the proposed method.

**",0.5
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,Learning speed is discussed in Section 4.5.,"• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ","The author's answer claims that learning speed, discussed in Section 4.5, addresses the question regarding sufficient information on computer resources for reproducing the experiments. However, the provided content about learning speed does not fulfill the guidelines requiring details on the type of compute workers (CPU/GPU), internal cluster or cloud provider including relevant memory and storage, the amount of compute required for individual experimental runs, estimation of total compute, and disclosure of compute used beyond reported experiments. The justification does not directly correlate with the detailed requirements outlined in the question.

**Feedback:**
1. **Specify Compute Workers:** Clearly specify the type of compute workers used in the experiments, i.e., whether CPUs or GPUs were utilized. Include the model or specifications of these compute resources to give readers a context of the computational power needed.
2. **Detail on Computing Environment:** If any internal cluster or cloud services were employed, these should be disclosed along with any specific configurations relevant to the experiment's reproduction.
3. **Memory and Storage Information:** Provide explicit information about the memory and storage requirements for running the experiments. This data helps in understanding the scalability of your method and its feasibility under different computational budgets.
4. **Compute Requirements:** Precisely document the compute requirements for individual experiments, including but not limited to, the number of CPU/GPU hours and the type of resources consumed. If possible, break down the compute requirements per major experiment or model training procedure described in the paper.
5. **Estimation of Total Compute:** Offer an estimation of the total computational resources expended throughout the research, including experiments not directly reported in the paper. This transparency aids in assessing the environmental impact and the practicality of replicating or building upon your work under different resource constraints.
6. **Address Unreported Experiments:** If applicable, disclose whether the reported research project consumed more compute resources beyond what is documented due to preliminary or failed experiments. This disclosure supports the research community in making informed decisions about the efficiency and resource demands of proposed methods.

Improving on these areas will significantly enhance the paper's alignment with NeurIPS guidelines for experimental reproducibility and may mitigate reviewer concerns regarding the practical applicability and environmental cost of the proposed methods.

**",0.5
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We are aware of the code of Ethics and read it. All is conform.,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ","The authors claim that their research conforms to the NeurIPS Code of Ethics. However, their brief justification provides minimal evidence or detailed explanation on how exactly their work adheres to these ethical guidelines beyond stating their awareness and conformity to the code. Based on the provided guidelines, a more thorough explanation or justification for their claimed adherence to the ethics code is expected. Additionally, preserving anonymity in their justification as per the guidelines is not directly relevant in this context since it pertains to the content review rather than ethical considerations.

Itemized, Actionable Feedback:
1. Expand the Justification: The authors should provide a thorough explanation of how each part of their research process adheres to the NeurIPS Code of Ethics, including the collection of data, the methodology applied, the analysis of results, and the presentation of findings.
2. Address Specific Ethical Considerations: Identify and address specific ethical considerations relevant to the research, such as consent for data used, privacy concerns with any collected data, potential biases in the data or algorithms, and any impact the technology may have if deployed.
3. Clarify Anonymity Statement: While preserving anonymity is part of the ethical guidelines, the response should clarify how the research itself, not just the paper submission, upholds the ethical standards regarding anonymity where applicable, such as in data collection processes or in protecting the identity of any individuals indirectly involved in the research.
4. Potential Misuse Discussion: Include a discussion on potential misuse of the research findings or technology developed, reflecting on the ethical guidelines that encourage consideration of broader impacts of the research.
5. Transparent Reporting: Ensure that the paper transparently reports all aspects of the research, including any limitations and failures, which aligns with ethical guidelines that promote transparency and honesty in research.

Concluding with a score based on the requirement for critical improvements identified:

",0.5
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,We discuss this in the Related Work section.,"• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ","The proposed author's ""Answer"" and ""Justification"" indicate a clear intention to address the societal impacts in their paper. However, there are discrepancies between these statements and the actual content of the paper as per the available excerpt and structure provided. Specifically, the paper largely focuses on the technical aspects of learning word embeddings with fuzzy paraphrases and does not provide a dedicated discussion on the societal impacts–positive or negative.

Feedback to improve the paper quality based on the guidelines:

1. **Explicit Section on Societal Impacts**: Add a dedicated section discussing both the potential positive and negative societal impacts of your work. Although foundational, your research could have implications (intended or unintended) that merit examination. This section should go beyond a brief mention in the ""Related Work"" and instead provide specific insights based on your research findings and methodology.

2. **Potential Positive Impacts**: Elaborate on how improved word embeddings can benefit societal goods, such as enhancing language models' understanding in assistive technologies, improving accessibility for non-native speakers, or benefiting educational tools. Concrete examples or speculative scenarios based on current or near-future applications would make this discussion more impactful.

3. **Addressing Negative Impacts**: Discuss potential risks, such as the misuse of advanced NLP technologies in creating misleading information (e.g., deepfakes) or in surveillance. Consider and suggest mitigation strategies for the negative impacts, even if they seem indirect or speculative. For instance, discuss fairness considerations, privacy, and the robustness of your method against adversarial uses. 

4. **Mitigation Strategies**: Where you identify negative impacts, provide suggestions on how they might be mitigated. This could include technical safeguards, policy recommendations, or areas for future research that specifically aims to address these challenges.

5. **Contextual Relativity of Impacts**: Acknowledge that the societal impacts may vary by region, culture, or application scenario. Discuss how your technology might have different implications depending on its deployment context and suggest ways to engage with stakeholders to minimize harm.

6. **Generalization of the Work**: If your work has broader implications for the field of machine learning or AI, articulate these carefully. How might advancements in your method influence the development of AI systems in general, and what societal impacts might this have?

7. **References to Prior Work on Societal Impacts**: Where applicable, refer to studies or analyses that have discussed similar technologies and their societal impacts. This can help ground your discussion in established literature and provide readers with routes to further explore these important issues.

Ensuring that your paper directly addresses the potential societal impacts of your work will not only align it with the guidelines provided by the NeurIPS conference but also offer a more comprehensive view of your research contributions.

",0.5
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,Our research does not present such risks.,"• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ","Upon review of the author's response and justification to the checklist question regarding responsible release of data or models, in relation to the content presented in their paper, it's observed that there might be a misunderstanding or oversight by the authors on the potential risks and the necessary considerations for responsible release discussed in the guidelines. The paper revolves around the use of a corpus and a lexicon for learning word embeddings, specifically addressing the issue of polysemantic words and how their meaning changes across different contexts. Considering the paper's methodology and scope, the following feedback outlines key areas for improvement:

1. **Clarify the Nature of the Data and Models**: The authors argue their work ""does not present such risks,"" suggesting a perspective that their research inherently carries no risk of misuse. However, with the increased scrutiny on natural language processing technologies, it becomes imperative to scrutinize any potential for misuse meticulously. The authors should reassess the nature of the data (corpora) and the models (word embeddings) used or produced, including how the lexicon influences word embeddings, to ensure comprehensive identification and mitigation of risks.

2. **Detailed Assessment of Misuse Potential**: Given the paper's focus on learning word embeddings that improve semantic information extraction, there's a latent potential for misuse in sensitive applications. For instance, refined word embeddings could, theoretically, enhance the capabilities of systems with intentions that contradict ethical guidelines (e.g., surveillance, biased content generation). The authors should conduct a more detailed assessment of how their generated models or methods could be misapplied in various contexts and describe any safeguards or recommendations to mitigate such risks.

3. **Discuss Safeguards for Lexicon Use**: Although the paper concentrates on a novel method for using fuzzy paraphrases from a lexicon to learn word embeddings more effectively, it lacks a discussion on the ethical considerations or safeguards concerning the lexicon's composition and usage. The authors should describe the provenance of the lexicon, any potential biases within, and methods employed to ensure its responsible usage.

4. **Transparency and Accessibility Considerations**: If the authors have or plan to make their models available, they should outline measures taken to ensure transparency and ethical use, despite claiming “NA” on the checklist. This could include usage guidelines, access restrictions, or methods to ensure the model’s application adheres to ethical standards. If the models or data are to be publicly released, clarifying these points would be beneficial for promoting responsible AI research.

5. **Engagement with Broader AI Ethics Discussions**: Finally, the paper would benefit from engaging with broader discussions on AI ethics and responsible research. Including a section discussing the societal implications of their research, even if perceived as benign, would demonstrate a holistic consideration of the research’s impact.

By addressing these aspects, the authors can significantly strengthen the paper's consideration of ethical and societal implications, aligning with the NeurIPS guidelines for responsible AI research.

**",0.5
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,All is conform.,"• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. ","The proposed author's ""Answer"" asserts that all assets (e.g., code, data, models) used in the paper have been properly credited, and their licenses and terms of use have been explicitly mentioned and respected. However, the ""Justification"" provided by the authors is vague and does not specify how this compliance has been achieved, simply stating ""All is conform."" This response does not adhere to the detailed requirements outlined in the guidelines, which include citing the original papers that produced the assets, stating which version of the asset is used, including URLs where possible, explicitly naming the licenses, and providing detailed information on the use of any scraped data, including compliance with copyright and terms of service.

### Actionable Feedback:

1. **Cite Original Creators and Licenses:** Ensure that every external asset (datasets, code libraries, or models) used within the research is accompanied by a citation to the original source paper or release. Also, clearly state the license under which each asset is available. For instance, if PPDB2.0 dataset [22, 21] is used, specify its version, source citation, and license explicitly.
   
2. **Version and URL:** Include specific versions and URLs for the assets used, such as PPDB2.0. This will aid in the reproducibility of your research. If the work employs a publicly available model, the exact version number and the source URL must be documented.

3. **Asset Modification and Repackaging:** If any asset has been modified or repackaged as part of the research, describe these modifications, the rationale behind them, and how the licensing might be affected. Both the original and derived license terms must be provided.

4. **Compliance with Scraped Data Regulations:** If the research involves data scraping from websites or online resources, document compliance with the respective source's terms of service or copyright claims. Include a discussion on the legality and ethical considerations of the scraping process.

5. **Clear Justification on Compliance:** Enhance the justification section by providing detailed evidence of compliance with each guideline point. Avoid general statements like ""All is conform."" Instead, detail steps taken to ensure compliance, such as obtaining permissions or conforming to open licenses.

6. **Enhance Transparency:** Consider releasing any code or datasets produced during the research under an open license, clearly stating the terms. This practice promotes transparency, reproducibility, and further research.

By acting on this feedback, the paper will provide clear, transparent, and comprehensive documentation of the use of external assets, including data, models, and codebases. This will not only align the submission with NeurIPS guidelines but also enhance the paper's credibility and reproducibility.

**",0.5
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Not Applicable,No new assets are provided.,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ","The authors' answer to the checklist question about the documentation of new assets introduced in the paper (""NA"") with the justification that no new assets are provided appears, at face value, to be in contradiction with the paper's content regarding the use of a corpus (enwiki9) and a lexicon (PPDB2.0) for training their model. While these assets might not be originally introduced by the authors themselves, the utilization and modification of these assets for their specific methodology (i.e., continuous bag-of-fuzzy-paraphrases (CBOFP) model) implies a degree of adaptation or generation of new assets that should be documented according to the provided guidelines.

**Actionable Feedback:**

1. **Clarify Definition of Assets:** The authors should clarify what constitutes a ""new asset."" If the methodology results in customized versions of the existing corpus and lexicon or generates new word embeddings, these should be considered as new assets, warranting documentation.

2. **Provide Documentation for Adapted Assets:** If the authors have modified existing assets (enwiki9 and PPDB2.0) for their methodology, it is imperative to document the modifications. Details such as the preprocessing steps, modifications to the lexicon, and any filtering criteria applied should be included. Providing this information will enhance reproducibility and transparency.

3. **Document Generated Word Embeddings:** Assuming the CBOFP model generates new word embeddings, the paper should include a structured template mentioning the training parameters, size of the embeddings, and an anonymized link or structure for accessing these embeddings. This is crucial for fostering reproducibility and allowing others in the community to benefit from the authors' work.

4. **Detail Consent for Used Assets:** Even if enwiki9 and PPDB2.0 are public assets, the paper should discuss the consent or permissions associated with their use for training word embeddings, especially if the assets are adapted in ways not originally intended by their creators.

5. **Anonymize Assets If Applicable:** Given that the review process for NeurIPS is double-blind, any provided links or references to the assets generated by the authors should be anonymized. If the authors are hosting the modified corpus or generated embeddings on a website, an anonymized URL or a cloud storage link with restricted access could be used.

6. **License Information:** If new assets are created, the paper should specify the license under which these assets are released. This is essential information for future users of these assets.

By addressing these points, the authors will not only comply with the submission guidelines related to asset documentation but also significantly improve the accessibility and impact of their contributions to the research community.

**",0.5
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,No research using crowdsourcing.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ","Based on the information provided in both the answer and justification sections by the authors and the actual content of the paper, there is a discrepancy related to the involvement of human participants or the use of crowdsourcing techniques. The provided answer by the authors is ""NA"" with the justification indicating that the research does not involve crowdsourcing. However, upon careful examination of the paper, there is no clear statement or section that explicitly confirms or denies the involvement of human subjects or the use of crowdsourcing for experiments. Considering the focus of the paper on learning word embeddings and the methodology described, it is plausible that human assessment could have been part of the validation process for the quality of word embeddings, such as in evaluating the semantic accuracy of paraphrases generated by the proposed method. 

### Actionable Feedback
1. **Clarification on Human Participation:** If there was indeed any form of human involvement in the assessment or any other part of the research, this should be clearly stated along with details as requested by the guidelines. This includes full text of instructions given to participants, screenshots if applicable, and details about compensation. If this information is present but not labeled or integrated correctly, reorganize the document to align with the submission guidelines.

2. **Explicit Statement Needed:** If the research genuinely did not involve any humans in the process, for clarity and to avoid assumptions or ambiguity, explicitly state within the paper that ""This research did not involve crowdsourcing experiments nor research with human subjects."" This statement can be included in the methodology section or as a footnote at the beginning or end of the relevant sections.

3. **Supplemental Material Check:** If there was any human participation that was documented in supplemental materials but not referenced correctly in the main paper, ensure that a clear reference is made to where the reader can find these details. This is important for transparency and for meeting ethical guidelines.

4. **Ethics and Acknowledgment Section:** Consider adding an ethics statement or acknowledgment section that clearly addresses or mentions the aspect of human participation. Even if participants were not used, stating the ethical considerations taken when designing the experiment could enhance the quality and integrity of the research paper.

5. **Revisit Guidelines:** Review the NeurIPS submission guidelines again to ensure all aspects concerning reporting research involving human participants or crowdsourcing are fully understood and accurately applied to the paper. This ensures compliance and addresses any potential ethical concerns.

By addressing these points, the paper can improve its alignment with the NeurIPS guidelines and ethics considerations, enhancing its acceptability.

### ",0.5
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,No human subjects.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The authors' response to the NeurIPS checklist question regarding potential risks to study participants and details on the disclosure and IRB approvals is marked as ""NA"" with the justification that there were no human subjects involved. However, their paper introduces a method for learning word embeddings that indirectly involves human-created resources, such as lexicons, ontologies, and a corpus derived from the English Wikipedia. While it's clear that direct human subjects were not part of an experimental study (e.g., in psychological or medical research), the use of human-created data sources introduces ethical considerations that the authors have not addressed. Specifically, biases present in the corpus and lexicon data sources could propagate or even amplify through the model, which constitutes an indirect risk related to the use of the developed technology.

### Actionable Feedback

1. **Clarify Ethical Considerations**: Even though direct human subjects were not involved, include a brief section discussing ethical considerations related to the use of human-generated data sets. This should cover potential biases in the data and their implications on the model outputs.

2. **Potential Misuse of Technology**: Consider and mention any potential for misuse of the developed technology, including but not limited to perpetuating gender, racial, ethnic biases found in natural language data or misuse in surveillance and privacy intrusions.

3. **Mitigation Strategies**: Suggest strategies or methods employed in the research to mitigate any potential harm identified above, such as debiasing techniques or limitations in the application of the technology.

4. **Future Work Recommendations**: Offer recommendations for future research to further address these issues, acknowledging that while this work does not directly involve human subjects, the outcomes of the research have broader societal implications that warrant careful consideration.

5. **Data Source Disclosure**: Explicitly acknowledge and describe the sources of your data (e.g., Wikipedia, PPDB2.0), including any known limitations or biases within these sources. This transparency can help readers better understand the context and potential limitations of your findings.

6. **Ethics of AI Development**: Briefly discuss the ethics of AI development in relation to your work, especially focusing on responsible AI usage that promotes fairness and equity.

By addressing these points, the paper can offer a more comprehensive view that respects the ethical landscape of AI research, acknowledging that the absence of direct human subjects does not exempt the work from ethical considerations related to its potential impact and the data used.

### Score
",0.5
