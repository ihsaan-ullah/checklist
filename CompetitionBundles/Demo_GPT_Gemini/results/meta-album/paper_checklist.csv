Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. . ","**Feedback**

1. **Comprehensive Scope and Utility Description**: Ensure the abstract and introduction explicitly delineate the comprehensive scope and innovative utility of Meta-Album across diverse ML tasks, such as few-shot learning, transfer learning, and meta-learning. It is crucial that the paper underscores how Meta-Album, with its variety in dataset domains and configurations, addresses existing gaps in multi-domain few-shot learning and generalizable AI model development.

2. **Clarification of Extensibility and Community Engagement**: The paper should clearly communicate the extensibility of Meta-Album and the mechanisms for community engagement. Explain the process for external contributions to the dataset repository, including quality control, dataset formatting standards, and the review process for inclusion. This will not only highlight Meta-Album's commitment to growth and relevance but also encourage active participation from the research community.

3. **Detailed Methodological Innovations and Comparisons**: In both the abstract and introduction, provide a concise yet detailed account of the methodological innovations introduced with Meta-Album. Include comparisons with existing meta-datasets to comprehensively establish Meta-Album's uniqueness in terms of scale, diversity, and utility. Such a comparison will help readers appreciate the value addition Meta-Album brings to the field.

4. **Explicit Assumptions and Limitations**: The paper should clearly enumerate any underlying assumptions and potential limitations of Meta-Album. While it's evident that the dataset facilitates a wide range of ML tasks, an open discussion on its limitations (e.g., in terms of domain coverage, data balance, etc.) will provide a balanced view and potentially spur future enhancements.

5. **Use Cases and Implications for Research and Practice**: Expand the discussion on the implications of Meta-Album for both fundamental research and practical applications. Include potential use cases that illustrate how Meta-Album can be leveraged to push the boundaries in few-shot learning, cross-domain generalization, and beyond. This will not only showcase Meta-Album’s immediate utility but also its potential to inspire new research directions.

6. **Clarify Aspirational Goals**: If the paper includes aspirational goals for Meta-Album, these should be clearly marked as such and distinguished from current contributions. This transparency will help manage reader expectations and underscore the forward-looking vision of the project.

**Conclusion**

Ensuring that the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope is pivotal for setting correct expectations and demonstrating the paper's value. The outlined recommendations focus on enhancing clarity, comprehensiveness, and engagement to underscore Meta-Album’s novelty and utility.

**",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,We have clearly explained the limitations of our work in Section: Discussion and conclusion,"• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate """" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. . ","**Feedback:**

1. **Separate Limitations Section:** The authors mention that limitations are discussed in the ""Discussion and Conclusion"" section. However, the guidelines suggest creating a separate ""Limitations"" section. It would improve transparency and make it easier for readers to find and understand the limitations.
   
   **Actionable recommendation:** Create a distinct ""Limitations"" section in the paper to explicitly discuss the limitations.

2. **Depth of Limitation Discussion:** The authors need to ensure the discussion on limitations critically reflects on the scope of the claims made and the robustness of the proposed method against the assumptions. This includes discussing the dataset size, diversity, representation, and any potential biases it might introduce.

   **Actionable recommendation:** Expand the discussion on limitations to include: 
   - How the choice and diversity of datasets in Meta-Album influence the generalizability of the proposed few-shot learning method.
   - Potential biases in the datasets and their impact on model performance across different settings.
   - Limitations related to dataset size and computational resources required for training and evaluating the models.

3. **Robustness to Violations of Assumptions:** The paper should consider the robustness of their method against violations of assumptions such as independence, noise, and model well-specification. If these aspects are not yet thoroughly considered or evaluated, this constitutes a significant limitation.

   **Actionable recommendation:** Conduct additional experiments or theoretical analysis to evaluate the robustness of the proposed method against violations of key assumptions. Discuss these findings in the ""Limitations"" section.

4. **Evaluation Scope:** The limitation discussion should reflect on the scope of evaluation, particularly if only a few datasets or runs were used for testing. 

   **Actionable recommendation:** If applicable, acknowledge the limitation related to the evaluation scope and suggest further studies or experiments that could help in understanding the method's performance across a wider range of conditions.

5. **Transparency in Limitation Disclosure:** The paper should maintain complete honesty about its limitations. Avoiding or understating limitations might negatively affect the paper's credibility. This integrity in discussing limitations is imperative for the research community's trust.

   **Actionable recommendation:** Ensure that the ""Limitations"" section is written with utmost honesty, clearly stating all potential weaknesses and areas for future improvement. This includes methodological limitations, data-related constraints, and any assumptions that may not hold in practice.

",0.5
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset,"• The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. . ","The provided author's ""Answer"" and ""Justification"" match the guidelines for a paper that does not contain theoretical results, marking it as N/A (""Not Applicable""). The paper's focus on a meta-dataset creation and empirical experiments using this dataset aligns with stating that the question about providing assumptions and proofs for theoretical results is not applicable.

**Actionable Feedback:**

1. **Clarify Dataset Contributions**: Although the justification emphasizes the empirical nature of the research, the paper could benefit from a short segment in the methodology or introduction sections that clearly outlines why the distinction between theoretical and empirical work is pertinent to the research contribution. This helps to set expectations for readers familiar with theoretical papers.

2. **Reference Similar Works**: To further bolster the justification, the paper could reference other empirical papers, especially in similar domains or those using meta-datasets for few-shot learning, that also classify theoretical questions as N/A. This will provide a precedent and strengthen the authors' position.

3. **Expand on Empirical Focus**: While the justification is clear, adding a few sentences about how the empirical focus (experiments and dataset creation) pushes the field forward could provide more context. This can articulate the novelty and importance of focusing on empirical results over theoretical ones in this specific research area.

4. **Outline Future Theoretical Implications**: Even if the paper is empirical, a discussion on any potential theoretical implications or future theoretical research directions prompted by the empirical findings could bridge the gap between the two types of research. This wouldn’t contradict the N/A answer but enrich the paper's contribution to the broader research landscape.

5. **Methodology Details**: Ensure the experimental setup and dataset creation processes are detailed meticulously. Given the absence of theoretical results, the empirical methodology becomes the core strength of the paper. Clear, replicable, and robust experimental design and execution are crucial.

6. **Address Any Possible Theoretical Extensions**: If applicable, briefly discuss how the dataset or experimental findings could serve as a foundation for future theoretical work. This doesn’t undermine the N/A to theoretical questions but shows openness to interdisciplinary research growth.

**
The recommended improvements are not critical flaws but enhancements that could significantly augment the paper's clarity, impact, and reception. Addressing these points would make the empirical contributions more robust and better understood within the context of broader research discourse.",0.5
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","The proposed author's answer suggests full disclosure for reproducibility based on the provision of code and dataset access through a GitHub repository and a dedicated website. This response might suggest completeness regarding the guidelines on ensuring reproducibility. However, even with code and data access provided, the commitment to complete reproducibility extends beyond the mere availability of resources. To align with the NeurIPS guidelines and best practices in reproducible research, it's imperative the authors also offer:

1. **Detailed Environment Specifications:** Merely stating the code is available does not ensure reproducibility if the environment specifics, including software versions and hardware requirements, are unmentioned. Including a Dockerfile or the equivalent could remedy this gap.
   
2. **Data Access and Preparation Instructions:** While the answer mentions dataset details are available on their website, it lacks clarification on any pre-processing steps necessary to use the data effectively. Explicit instructions on data handling align with NeurIPS guidelines for reproducibility.

3. **Complete Command Set for Experiments:** The provision of code on GitHub is commendable, but the absence of exact command lines or scripts that replicate the experimental results could hinder the process. The authors should include a step-by-step guide or a makefile to ensure peers can exactly reproduce the results.

4. **Explanation on Subset of Experiments:** If the scripts do not cover all experimental results presented in the paper, the authors must clarify which are omitted and why. This disclosure ensures transparency and aligns with the guidelines.

5. **Anonymized Version for Review:** As per the submission policy, providing an anonymized version of the code and data (if applicable) during the review phase is crucial to maintain the integrity of the double-blind review process.

#### Actionable Suggestions:
- Include a detailed `README` file in the repository explaining how to set up the environment, including dependencies with their versions.
- Provide clear instructions on accessing, processing, and using the dataset for replication purposes.
- Offer a comprehensive script or command list that directly reproduces the experiments and results featured in the paper.
- Clearly state any experiments not covered by the provided scripts and the reason for their exclusion.
- Ensure an anonymized version of the dataset and code is available for review, respecting the NeurIPS submission guidelines.

Considering the guidelines and the content of the paper, improvements are necessary for ensuring the reproducibility of the research. The current provision does indeed show effort towards reproducibility but falls short in terms of detail and thoroughness expected by the guidelines provided by NeurIPS.

**",0.5
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,We have publicly provided the code on our github repository and details about how to access the datasets on our website.,"• The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. . ","**Feedback for Improvement:**

1. **Dataset Domain Categorization and Extensibility:**
   - The paper should clarify the process or criteria used for categorizing datasets into domains. Providing specific examples or a clearer methodology for this categorization will improve the readability and usability of Meta-Album for different research purposes.
   - For extensibility, while it’s mentioned that Meta-Album is designed to be extended by the community, the paper should detail the process for contributing new datasets. Including guidelines, review criteria, and how contributions will be integrated can encourage community participation and ensure quality control.

2. **Uniformity in Dataset Processing:**
   - The paper notes differences in preprocessing steps across datasets (e.g., cropping, resizing, background padding). It’s crucial to standardize these preprocessing steps as much as possible to ensure uniformity and minimize potential biases introduced during this phase. The paper should provide a more detailed justification for the chosen preprocessing techniques, especially in cases where specific datasets require unique treatment.

3. **Benchmarking on Cross-Domain Few-Shot Learning:**
   - The experimental section focusing on within-domain and cross-domain few-shot learning provides extensive details on performance. However, the paper would benefit from a deeper analysis of why certain models perform better or worse in specific domains. Insights into model behavior across different domains could provide valuable knowledge for future research in few-shot learning.

4. **Accessibility and Usability:**
   - While the paper mentions that all datasets in Meta-Album are reformatted and preprocessed for ease of use, it should further discuss the accessibility features implemented. For instance, how Meta-Album facilitates easy dataset downloading, manipulation, and benchmark setup could be elaborated. Providing examples of a few lines of code to get started with Meta-Album might drastically improve its adoption.

5. **Baseline Methods and Benchmarking:**
   - The selection of baseline methods for benchmarking Meta-Album’s datasets is crucial. The paper should discuss the rationale behind choosing specific baseline models for few-shot learning and how they are representative of the current state-of-the-art. Additionally, considering the inclusion of newer or more varied baseline methods could improve the thoroughness of the benchmarking.

6. **Discussion on Potential Biases and Limitations:**
   - A more detailed discussion on potential biases in Meta-Album datasets (originating from data collection or preprocessing) and how they could affect benchmark results is necessary. The paper should also outline the limitations of Meta-Album, including any domain-specific challenges that were encountered but not addressed in the current version.

7. **Comparison with Other Meta-Datasets:**
   - While the paper compares Meta-Album with other datasets, focusing on specific areas where Meta-Album outperforms or lacks compared to these datasets could provide clearer positioning. Elaborating on how Meta-Album addresses gaps in existing meta-datasets can highlight its contribution more effectively.

8. **Future Directions:**
   - Specifying more concrete future directions for Meta-Album, including planned domains or datasets, expected updates, and how feedback from the research community will be incorporated, can help readers understand the roadmap and potential long-term impact.

**",0.5
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.,"• The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. . ","The proposed authors' answer suggests full compliance with the requirement to specify all necessary details about training and testing setups. However, the justification introduces potential discrepancies or lacks needed for a comprehensive understanding, as it refers to two sources for additional details—Section 3.2 in the paper and an external GitHub repository. This bifurcated approach may not fully align with the guidelines that emphasize the importance of providing experimental settings in the core of the paper to a satisfactory level of detail. Here is an itemized, actionable feedback based on the provided guidelines:

1. **Consolidation of Experimental Details**: Ensure that all pivotal experimental setup details are accessible within the main body of the paper, specifically in Section 3.2 as mentioned. This includes hyperparameter settings, optimizer details, data splits, and any preprocessing steps.
    - Action: Revise the section to include missing details directly within the paper or provide a clear, concise appendix within the same document.

2. **Clarification of External Resources**: If additional details are provided in an external GitHub repository, explicitly state what those details are and their relevance to understanding the experiments and results.
    - Action: Add a paragraph in Section 3.2, summarizing the nature of the details available in the GitHub repository and how they complement the information provided in the paper.

3. **Justification for External Details**: Provide a rationale for why certain details are relegated to the GitHub repository instead of being included directly in the paper or its appendix.
    - Action: Briefly explain in Section 3.2 why some details are external, focusing on the intention to keep the paper concise, provide dynamic updates, or offer executable code that complements the static descriptions in the paper.

4. **Accessibility and Permanence**: Ensure that the external GitHub repository is publicly accessible and maintained for a foreseeable future to serve as a reliable resource for reviewers and readers.
    - Action: Mention the commitment to maintain the GitHub repository in the paper and check that the repository settings allow public access.

5. **Version Control**: Clearly specify the status or version of the repository content at the time of the paper submission to ensure that changes to the repository after submission don’t lead to inconsistencies between the paper and the repository.
    - Action: Include a repository commit hash or release version in the paper to reference the exact state of the supplementary details as they relate to the paper submission.

6.  **Details on Criteria for Hyperparameter Selection**: Often, the decision-making process for choosing hyperparameters, including the number of layers in a neural network, the learning rate for optimization, or batch sizes, is critical for the reproducibility and understanding of the results.
    - Action: Elaborate on how hyperparameters were chosen, whether through grid search, literature precedent, or experimentation, and include this discussion either in the main paper or clearly reference its location in the supplementary materials.

Conclusion:
Ensuring that the paper serves as a standalone resource for understanding the experimental setup and results, while using external repositories judiciously to provide supplementary materials, will enhance its clarity, integrity, and contribution to the field.

",0.5
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars","• The answer NA means that the paper does not include experiments. • The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. . ","The proposed author's answer and justification for reporting error bars suitably and correctly defined, or other appropriate information about the statistical significance of the experiments, appear partially adherent to the guidelines provided. The authors report including error bars in various figures and appendices for within-domain and cross-domain few-shot learning settings. However, the justification offered lacks depth concerning the specifics of the error bars, particularly regarding how they were calculated and what factors of variability they capture. According to the guidelines, not only should error bars be reported, but the paper should also:

1. Clearly state the factors of variability captured by the error bars (e.g., train/test split, initialization).
2. Explain the method used for calculating the error bars (e.g., closed-form formula, bootstrap).
3. Specify the assumptions made (e.g., normally distributed errors).
4. Clarify whether the error bar represents standard deviation or standard error of the mean.
5. If applicable, mention whether the error bars are symmetric or not, especially for asymmetric distributions.

#### Feedback for Improvement

1. **Clarification on Error Bars Calculation**: The paper needs to include more detailed information on how the error bars in figures-2, figures-3, Appendices E, and F were calculated. Specify the statistical method used (e.g., standard deviation, confidence intervals, bootstrap methods) and mention any assumptions made (e.g., data normality).

2. **Factors of Variability**: Clearly articulate the factors of variability captured by the error bars. It is crucial to mention whether the variability comes from different train/test splits, model initializations, or other sources of randomness in the experimental setup.

3. **Symmetry of Error Bars**: If the distribution of calculated metrics (such as accuracy or error rates) is asymmetric, ensure this is properly represented or acknowledged when reporting error bars. This is particularly important for metrics that are bounded or not normally distributed.

4. **Reference in the Text**: Enhance the paper by explicitly referencing the figures or tables where error bars are reported in the corresponding methodological or results sections. This inclusion will improve the readability and accessibility of the statistical significance of the results.

5. **Error Type Clarification**: Specify whether the reported error bars represent the standard deviation or the standard error of the mean. This distinction is crucial for readers to understand the presented data correctly.

Implementing these recommendations will increase the paper's alignment with the guidelines for reporting statistical significance and improve its clarity, rigor, and accessibility for readers.

#### Conclusion

Enhancements in the reporting and explanation of error bars, as suggested, will benefit the paper by ensuring its adherence to the statistical reporting standards expected at the NeurIPS conference. Presently, the description provided seems partially complete but needs more depth concerning methodology and assumptions.

**",0.5
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning","• The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). . ","The proposed author's answer and justification for sufficient information on computer resources needed to reproduce experiments are generally supportive of the guidelines provided. However, the review of the actual content and the appendices provided in the summary text raises a few critical points that need to be addressed to ensure the paper fully complies with the provided guidelines. Here is the itemized, actionable feedback:

1. **Clarify Compute Resource Details**: The paper mentions the type of GPUs used in the experiments but lacks specific details such as the CPU specifications, the exact model of GPU (beyond the family), memory, storage, and the type of internal cluster or cloud provider (if used). It is recommended to include these details in Section 3.2 and Appendices E & F.

2. **Compute Required for Experiments**: Although Appendices E and F mention the use of GPUs, the exact amount of compute required for individual experimental runs is not detailed. Authors should provide estimates on the number of GPU hours or the computational cost associated with each experiment to enhance reproducibility.

3. **Discuss Preliminary or Failed Experiments**: The paper does not disclose whether the full research project required more compute than the reported experiments due to preliminary or failed experiments. Including a discussion on the computational resources spent on preliminary testing, failed experiments, or hyperparameter tuning would provide a clearer picture of the compute requirements.

4. **Validation and Testing Computing Details**: While the paper mentions the use of specific GPUs for experiments, it does not clarify if the validation and testing phases required the same or different computational resources. Clarifying whether the computational setup was consistent across all experimental stages will help in reproducibility.

5. **Impact of Different Compute Environments**: The influence of using different computational environments (such as different GPUs or cloud setups) on the experiments' outcomes is not discussed. A brief discussion on how the findings might vary with changes in the computational setup would be informative.

6. **Batch Sizes and Training Times**: Although the paper provides details on the GPUs used, information on the batch sizes for training, the number of training epochs, and the total time taken for training and testing would aid in replicability. Including this information in the appendices or the main body of the paper would be helpful.

Based on these considerations and aligning with the “Guidelines” for the NeurIPS conference, my recommendation is:


The recommended improvements should be made to enhance the likelihood of acceptance, though no fatal flaws exist.",0.5
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We completely comply with the NeurIPS Code of Ethics,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). . ","The proposed author's answer and justification to the NeurIPS checklist question on ethics compliance appear to lack depth and detailed consideration of the NeurIPS Code of Ethics. The response ""We completely comply with the NeurIPS Code of Ethics"" is overly simplistic, considering the complex nature of ethical issues inherent in AI and machine learning research. Below are itemized, actionable feedback points aimed at enhancing the paper's quality concerning the ethical considerations section:

1. **Ethical Considerations Section**: Expand the ethics considerations section within the paper to include detailed explanations of how the research specifically adheres to the NeurIPS Code of Ethics. This should cover aspects such as the societal impact of the research, data privacy concerns, fairness and bias in the dataset, and more.

2. **Bias and Fairness**: Given the variety of datasets included in Meta-Album, explain any measures taken to ensure that biases (e.g., cultural, gender, geographical) in these datasets are identified and mitigated. Also, discuss the steps taken or planned to ensure the fairness of algorithms trained on the Meta-Album datasets.

3. **Data Privacy and Consent**: Detail the source of each dataset included in Meta-Album, making sure that any personal or sensitive information contained in the datasets is collected and used with proper consent and in accordance with applicable privacy laws and regulations.

4. **Robustness and Safety**: Discuss any efforts or designs in your research to ensure the robustness and safety of algorithms trained on Meta-Album, especially in real-world applications. Mention if there are any limitations or considerations that future researchers should be aware of when utilizing the dataset or applying the findings.

5. **Environment and Sustainability**: Given the mention of datasets from diverse domains such as ecology, manufacturing, etc., provide considerations on environmental sustainability issues. If applicable, discuss the environmental impact of the computational processes involved in your research.

6. **Usage of Meta-Album**: Since Meta-Album is designed to facilitate other research, provide guidelines on ethical usage of the dataset. Encourage researchers to consider the ethical implications of their work when utilizing Meta-Album for training models, especially regarding the potential societal impact.

7. **Continual Learning and Extensibility**: Reflect on the ethical implications of continuously expanding Meta-Album with new datasets and domains. Discuss how future contributions will be vetted for ethical compliance, especially contributions from less-regulated sources.

---

Improvement in these areas will not only demonstrate a thorough understanding and commitment to ethical research practices but will also align the paper more closely with the values and expectations set forth by the NeurIPS Code of Ethics. 

**",0.5
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.","• The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). . ","**Feedback:**

1. **Broaden Discussion on Potential Negative Societal Impacts:** The authors claim no negative societal impacts; however, most technology, including datasets, can be used in unintended negative ways. It's crucial to acknowledge these possibilities even if they seem remote or require malicious intent to materialize. For instance, technologies facilitating few-shot learning could inadvertently advance systems with dual-use applications, including surveillance or automated content generation for misinformation campaigns. Recognizing and discussing such risks, even briefly, would align better with the guidelines.

2. **Mitigation Strategies for Negative Impacts:** Although the authors assert no negative societal impacts, discussing potential mitigation strategies for any envisaged risks would be beneficial. This could include mechanisms for monitoring use, restrictions on access, ethical guidelines for use, and engagement with ethical researchers to foresee and counteract malicious uses. Providing this discussion aligns with the best practices and guidelines provided by the conference, emphasizing responsible AI research and development.

3. **Expand on ""Recommended Use"" Section:** While the paper mentions ""recommended use"" in Section 1.3, it primarily focuses on what Meta-Album is designed for, missing an opportunity to discuss guidelines or considerations for ethical use explicitly. Including explicit guidance on ethical considerations, consent where applicable, and privacy concerns (even if not directly relevant) would make the usage recommendations more comprehensive.

4. **Justification for NA on Societal Impacts Needs Revision:** The answer of ""No negative societal impacts"" appears to overlook the guidance that, even for foundational research, potential pathways to negative applications should be considered. A revised answer might acknowledge the challenge in foreseeing all future applications but note current measures to mitigate foreseeable risks.

5. **Engage with Existing Literature on Societal Impacts of Similar Technologies:** The paper could benefit from engaging with existing literature or documented cases where similar technologies had unintended societal impacts. This engagement could provide a more nuanced view of potential risks and the importance of proactive mitigation strategies.

6. **Clarify Data Collection and Use Concerns:** While the data described mainly involves publicly available datasets, clarifying any privacy, consent, or ethical considerations around data collection and use (even if already addressed by the datasets included) would strengthen the paper's alignment with responsible research practices.

**Conclusion:**
The paper sidesteps a thoughtful engagement with potential negative societal impacts and mitigation strategies for the technology it presents. While the foundational nature of the research might make direct impacts less apparent, adhering to the guidelines to at least discuss these aspects is crucial for responsible AI research. Addressing these points would not only align with NeurIPS expectations but also contribute to broader conversations on ethical AI development.

**",0.5
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Yes,"Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.","• The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. . ","Despite the detailed response provided, several issues must be addressed to ensure the paper aligns with the NeurIPS checklist requirements regarding the responsible release of models and datasets. The following feedback aims to help improve the paper quality:

1. **License and Terms of Use Specifics**: The response mentions the release of datasets with proper license and terms of use but does not provide specifics about the type of licenses or how they effectively mitigate risks of misuse. For datasets and models with potential for misuse, a clear description of the license type (e.g., Creative Commons, MIT, proprietary with restrictions) and a summary of terms that specifically address misuse scenarios need to be included.

2. **Data Source Transparency**: While the paper mentions datasets are ""verified with licences"" and come from ""diverse domains,"" it lacks detail on the sourcing process for these datasets. Clarify whether datasets obtained from potentially sensitive or private sources (e.g., personal data scraped from social media) undergo an ethical review. Additionally, specify any measures taken to ensure data privacy and compliance with data protection regulations (e.g., GDPR).

3. **Safeguarding Against Misuse**: The response briefly mentions recommended usage without detailing what those recommendations entail or how they are enforced. Include a section on the ethical implications of dataset misuse and detail any technical or administrative safeguards, such as watermarking data to trace its origin or requiring users to sign a use agreement emphasizing ethical use.

4. **Handling of Biased or Unsafe Content**: For datasets that have been scraped or otherwise automatically collected, provide an analysis of potential biases or the presence of unsafe/harmful content. Describe any filtering or moderation processes used to ensure the dataset does not propagate stereotypes or contain offensive material.

5. **Accessibility Controls**: If applicable, describe how access to the datasets is controlled to prevent misuse. For example, is there an approval process for researchers or developers wishing to access the data? Are there any restrictions on the use of the dataset for certain types of research or development (e.g., military applications)?

6. **Community Engagement and Feedback**: Mention any mechanisms in place for the community to report issues with the dataset, such as biases, incorrect labels, or misuse cases. Describe how such feedback is addressed to continuously improve the dataset's ethical and responsible use.

7. **Impact Statement**: Expand the discussion on the potential impacts, both positive and negative, of releasing the dataset. Consider scenarios where the dataset could be misused and the societal or individual harms that could result. Also, provide guidelines on how to mitigate these risks by responsible use of the dataset.

8. **Consent for Data Usage**: For datasets involving human subjects (directly or indirectly), clarify how consent was obtained, or why it was not necessary. If the dataset includes or could include personal or sensitive information, elaborate on the consent process and measures to protect individuals' privacy.

Addressing these points will significantly strengthen the paper's alignment with ethical research practices and responsible data handling, ultimately enhancing its contribution to the field.

",0.5
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets","• The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. . ","The proposed authors' **""Answer""** mentions compliance with crediting the creators or original owners of assets used in their paper and respecting licensing terms. The **""Justification""** also mentions providing license information for original datasets and the datasets they are releasing, aligning with the **""Guidelines""** requiring clear license, copyright information, and original source citation.

However, to provide concrete feedback and identify potential areas of improvement, a thorough comparison with the actual paper content and guidelines requirements is essential. Based on the provided details, here are key considerations and actionable feedback for enhancing compliance and clarity regarding asset crediting and license respect:

1. **Comprehensive Citation of Original Papers:** Ensure that for every dataset or code package utilized, the original paper producing it is explicitly cited in the main text where these datasets or codes are mentioned, not just in an appendix or a supplementary material file. 

2. **Clear Versioning and URL Inclusion:** It's crucial to specify the version of the dataset or code package used, as different versions might have different licensing terms or content. Including direct URLs to the specific versions used can aid in transparency and reproducibility.

3. **Explicit License Details in the Main Text:** While Appendix B is mentioned for license details, considering the inclusion of a brief statement or table in the main body of the paper summarizing the licenses of external assets ensures immediate clarity for readers and reviewers without needing to navigate to the appendix.

4. **Addressing Copied or Derived Work Properly:** If any data or code is not original but copied or derived from another work, it should be clearly stated, along with the modifications (if any) made to it. This clarity helps in maintaining academic integrity and respects the original creators’ copyrights.

5. **Provision for Scraped Data:** If any dataset has been scraped from websites (not the case mentioned here but generally advisable if applicable), ensure providing detailed information on the compliance with those websites’ terms of service, copyright, and any scraping guidelines followed.

6. **License Information for Release Packages:** For any datasets or code packages being released as part of the paper, ensure to include a LICENSE file within the package, stating the copyright details, license terms, and permissions for use, modification, and distribution.

7. **Acknowledgments Section:** Besides the licenses, an acknowledgment section crediting the creators of the utilized assets (both data and code) can enhance the courtesy and professional respect towards the original works.


The recommended improvements target enhancing transparency, reproducibility, and compliance with licensing terms. Although no fatal flaws exist in the provided information, attending to these recommendations can significantly improve the paper's adherence to best practices for using and crediting external assets.",0.5
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information,"• The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. . ","Based on the information provided and in line with the guidelines of the NeurIPS conference for preparing a submission, the proposed author's answer and justification towards the documentation and provision of assets alongside their paper seem satisfactory. However, to ensure a comprehensive review and actionable feedback according to the guidelines detailed in the hypothetical scenario - particularly on aspects such as dataset documentation, anonymization of assets, and consent for usage, a few significant improvements are proposed below:

1. **Extend Documentation for Assets**: Ensure that each asset (dataset/code/model) is accompanied by extensive documentation in structured templates as mandated. This includes not just metadata and datasheets but also a detailed description of the content, source, collection process, licensing information, accessibility, and any preprocessing steps undertaken.

2. **Clarify Consent Process**: The paper must explicitly state how consent was obtained for any data derived from human subjects or that could potentially be traced back to individuals. This is not clearly mentioned in the author's justification and must be addressed to adhere to ethical standards.

3. **Anonymization of Assets**: The paper mentions a website for accessing datasets and related information. It is critical to ensure that at submission time, any asset that is made available is properly anonymized. This includes anonymizing URLs or ensuring that the datasets provided through the website do not contain identifiable information. The current justification lacks details on whether this guideline has been strictly followed beyond stating a website for distribution.

4. **Detailed License Information**: There must be unequivocal clarity on the licensing under which the datasets are released. Simply indicating verified licenses may not be sufficient. It’s important to specify whether the datasets can be freely used, shared, and modified. Moreover, it should be reviewed if the licenses of the source datasets permit the creation of derived datasets like Meta-Album without restrictions.

5. **Limitations and Future Work**: Documentation should also critically discuss any limitations of the new assets introduced, how they compare with existing datasets, and potential bias or imbalances. Although the paper's abstract mentions verified licenses and preprocessing steps, a more detailed discussion in the body or appendices would enhance transparency.

6. **Accessibility and Long-term Availability**: While a website for distribution is a good start, considerations for long-term accessibility should be addressed. Including plans for dataset versioning, maintenance, potential downtimes, and contact information for queries can significantly improve the usability of the assets introduced.

**Conclusion**: The paper introduces valuable assets to the research community, as described in the abstract and introduction. However, to fully align with NeurIPS' guidelines on new assets, enhancements in documentation, consent clarification, anonymization, licensing details, limitation discourse, and accessibility plans are recommended. Ensuring these improvements would not only strengthen the submission but also ensure it meets ethical, legal, and community standards.

**",0.5
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This does not apply on our research as we are not doing any crowdsourcing experiments,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. . ","The proposed author's answer to the checklist question regarding the inclusion of the full text of instructions given to participants, screenshots (if applicable), and details about compensation is marked as ""N/A"" with the justification that their research does not involve any crowdsourcing experiments. 

Based on the ""Guidelines"" provided and the content available in the ""Paper,"" several key improvements are recommended for ensuring compliance and clarity:

1. **Clarification on Human Subjects:** If the paper involves the use of human-generated data, even if not directly crowdsourced, it should detail the source of this data, instructions provided for data generation or labeling, and any compensation details. This applies even if the data were previously collected or obtained through third-party datasets.

2. **Dataset Source and Collection Method:** Clarify within the paper whether any part of the dataset compilation involved human subjects indirectly. If it includes datasets that may have been crowdsourced (like many publicly available datasets), this should be explicitly mentioned with appropriate references to the original data collection protocols and compensations.

3. **Ethical Review and Consent Process:** If human subjects were involved at any stage of the dataset compilation or experiments indirectly, mention any ethical review the study underwent. Also, describe the consent process for participants whose data was used, even if the paper's authors did not directly collect the data.

4. **Data Anonymity and Privacy:** Clarify the measures taken to ensure the privacy and anonymity of individuals whose data might be part of the used datasets. This is crucial for research integrity and respecting participants' privacy rights.

5. **Supplemental Material:** If applicable, consider adding supplemental material that includes more detailed information about dataset collection, instructions given to participants for any crowdsourced component, and compensation details. This might not be directly relevant if the research did not directly involve crowdsourcing, but if human-labeled or generated datasets were used, this background is crucial.

6. **Explicit NA Justification:** Enhance the justification for marking the checklist question as ""N/A"" by detailing the assessment process that led to this conclusion. This includes a brief explanation of why the research or datasets used do not fall under the categories requiring detailed human subject research information.


This score reflects the necessity for the authors to provide more detailed justification regarding their N/A response to the checklist question, especially in the context of using datasets that might indirectly involve human subjects or crowdsourcing efforts. While there might not be any direct crowdsourcing involved, clarity on these aspects is crucial for transparency and ethical research practices.",0.5
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This does not concerns us as we are not dealing with human subjects in our research.,"• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.","The provided response attempts to answer a checklist question regarding the ethically sensitive aspects of research, specifically focusing on the involvement of human subjects. Given the complexity and breadth of the provided materials, encompassing a detailed documentation of the Meta-Album project—a multimodal meta-dataset aimed at facilitating few-shot learning and similar tasks—the response appears to suffer from a scope mismatch relative to the question asked about research ethics and participant safety. Below are the recommended improvements based on the NeurIPS guidelines and the specific content of the provided material:
 
1. **Clarify Participant Involvement**: Although the response initially labels as ""N/A"" under the pretense of not involving human subjects, the extensive documentation of datasets, particularly those related to ""Human Actions,"" suggests potential indirect involvement of individuals via the dataset images. Actionable feedback includes urging the authors to:
   - Explicitly define the scope of ""human subject involvement"" as per their understanding and the guidelines of their institutional review board (IRB) or equivalent ethical review authority.
   - Document any steps taken to anonymize or secure consent for identifiable human images in datasets, directly addressing the NeurIPS expectation of elucidating potential risks to subjects.

2. **Explicitly Address IRB Review Necessity**: Given the international collaboration on the Meta-Album project and the diverse source datasets included, it's recommended to:
   - Provide a statement regarding the ethical review status of the contributing datasets, particularly those with human images, acknowledging any IRB or equivalent ethical review processes they underwent.
   - If possible, include a general statement on the adherence to ethical guidelines for research involving human subjects, even if the primary research does not directly engage participants.

3. **Inclusion of Dataset Licensing and Consent Information**: Although the detailed dataset descriptions include licensing information, there's a lack of clarity on whether these licenses cover the use of human images and if consent was obtained for these uses, particularly for datasets like ""Human Actions."" The authors should:
   - Include a brief statement on the compliance of dataset licenses with ethical guidelines for the use of human subjects’ images.
   - Highlight any steps taken to ensure the datasets included in the Meta-Album project did not violate the consent or privacy rights of individuals depicted in the dataset images.

4. **Dataset Preprocessing and Ethical Considerations**: The comprehensive documentation on dataset preprocessing steps lacks a connection to ethical considerations, especially regarding potential de-identification of human subjects. Recommending:
   - A short section discussing how preprocessing decisions were informed by ethical guidelines to prevent privacy violations or the unintentional inclusion of sensitive human data.

5. **Broaden the Concept of 'Risk' to Include Data Misuse**: The broad application potential of the Meta-Album project necessitates a discussion on the misuse of data, particularly concerning datasets involving human subjects. It's advisable to:
   - Discuss potential risks associated with data misuse, emphasizing the protective measures or recommendations for ethical use provided to Meta-Album users.

Conclusion: The authors should refine their submission to directly address the checklist question on ethical considerations, especially focusing on IRB approvals, human subjects' involvement, and dataset consent information. These adjustments would not only elevate the paper's alignment with NeurIPS guidelines but also enhance its ethical transparency.

",0.5
