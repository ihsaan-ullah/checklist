Question,Question_Title,Answer,Justification,Guidelines,Review,Score,LLM
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,Our main claims are listed in section 1 and highlight this paper's contributions.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the abstract and introduction do not include the claims made in the paper.
The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
Please only check the claims made in the abstract and introduction and verify that they summarize contributions and scope of the paper appropriately. Ignore the authors' justification for this question.
","The authors' proposed answer and justification seem generally aligned with the paper’s content regarding accurately reflecting the paper's contributions and scope in the abstract and introduction. The main claims related to addressing hyperparameter integration issues in Neural Architecture Search (NAS) and proposing a framework for simplifying the architecture search for Spatio-Temporal Graph Neural Networks (STGNNs) are supported by the paper’s content, specifically in sections 1 and 4, where these contributions are detailed. The abstract succinctly outlines the motivation, central problem, proposed approach, and key findings, aligning well with the detailed content in the subsequent sections.

However, to further improve the paper quality and ensure the completeness of compliance with the submission guidelines, consider the following feedback:

1. **Clarity and Specificity of Claims:** Ensure that the claims made in the abstract and introduction are as specific and clear as possible. While the justification mentions where to find related material, it could further benefit from explicitly stating how each claim is addressed within those sections. This adjustment would help readers quickly verify the claims’ validity and understand the paper's contributions more deeply.

2. **Highlight Limitations and Assumptions:** It appears the abstract and introduction predominantly focus on the contributions without equally highlighting any important assumptions or limitations of the proposed approach. Incorporating these elements can provide a balanced view and set realistic expectations for the readers. Referring to any considerations made regarding the computational complexity or the specific contexts in which the proposed method outperforms others would be valuable.

3. **Direct Reference to Experimental Validation:** Increase direct references to the experimental setup and results in the abstract and introduction that validate the main claims. Highlighting the key findings from the comparative analysis with state-of-the-art methods could strengthen the paper's impact and help readers grasp the significance of the contributions quickly.

4. **Consistency in Terminology:** Ensure consistency in the use of terminology throughout the abstract, introduction, and the rest of the paper. This includes terms related to the proposed solutions, methodologies, and frameworks. Consistency aids in avoiding confusion and contributes to a more cohesive reading experience.

5. **Expand on Contribution to the Field:** While the introduction outlines the contributions, adding a brief discussion on how these contributions advance the field or address a gap in existing research could offer additional value. This might involve comparing with baseline approaches or highlighting the novelty of the disentangled framework and hyperparameter understanding in the context of STGNNs.

Implementing these suggestions can enhance the paper's quality by providing clearer, more comprehensive insights into the paper’s contributions, assumptions, and impacts on the existing body of knowledge.

",0.5,GPT
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,"We have discussed the potential limitations in section 6, including the application domain and considered temporal modeling methods.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
The authors are encouraged to create a separate ""Limitations"" section in their paper.
The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
","Based on the authors' proposed answer and justification, as well as adherence to the provided guidelines, the discrepancies with the actual paper content need to be identified and addressed to improve the quality of the paper. Here are the itemized, actionable feedbacks:

1. **Specificity of Limitations**: While Section 6 is referenced for discussing limitations, the content primarily sums up the study's outcomes and future work directions. It lacks a detailed discussion on the limitations regarding model applicability in various domains, robustness against different types of data noise, or dependence on specific types of graph structures. Providing examples of scenarios where the model might underperform or specific conditions where the assumptions do not hold would enhance transparency.

2. **Assumptions and Robustness**: The guidelines suggest discussing the model's robustness to assumptions violation. The paper briefly mentions the modelling of spatio-temporal correlations and reliance on traffic prediction datasets, but it does not thoroughly analyze the implications if these conditions are unmet, such as non-traffic datasets or irregular data collection intervals. A more detailed exploration of the model's performance variability across different conditions or dataset characteristics would be valuable.

3. **Scope of Claims**: The paper ambitiously promotes the generated models' superior performance across tasks. However, it primarily focuses on traffic forecasting. An explicit section discussing the boundary conditions for these claims, including dataset characteristics and environmental factors, would prevent overgeneralization and clarify the conditions under which the model excels or struggles.

4. **Computational Efficiency**: The paper introduces a streamlined architecture search method but lacks a detailed discussion on computational efficiency, especially regarding scaling up to larger datasets or more complex graph structures. Commenting on the computational demands, potential bottlenecks, and scalability issues would align with the guidelines' suggestion to discuss algorithm efficiency and dataset size scaling.

5. **Privacy and Fairness**: Although not directly relevant to the study's core focus, acknowledging potential limitations around data privacy (e.g., when applied to personal mobility data) and fairness (e.g., equitable service predictions across different regions) considerations could be pertinent, especially if the technology is to be deployed in real-world scenarios.

6. **Error Bars and Reproducibility**: The paper discusses model performance and architecture search outcomes. Still, it does not clearly mention the reproducibility of results or the reporting of error bars for performance metrics, which are crucial for evaluating the model's consistency and the statistical significance of the reported improvements.

Actionable recommendations for improvement:
- Expand the limitations section to include specific scenarios where the model's performance may be compromised.
- Address the robustness of the model to assumption violations directly in the context of the presented model and dataset characteristics.
- Clarify the scope and applicability of the model's claims by outlining the conditions and dataset characteristics that were used to validate those claims.
- Include a subsection on computational efficiency and scalability that discusses the model's performance concerning computational resources and dataset sizes.
- Briefly touch upon considerations around data privacy and fairness, even if not directly applicable, to foster a comprehensive understanding of the model's deployment implications.
- Ensure reproducibility and statistical rigor by reporting error bars or confidence intervals for performance metrics.

",0.5,GPT
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,This paper does not include theoretical results.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include theoretical results. 
All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced. 
Theory shall mean that the paper contains theorems and requires proofs. If there are no theorems and the authors claim there is no theory, then consider everything is ok. 
","Given the provided content and guidelines, the proposed author's ""Answer"" (NA) and their ""Justification"" (the paper does not include theoretical results) appear to be consistent with the guidelines and the content of the paper provided. The paper focuses on understanding and simplifying architecture search in Spatio-Temporal Graph Neural Networks and does not claim to provide theoretical results such as theorems or proofs which would require a detailed analysis of assumptions and proofs. Instead, it revolves around empirical studies, framework proposals, and the development of a method for neural architecture search which is validated through experiments.

To further improve the paper quality, particularly in relation to the question posed and based on the guidelines provided, the following feedback can be beneficial:

1. **Clarification on Theoretical Contributions**: Even though the paper primarily focuses on empirical research, any underlying theoretical assumptions made during the design, analysis, or interpretation of the results should be explicitly stated. For example, assumptions regarding the data distribution, network behavior, or optimization properties could be more clearly described if applicable.

2. **Justification for NA Answer**: Reinforce the justification for the NA answer by explicitly stating in the paper sections (such as in the methodology or discussion) that the research does not involve theoretical claims necessitating formal proofs. This explicit statement helps to clarify to readers and reviewers the nature of the contributions and the scope of the paper.

3. **Supplemental Material**: Even in the absence of traditional theoretical results, it might be useful to include any supplemental analyses or background material that underpins the empirical work in an appendix or supplemental material. This could involve, for example, a more detailed discussion on the rationale behind certain design choices in the architecture or hyperparameter search space.

4. **Connect Empirical Findings to Theory**: Where possible, link empirical findings back to existing theories or principles. For instance, if the disentangled framework proposed leads to better understanding or simplification of the architecture search, discuss why this is the case from a theoretical standpoint, even if the paper's contribution is not in proving new theorems.

5. **References to Theoretical Works**: Clearly reference any theoretical works or frameworks that informed the development of the empirical methods used. This helps situate the paper within the broader scientific discourse and provides readers with resources for deeper theoretical insights.


This score is given because the authors' proposed answer and justification directly align with the content of the paper as it pertains to the guidelines provided. The feedback aims at further strengthening the paper by suggesting ways to make any underlying assumptions more explicit and to solidify the connection between the empirical work and theoretical considerations, even in a paper that does not directly present theoretical results.",1,GPT
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,"We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
    - If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
    - If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
    - If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
    - We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

","The proposed author's answer and justification seem to match well with the guidelines provided for the NeurIPS conference regarding reproducibility and disclosure of information necessary to reproduce the main experimental results. Specifically, the answer provided clearly states a ""Yes"", and the justification details the specific sections where the experiments, methodology, and other relevant information such as datasets, baseline comparisons, implementation details, and results are discussed. Further, the authors have provided a github link to their code, which is a strong addition in favor of reproducibility. However, to ensure the feedback is comprehensive and actionable, a closer examination of the paper content against the justification provided is necessary. The evaluation of the paper content reveals an exhaustive detailing across the sections specified and in the appendices, meeting the general expectations for reproducibility. Nonetheless, a few improvements could enhance clarity and accessibility.

1. **Clarify Dataset Access and Licensing**: While the paper mentions datasets and provides a Github link for the code, clarity on the datasets' licensing or any constraints around their use could further align with best practices in reproducibility. If any datasets have licensing restrictions, mentioning them and suggesting possible alternatives can improve accessibility for reproducing results.

2. **Detailed Hyperparameter Settings and Ranges**: The paper discusses hyperparameters in several sections but ensuring a consolidated and comprehensive listing of all hyperparameters used, including those not altered from defaults, can aid in better reproducibility. Considering the vast hyperparameter space, especially in neural architecture search and deep learning models, providing exact values and ranges tested can significantly reduce the effort required to replicate the study.

3. **Dependencies and Environment Specifications**: For the code provided in the Github repository, including a detailed requirements.txt file or equivalent to specify the versions of libraries and dependencies can prevent compatibility issues. Further, a Dockerfile or an environment.yml file for replicating the software environment could be beneficial.

4. **Additional Context on Codebase Usability**: Enhancing the documentation within the Github repository with more comprehensive instructions on how to run the code, including example commands for training, testing, and reproducing figures in the paper, will make it easier for others to follow the work. Comments within the code explaining complex or critical sections will also help in understanding the implementation better.

5. **Expand on Limitations and Assumptions**: While the paper discusses the proposed method in detail, expanding the discussion around limitations and assumptions made in the study could help future work better build upon or address these areas. This includes acknowledging any computational resources required that might not be widely accessible.

Acknowledging these enhancements could strengthen the paper's alignment with the reproducibility standards expected at NeurIPS and provide clearer pathways for others to follow and extend the work.

",0.5,GPT
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,"All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
","The authors' proposed answer and justification appear to adhere to the NeurIPS guidelines and are consistent with the content of the paper as provided. The key points to address in the feedback to improve the paper quality are as follows:

1. **Clear Access to Data and Code**: While the authors have mentioned that all data and code are open access, with the dataset information found in sections 4, 5, and Appendix B, and the code available on GitHub, it's vital to ensure the GitHub repository includes comprehensive instructions for reproducing the experiments. This includes the exact commands, dependencies, environment setup, and any data preprocessing steps required.

2. **Reproducibility and Instructions Detailing**: Particular attention should be paid to ensuring the GitHub repository contains a straightforward README file that guides readers through setting up their environment, accessing and preparing the dataset(s), and executing the code to reproduce the results. This also involves specifying the versions of any external libraries or frameworks used and ideally providing a requirements.txt file or an environment.yml file for easy setup.

3. **Dataset Licensing and Access**: The authors should explicitly state if there are any licensing issues with the datasets or if any datasets are proprietary or have restrictions on their use. If all datasets are indeed publicly accessible and have open licenses, this should be clearly mentioned either in the appendix or directly in the section that introduces the datasets.

4. **Supplemental Material Completeness**: Ensure the supplemental material not only includes code and data but also any additional instructions, configurations, or scripts needed to reproduce the experimental results fully. This could involve hyperparameter settings, training procedures, or architectural details not fully covered in the main body of the paper.

5. **Anonymity Preservation**: Since this is a submission phase review, verifying that the provided GitHub link does not inadvertently de-anonymize the authors is crucial. Ensure the GitHub repository adheres to the double-blind review process, either by being anonymized or set up in a way that does not reveal the authors' identities.

6. **Experimental Results Reproducibility**: Confirm that all main experimental results presented in the paper can be reproduced with the provided code and datasets. If only a subset of results can be reproduced due to computational or other practical constraints, this limitation should be clearly stated, explaining which results are reproducible and providing justification for any exceptions.

7. **Validation on Multiple Datasets**: While the paper mentions experiments conducted on multiple datasets, ensuring that instructions for reproducing these results across all datasets are included in both the paper and the supplemental material is essential for validating the generality of the proposed method.

By addressing these points, the authors can significantly enhance the reproducibility and integrity of their research submission, adhering to NeurIPS guidelines and contributing valuable resources to the research community.

",0.5,GPT
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental material.
","The authors' proposed answer and justification indicate that they have provided the necessary details for understanding their results, including hyperparameter sampling, training and test details, as mentioned in sections 4 and 5 and appendix B of the paper. To align with the NeurIPS guidelines, let's focus on improving the quality of the given answers and justifications.

### 1. Clarity and Specificity in Justification:
- **Ensure Specificity in Sections**: The justification mentions sections 4 and 5 and Appendix B but lacks specific subsections or paragraph information. It's beneficial to pinpoint exact locations (e.g., Section 4.2 for hyperparameter details) where these details are discussed for improved clarity and accessibility.
- **Detail Appendix Content**: If Appendix B contains substantial methodological details not found in the main paper, summarizing key points in the justification can help reviewers and readers grasp the depth of provided training/test details.

### 2. Comprehensive Hyperparameter Information:
- **Clear Hyperparameter Rationale**: If the paper selects certain hyperparameters or settings based on empirical observations or literature review, briefly mentioning this rationale in the justification strengthens the understanding of methodological choices.
- **Consideration of Reproducibility**: Indicating whether hyperparameter choices were made to balance performance and computational cost can be valuable, especially if some were selected to ensure reproducibility by others with different computational resources.

### 3. Description of Data Splits:
- **Explicit Data Split Ratios**: While the justification mentions training and test details, explicitly stating the ratio of data splits within the paper or appendix (e.g., 70% training, 15% validation, 15% test) improves transparency and reproducibility.
- **Handling of Dataset Specifics**: If the paper involves multiple datasets with differing characteristics, clarifying how data splits or preprocessing were adapted for each can prevent confusion.

### 4. Optimization and Evaluation:
- **Optimizer Details**: Providing specifics on the type of optimizer(s) used, along with justifications for their selection (e.g., Adam for faster convergence), can enhance the method's context.
- **Evaluation Metric Consistency**: Confirm that the evaluation metrics used across experiments are consistent and justified in their choice, enabling fair comparison of results.

### 5. Public Availability of Code and Data:
- Ensure that the paper mentions the availability of code and datasets or provides a reason if any of these cannot be shared. This is crucial for reproducibility and adherence to NeurIPS guidelines.

By addressing these points, the paper will provide a clearer, more detailed, and easily reproducible account of the experimental setup and hyperparameter choices, aligning with the NeurIPS expectations for transparency and reproducibility.

**",0.5,GPT
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error of the mean.
It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
","The author's proposed answer indicates that error bars or statistical significance information is reported for the experiments, specifically mentioning variance as error bars across sections 4 and 5. However, the provided paper excerpt does not contain specific evidence or detailed information that directly confirms or contradicts the claim within the constraints of the summary provided. Without access to the full text of Sections 4 and 5, and based solely on the summary, we cannot fully evaluate the accuracy of the author's claim regarding the reporting of error bars or statistical significance.

**Actionable feedback based on the given guidelines to improve paper quality:**

1. **Explicitly Mention Error Bar Details**: Ensure that the sections discussing the experiments (Sections 4 and 5) explicitly mention the type of error bars used (e.g., standard deviation, standard error), how they are calculated, and what variability they represent. This is vital for the reader's understanding of the statistical significance and repeatability of the results.

2. **Clarify Assumptions and Error Bar Calculations**: According to the guidelines, if error bars are based on assumptions such as normal distribution of errors, this should be clearly stated along with the rationale. Include a brief description or reference to the method of calculation for the error bars, whether it's a closed-form formula, a library function, or a bootstrap method.

3. **Reference Figures or Tables for Error Bars**: If error bars are visualized in figures or reported in tables, ensure these are explicitly referenced in the text where the experimental results are discussed. This makes it easier for readers to find and understand the data supporting the paper's claims.

4. **Discuss Factors of Variability**: Clearly state the factors of variability accounted for by the error bars. This could include variations due to different initializations, dataset splits, or model configurations. This information is crucial for assessing the robustness of the findings.

5. **Consider Reporting Confidence Intervals**: If applicable and especially for asymmetric distributions, consider reporting confidence intervals in addition to or instead of standard error bars. This could provide a more accurate picture of the data's variability and the results' significance.

6. **Statistical Tests**: If possible and relevant, include statistical tests to underscore the significance of the results. This could strengthen the paper's claims about the efficacy of the proposed methods.

By addressing these points, the authors could enhance the transparency and robustness of their experimental results reporting, thereby increasing the paper's contribution to the field.

",0.5,GPT
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
","Discrepancies Identified:
1. The paper mentions providing GPU hours in Section 5. However, the required comprehensive details on computer resources such as ""type of compute workers (CPU or GPU, internal cluster, or cloud provider)"", ""relevant memory and storage"", or ""amount of compute required for each experimental run"" are not explicitly detailed.
2. The justification does not mention the disclosure of whether more compute than reported was used for the experiments comprising the research project, which includes preliminary or failed experiments not making it into the final paper.

Actionable Feedback:
1. **Specify Compute Worker Details**: Clarify the type of compute workers used for experiments (e.g., CPU type, GPU model). If a mix of resources was utilized, detailing each setup separately is recommended.
2. **Memory and Storage Explanation**: Provide information on the memory and storage capacities utilized for the experiments. If varied capacities were used across experiments, consider tabulating these differences.
3. **Detailed Compute Requirements**: Break down the individual compute requirements for each experimental run. This includes not just the GPU hours already reported but also should encompass CPU hours (if applicable), the count of runs, and variance in resource usage by experiment type.
4. **Broaden Compute Usage Disclosure**: Extend the discussion to include any additional compute resources consumed outside the scope of reported experiments, such as for pre-processing, model development phases, or discarded experiments. This will add transparency regarding the computational cost of your research.
5. **Direct Referencing and Structuring**: Ensure that each required piece of information regarding compute resources is easy to find in the paper. Consider creating a separate subsection within Section 5 or an appendix dedicated to these details if they are extensive.

Improving these aspects will significantly contribute to the reproducibility and transparency of your research, enabling others to better understand and potentially replicate your work, an essential consideration for the NeurIPS conference.

",0.5,GPT
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We review the code of ethics and our paper conform with it.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
","The answer provided by the authors does not meet the NeurIPS guidelines for several reasons. The justification given is overly simplistic and lacks specific references to the paper's content that align with the NeurIPS Code of Ethics. According to the guidelines, if the authors answer ""Yes,"" they should point to specific sections where the ethical considerations are discussed or addressed, which the authors failed to do.

Feedback:

1. Reference Specific Sections: The authors should explicitly reference sections in the paper where ethical considerations, data usage permissions, potential biases in data or models, and impacts of the research are discussed in relation to the NeurIPS Code of Ethics.

2. Discuss Ethical Considerations: The paper should include a discussion on the ethical implications of their research. This includes, but is not limited to, data privacy concerns, potential misuse of the technology, and how biases in data are addressed.

3. Address Data Usage and Permissions: Clarify the permissions for the use of datasets, especially if the data involves personally identifiable information or potentially sensitive attributes.

4. Consider Impact on Society: Reflect on how the research findings might impact society, both positively and potentially negatively. Discuss measures taken to ensure the research contributes positively to society.

5. Potential Algorithmic Biases: If applicable, discuss any steps taken to identify and mitigate potential biases in the models or algorithms developed as part of the research.

6. Include a Broader Impact Statement: If not already included, a broader impact statement should be added to the paper, detailing the potential societal impact of the research findings, including both benefits and drawbacks.

The authors are advised to update their justification to include specific references to sections of the paper that address these points, ensuring compliance with the NeurIPS Code of Ethics.

",0,GPT
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"We discuss the broader impacts in the section ""Broader Impact Statement"".","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
","Feedback:

1. Discuss Both Impacts: Ensure the ""Broader Impact Statement"" section truly discusses both the potential positive and negative societal impacts of the work performed. The current justification lacks specificity regarding the nature of both types of impacts. Consider including explicit examples of potential beneficial applications of STGNNs in traffic prediction and urban planning, as well as potential risks such as privacy concerns, surveillance, or misuse of the technology in ways that could lead to unfair or biased outcomes.

2. Expand on Mitigation Strategies: If the paper identifies negative societal impacts, it should also discuss possible mitigation strategies to address these concerns. This could include recommendations for ethical guidelines, technical safeguards, or policy measures that could help prevent misuse or mitigate negative effects. These strategies should be specific to the identified risks and practically feasible within the context of the work.

3. Reference to Relevant Sections: The justification mentions a ""Broader Impact Statement"" section, but it should be clearer about where in the paper these discussions occur. Ensure that the discussions on positive and negative impacts are easily identifiable and that they provide substantial, thoughtful analysis. If this section is not adequately detailed, consider expanding it to more thoroughly examine the societal implications of the work.

4. Consider Broader Application Contexts: While the paper focuses on traffic prediction, the technology could have broader applications in other domains. Discuss how the findings might generalize to other uses of STGNNs, and what societal impacts might be relevant in those contexts. This will help illustrate the potential scope of the technology's impact.

5. Address Guidelines Comprehensively: Ensure the discussion aligns with the guidelines provided, which emphasize the importance of considering both intended use cases and potential for misuse. The paper should reflect a balanced view that recognizes the complexity of technological impacts on society.

6. Improve Transparency: If there are limitations in the exploration of societal impacts due to the paper's scope or the authors' expertise, these should be acknowledged transparently. Offering avenues for future research or collaboration with social scientists and ethicists could be beneficial.


This score reflects that while the paper has acknowledged the importance of discussing societal impacts, there is room for improvement in providing a more detailed, balanced, and actionable analysis in line with the guidelines.",0.5,GPT
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,This paper poses no such risks.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").
""
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
","The provided author answer to the NeurIPS checklist question about responsible release of data or models potentially at high risk for misuse is ""NA"" with the justification being that the paper poses no such risks. Given the content of the paper's abstract and sections provided, the authors are primarily contributing to the academic exploration of neural architecture search (NAS) techniques within the context of Spatio-Temporal Graph Neural Networks (STGNNs), particularly for traffic prediction applications. There is no explicit mention of releasing any pretrained models or data that could present direct misuse risks. However, considering the guidelines, even if a paper does not directly release data or models, discussing potential misuse scenarios or implications could still be beneficial, especially when dealing with data-driven models in potentially sensitive applications such as traffic prediction.

**Feedback for Improvement**:

1. **Clarify Potential Risks**: Even if the paper's contributions do not directly include releasing models or datasets, briefly discussing any potential risks associated with the deployment of proposed methods could enhance the responsible dissemination of research. For instance, biases in traffic data, potential for surveillance, or misuse in predictive policing, if relevant.

2. **Model Deployment Considerations**: Acknowledge the broader implications of deploying NAS-optimized STGNNs in real-world settings. This includes any ethical considerations or potential unintended consequences, ensuring the paper responsibly flags any broader societal or ethical implications.

3. **Data Privacy and Security**: If the methods proposed could potentially be applied to datasets that contain sensitive information, a brief discussion on data privacy, security, and compliance with relevant regulations (e.g., GDPR) could be added. This is important even if the paper itself does not release any datasets.

4. **Future Work Directions**: Suggesting future research directions that consider the responsible and ethical application of the proposed architecture search and design methodology, such as developing mechanisms to audit or mitigate biases in spatio-temporal predictions, can be a valuable addition.

5. **Community Engagement**: Encourage or provide guidelines for the community to responsibly use the proposed methods, including considerations for licensing of any resultant models, suggested ethical use cases, and discouraging misuse.

Given the lack of mention of potential risks or misuse and the focus on advancing NAS in STGNN for traffic prediction, it appears the ""NA"" response is technically accurate but could be expanded with a bit more nuance on the broader implications of deploying such technologies.

**",0.5,GPT
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
If this information is not available online, the authors are encouraged to reach out to the asset's creators.
","The authors claim that they have properly credited the creators or original owners of assets used in the paper and have mentioned the license and terms of use, citing the sources in references [13, 1, 10]. To provide actionable feedback based on the provided guidelines and the content of the paper, the following improvements are necessary:

1. **Explicit License Mention**: While the authors have cited the sources of their datasets, there's no explicit mention of the licenses for these datasets within the given text. As per the guidelines, the name of the license (e.g., CC-BY 4.0) should be included for each asset used. The authors should update the paper to include a specific section or a note in the dataset citation that clearly states the license under which each dataset is released. 

2. **Terms of Use**: The authors have not mentioned the terms of use for the datasets. They need to explicitly state these terms to ensure that they respected them in their research. The paper should be updated to include a brief description of the terms of use for each dataset, especially focusing on any restrictions or requirements imposed by the licenses.

3. **Version of the Asset Used**: The authors have provided a URL for accessing the datasets but did not mention which version of the asset was used. For the sake of reproducibility and clarity, the authors should mention the specific version of the dataset they used in their experiments.

4. **Details on Code Assets**: If any code from external sources was used, the same level of detail (source citation, license, terms of use, and version) should be provided for these assets. It's unclear from the answer if other assets like code libraries or pre-trained models were used. If so, the authors should update the manuscript to credit these resources properly.

5. **Dataset License Availability**: Authors are encouraged to verify if the license information is available online. If not, reaching out to the dataset creators for clarification would be advisable. This step ensures compliance and respect for intellectual property rights.

6. **Link Correction and Accessibility**: Ensure that the provided GitHub link is accurate, functional, and links directly to the dataset and code repository with clear labeling of the datasets used. If the datasets are hosted within a larger repository, specifying the exact path would greatly assist readers and future researchers.

By addressing these points, the paper will meet the requirements for properly crediting the assets used in the research and ensure adherence to licensing and terms of use, which upholds academic integrity and respects the rights of the original asset creators.

",0.5,GPT
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Not Applicable,The paper does not release new assets.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
The paper should discuss whether and how consent was obtained from people whose asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
","The provided author's answer states ""NA"" with the justification that the paper does not release new assets. However, within the paper content, there is a direct contradiction to this answer. Specifically, in the section detailing the main contributions and in various parts throughout the paper, the authors mention, ""Our code is available at https://github.com/AutoML-Research/SimpleSTG,"" indicating that new assets, namely code, have indeed been released as part of the paper's submission. This contradicts the authors' proposed answer to the checklist question regarding the documentation of new assets.

### Actionable Feedback:

1. **Correct the Answer to the Checklist Question**: The authors should change their answer from ""NA"" to ""Yes"" to accurately reflect that new assets (code) are introduced and shared as part of the paper.

2. **Provide Details in Justification**: In the justification section, authors need to detail where the assets (code and/or models) are documented. This should include:
    - A direct mention of the repository URL within the paper where the code is available.
    - Information on how the code is organized, any dependencies required to run the code, and basic usage instructions.
    - An explicit section in the paper or an appendix that outlines the structure of the repository, if not directly included within the main sections related to methodologies and experiments.

3. **Ensure Comprehensive Documentation of the Released Code**: Verify that the GitHub repository (or other hosting service) includes:
    - A README file with a comprehensive guide to installing, configuring, and running the code.
    - Documentation on the dataset(s) used, including how to access them, preprocess them, and use them with the provided code.
    - Licensing information that clearly states how the code and any potential datasets are licensed to ensure users understand their rights for using and modifying the assets.

4. **Discuss Limitations and Dependencies**: In the paper, briefly discuss any limitations or dependencies of the provided code. For example, if the code requires specific hardware capabilities (e.g., certain GPU requirements), this should be clearly stated.

5. **Include a Statement on Ethical Considerations and Permissions**: If the released code uses any datasets or pre-trained models, the paper should discuss whether and how consent was obtained for their use, alongside any ethical considerations addressed during the research.

6. **Anonymity During Review Process**: Since the NeurIPS submission and review process is double-blind, ensure that the GitHub repository or any externally linked assets do not reveal the authors’ identities. Consider using an anonymized link or a placeholder URL if necessary during the review phase, with a plan to update it upon acceptance.

By addressing these actionable items, the paper will not only comply with the conference submission guidelines concerning the release of new assets but also enhance the reproducibility and transparency of the research being presented.

",0,GPT
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
","The proposed author's answer and justification correctly align with the actual paper content, which focuses on understanding and simplifying architecture search in Spatio-Temporal Graph Neural Networks. The paper does not involve crowdsourcing experiments or research with human subjects; thus, it correctly indicates the NA (Not Applicable) response for the NeurIPS conference checklist question about including full text instructions, screenshots, and details about compensation for human subjects. 

Feedback for improvement:

1. Ensure Comprehensive Documentation: Although the current question is NA for your paper, ensure that any future modifications or extensions of the work that may involve human subjects or crowdsourcing experiments are documented comprehensively. This includes detailed instructions, compensation details, and ethical considerations.

2. Consider Broadening Research Methodologies: If future work could involve human-centric studies or crowdsourcing for data annotation or model evaluation, plan to include such sections well in advance. Establish protocols for ethical compliance, informed consent, and data privacy in accordance with the NeurIPS guidelines.

3. Ethical Considerations: Independently of the current NA response, always assess and discuss the broader ethical impact of your research in the paper, especially concerning data privacy, model bias, and potential misuse of research findings.

4. Replicability and Open Science: Enhance the accessibility of your research by providing comprehensive supplementary material, which might include, but not limited to, code repositories, data access links (ensuring they comply with privacy norms), and detailed hyperparameter settings. Even when experiments do not involve humans, comprehensive documentation strengthens the scientific value of the work.

5. Future Work Section: Elaborate on potential applications of your findings in areas that might involve human interaction or crowdsourcing, such as interactive machine learning, and detail how you would approach the ethical and logistical complexities therein.

Conclusion:

The paper appropriately justifies the NA response to the NeurIPS conference checklist question concerning human subjects and crowdsourcing. Future enhancements can focus on broader documentation and ethical considerations relevant to the research domain. 

",1,GPT
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
","The proposed answer and justification from the authors states that the question regarding potential risks, disclosures, and IRB approvals is not applicable (NA) because their study doesn't involve crowdsourcing or research with human subjects. Based on the complete text provided in the “Paper” section, this response is consistent with the content described. The paper focuses on the development and evaluation of Spatio-Temporal Graph Neural Networks (STGNNs) for traffic prediction, utilizing data-driven techniques without the involvement of human subjects in the data collection or analysis processes. Therefore, the nature of this research does not inherently require IRB approval, nor does it necessitate the disclosure of potential risks to study participants because it relies on traffic data rather than human subject research.

Feedback for Improvement:

1. **Clarify Data Usage and Sources**: Enhance transparency by detailing the sources of traffic data, ensuring that readers understand the data's origin and confirm there's no indirect involvement of human subjects or privacy concerns. This can be added in a subsection within the methodology or data description.

2. **Address Indirect Impacts**: Furthermore, it might be insightful to discuss any indirect implications or potential risks associated with the application of the system developed, such as privacy concerns related to location data, even if it's not personally identifiable. This could be in the form of a brief ethical considerations section.

3. **Data Licensing and Permissions**: Although there are no human subjects directly involved, verifying and stating the licensing or permissions for using the traffic data would add to the paper's ethical and legal rigor. This is especially relevant if the data comes from third-party sources.

4. **Future Work on Ethical Implications**: A suggestion for future research directions could include exploring any ethical implications of deploying such predictive technologies in real-world scenarios, considering societal, privacy, and security aspects. Adding a few sentences in the conclusion or future work section would suffice.

5. **Broader Impact Statement**: Though the paper does not directly involve human subjects, including a broader impact statement that covers potential positive and negative implications of the research can enrich understanding of the societal relevance of the work being presented.

Concluding these improvements would not alter the fundamental nature of the paper's contribution to the field but would strengthen its ethical stance and potentially widen its appeal to a broader readership concerned with the societal impact of AI and machine learning technologies.

**",1,GPT
