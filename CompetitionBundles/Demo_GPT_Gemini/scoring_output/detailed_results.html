<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }
    .container {
        margin: 20px auto;
        padding: 0 20px;
        position: relative;
    }
    .button {
        padding: 10px;
        font-size: 16px;
        text-align: center;
        background-color: #f93361;
        color: #fff;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        text-decoration: none;
    }
    .button:hover {
        background-color: #bc0530;
    }
    .content {
        padding-top: 60px;
        padding-bottom: 40px;
        margin-bottom: 20px;
    }
    h1 {
        margin-top: 0;
        margin-bottom: 30px;
    }
    hr {
        margin-top: 50px;
        margin-bottom: 50px;
    }
    .review {
        margin-bottom: 30px;
        border: 1px solid #ccc;
        padding: 20px;
        border-radius: 5px;
        background-color: #f9f9f9;
    }
    .review h2 {
        margin-top: 0;
    }
    .review p {
        margin: 10px 0;
    }
    .question {
        /* color: #0033ff; */
        color: #000;
    }
    .answer {
        /* color: #28a745; */
        color: #000;
    }
    .justification {
        /* color: #de750b; */
        color: #000;
    }
    .user_input {
        padding: 20px;
        border-radius: 5px;
        background-color: #fff;
        border: 1px solid #3a3a3a;
    }
    .llm_review {
        color: #000;
        padding: 20px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .llm_review-red {
        background-color: #eacfcf;
        border: 1px solid #FF0000;
    }
    .llm_review-green {
        background-color: #c6e9c6;
        border: 1px solid #008000;

    }
    .llm_review-orange {
        background-color: #ebdecf;
        border: 1px solid #FF8C00;
    }
    table {
        border-collapse: collapse;
    }
    th, td {
        padding: 8px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }
    th {
        background-color: #f2f2f2;
    }
    .score-label {
        display: inline-block;
        padding: 5px 15px;
        border-radius: 5px;
        text-decoration: none;
    }
    .score-green {
        background-color: #c6e9c6;
        color: #000;
        border: 1px solid #008000;
    }
    .score-red {
        background-color: #eacfcf;
        color: #000;
        border: 1px solid #FF0000;
    }
    .score-orange {
        background-color: #ebdecf;
        color: #000;
        border: 1px solid #FF8C00;
    }
    .score-blue {
        background-color: #c8d8e6;
        color: #1b455e;
        border: 1px solid #1b455e;
    }
    .score-purple {
        background-color: #cac4e7;
        color: #271b5e;
        border: 1px solid #271b5e;
    }
    .scroll-button {
        padding: 5px 15px;
        font-size: 14px;
        cursor: pointer;
        text-decoration: none;
        border-radius: 3px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .button-green{
        background-color: #c6e9c6;
        color: #000;
        border: 1px solid #008000;
    }
    .button-orange{
        background-color: #ebdecf;
        color: #000;
        border: 1px solid #FF8C00;
    }
    .button-orange:hover {
        background-color: #623600;
        color: #fff;
        border: none;
    }
    .button-green:hover {
        background-color: #015f01;
        color: #fff;
        border: none;
    }
    .move-to-top {
        padding: 5px 10px;
        font-size: 12px;
        color: #212121;
        border: 1px solid #212121;
        border-radius: 3px;
        cursor: pointer;
        text-decoration: none;
    }
    .move-to-top:hover {
        background-color: #212121;
        color: #fff;
    }
    .legend{
        margin-bottom: 10px;
    }
</style>
</head>
<body>

<div class="container">
    <div class="content">
        <h1>Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks</h1>

        <hr>

         <!-- Survey -->
         <div id="survey">
            <p>After reading through the LLM’s response below, <strong>please complete the following survey about your experience:</strong></p>
            <a class="button" href="https://docs.google.com/forms/d/e/1FAIpQLScr4fjvUGhtiTzBfsqm5CCVvAGafp3sLSSB_Txz2YHhnLiiyw/viewform?usp=pp_url&entry.1830873891=VW5kZXJzdGFuZGluZyBhbmQgU2ltcGxpZnlpbmcgQXJjaGl0ZWN0dXJlIFNlYXJjaCBpbiBTcGF0aW8tVGVtcG9yYWwgR3JhcGggTmV1cmFsIE5ldHdvcmtzIC0gMjAyNC0wNS0xNCAyMzo0MDoyMQ==" target="_blank">Click to submit author experience survey</a>
            <p>Your feedback is critical to evaluate the effectiveness of this LLM checklist assistant!</p>
        </div>

        <hr>

        <!-- <h2>Scores</h2>
        <table>
            <tr>
                <td><strong>Paper Quality Score:</strong></td>
                <td><span class="score-label score-blue">0.53</span></td>
            </tr>
            <tr>
                <td><strong>LLM Accuracy:</strong></td>
                <td><span class="score-label score-purple">0.43</span></td>
            </tr>
        </table>

        <hr> -->

        <h2 id="review-summary">Review Summary</h2>
        <div>
            <span class="legend score-label score-green">&nbsp; &nbsp;</span> The LLM found no significant concern
            <br>
            <span class="legend score-label score-orange">&nbsp; &nbsp;</span> The LLM proposes possible improvements
        </div>
        <table>
            <tr>
              <th>Question</th>
              <th></th>
            </tr>
            
            <tr id="summary-question-1">
                <td>1. Claims</td>
                <td><a href="#question-1" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-2">
                <td>2. Limitations</td>
                <td><a href="#question-2" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-3">
                <td>3. Theoritical assumptions and proofs</td>
                <td><a href="#question-3" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-4">
                <td>4. Experiments reproducibility</td>
                <td><a href="#question-4" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-5">
                <td>5. Code and data accessibility</td>
                <td><a href="#question-5" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-6">
                <td>6. Experimental settings/details</td>
                <td><a href="#question-6" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-7">
                <td>7. Error bars</td>
                <td><a href="#question-7" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-8">
                <td>8. Compute resources</td>
                <td><a href="#question-8" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-9">
                <td>9. NeurIPS code of ethics</td>
                <td><a href="#question-9" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-10">
                <td>10. Impacts</td>
                <td><a href="#question-10" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-11">
                <td>11. Safeguards</td>
                <td><a href="#question-11" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-12">
                <td>12. Credits</td>
                <td><a href="#question-12" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-13">
                <td>13. Documentation</td>
                <td><a href="#question-13" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-14">
                <td>14. Human subjects</td>
                <td><a href="#question-14" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-15">
                <td>15. Risks</td>
                <td><a href="#question-15" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
        </table>

        <hr>

        <h2>Detailed Review</h2>
        
        
        <div class="review" id="question-1">
            <div class="user_input">
                <p><strong class="question">Question # 1:</strong> Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Our main claims are listed in section 1 and highlight this paper's contributions.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer and justification seem generally aligned with the paper’s content regarding accurately reflecting the paper's contributions and scope in the abstract and introduction. The main claims related to addressing hyperparameter integration issues in Neural Architecture Search (NAS) and proposing a framework for simplifying the architecture search for Spatio-Temporal Graph Neural Networks (STGNNs) are supported by the paper’s content, specifically in sections 1 and 4, where these contributions are detailed. The abstract succinctly outlines the motivation, central problem, proposed approach, and key findings, aligning well with the detailed content in the subsequent sections.<br><br>However, to further improve the paper quality and ensure the completeness of compliance with the submission guidelines, consider the following feedback:<br><br>○ &nbsp;&nbsp;  <b>Clarity and Specificity of Claims:</b> Ensure that the claims made in the abstract and introduction are as specific and clear as possible. While the justification mentions where to find related material, it could further benefit from explicitly stating how each claim is addressed within those sections. This adjustment would help readers quickly verify the claims’ validity and understand the paper's contributions more deeply.<br><br>○ &nbsp;&nbsp;  <b>Highlight Limitations and Assumptions:</b> It appears the abstract and introduction predominantly focus on the contributions without equally highlighting any important assumptions or limitations of the proposed approach. Incorporating these elements can provide a balanced view and set realistic expectations for the readers. Referring to any considerations made regarding the computational complexity or the specific contexts in which the proposed method outperforms others would be valuable.<br><br>○ &nbsp;&nbsp;  <b>Direct Reference to Experimental Validation:</b> Increase direct references to the experimental setup and results in the abstract and introduction that validate the main claims. Highlighting the key findings from the comparative analysis with state-of-the-art methods could strengthen the paper's impact and help readers grasp the significance of the contributions quickly.<br><br>○ &nbsp;&nbsp;  <b>Consistency in Terminology:</b> Ensure consistency in the use of terminology throughout the abstract, introduction, and the rest of the paper. This includes terms related to the proposed solutions, methodologies, and frameworks. Consistency aids in avoiding confusion and contributes to a more cohesive reading experience.<br><br>○ &nbsp;&nbsp;  <b>Expand on Contribution to the Field:</b> While the introduction outlines the contributions, adding a brief discussion on how these contributions advance the field or address a gap in existing research could offer additional value. This might involve comparing with baseline approaches or highlighting the novelty of the disentangled framework and hyperparameter understanding in the context of STGNNs.<br><br>Implementing these suggestions can enhance the paper's quality by providing clearer, more comprehensive insights into the paper’s contributions, assumptions, and impacts on the existing body of knowledge.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-2">
            <div class="user_input">
                <p><strong class="question">Question # 2:</strong> Does the paper discuss the limitations of the work performed by the authors?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We have discussed the potential limitations in section 6, including the application domain and considered temporal modeling methods.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> Based on the authors' proposed answer and justification, as well as adherence to the provided guidelines, the discrepancies with the actual paper content need to be identified and addressed to improve the quality of the paper. Here are the itemized, actionable feedbacks:<br><br>○ &nbsp;&nbsp;  <b>Specificity of Limitations</b>: While Section 6 is referenced for discussing limitations, the content primarily sums up the study's outcomes and future work directions. It lacks a detailed discussion on the limitations regarding model applicability in various domains, robustness against different types of data noise, or dependence on specific types of graph structures. Providing examples of scenarios where the model might underperform or specific conditions where the assumptions do not hold would enhance transparency.<br><br>○ &nbsp;&nbsp;  <b>Assumptions and Robustness</b>: The guidelines suggest discussing the model's robustness to assumptions violation. The paper briefly mentions the modelling of spatio-temporal correlations and reliance on traffic prediction datasets, but it does not thoroughly analyze the implications if these conditions are unmet, such as non-traffic datasets or irregular data collection intervals. A more detailed exploration of the model's performance variability across different conditions or dataset characteristics would be valuable.<br><br>○ &nbsp;&nbsp;  <b>Scope of Claims</b>: The paper ambitiously promotes the generated models' superior performance across tasks. However, it primarily focuses on traffic forecasting. An explicit section discussing the boundary conditions for these claims, including dataset characteristics and environmental factors, would prevent overgeneralization and clarify the conditions under which the model excels or struggles.<br><br>○ &nbsp;&nbsp;  <b>Computational Efficiency</b>: The paper introduces a streamlined architecture search method but lacks a detailed discussion on computational efficiency, especially regarding scaling up to larger datasets or more complex graph structures. Commenting on the computational demands, potential bottlenecks, and scalability issues would align with the guidelines' suggestion to discuss algorithm efficiency and dataset size scaling.<br><br>○ &nbsp;&nbsp;  <b>Privacy and Fairness</b>: Although not directly relevant to the study's core focus, acknowledging potential limitations around data privacy (e.g., when applied to personal mobility data) and fairness (e.g., equitable service predictions across different regions) considerations could be pertinent, especially if the technology is to be deployed in real-world scenarios.<br><br>○ &nbsp;&nbsp;  <b>Error Bars and Reproducibility</b>: The paper discusses model performance and architecture search outcomes. Still, it does not clearly mention the reproducibility of results or the reporting of error bars for performance metrics, which are crucial for evaluating the model's consistency and the statistical significance of the reported improvements.<br><br>Actionable recommendations for improvement:<br>○ &nbsp;&nbsp; Expand the limitations section to include specific scenarios where the model's performance may be compromised.<br>○ &nbsp;&nbsp; Address the robustness of the model to assumption violations directly in the context of the presented model and dataset characteristics.<br>○ &nbsp;&nbsp; Clarify the scope and applicability of the model's claims by outlining the conditions and dataset characteristics that were used to validate those claims.<br>○ &nbsp;&nbsp; Include a subsection on computational efficiency and scalability that discusses the model's performance concerning computational resources and dataset sizes.<br>○ &nbsp;&nbsp; Briefly touch upon considerations around data privacy and fairness, even if not directly applicable, to foster a comprehensive understanding of the model's deployment implications.<br>○ &nbsp;&nbsp; Ensure reproducibility and statistical rigor by reporting error bars or confidence intervals for performance metrics.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-3">
            <div class="user_input">
                <p><strong class="question">Question # 3:</strong> For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper does not include theoretical results.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> Given the provided content and guidelines, the proposed author's "Answer" (NA) and their "Justification" (the paper does not include theoretical results) appear to be consistent with the guidelines and the content of the paper provided. The paper focuses on understanding and simplifying architecture search in Spatio-Temporal Graph Neural Networks and does not claim to provide theoretical results such as theorems or proofs which would require a detailed analysis of assumptions and proofs. Instead, it revolves around empirical studies, framework proposals, and the development of a method for neural architecture search which is validated through experiments.<br><br>To further improve the paper quality, particularly in relation to the question posed and based on the guidelines provided, the following feedback can be beneficial:<br><br>○ &nbsp;&nbsp;  <b>Clarification on Theoretical Contributions</b>: Even though the paper primarily focuses on empirical research, any underlying theoretical assumptions made during the design, analysis, or interpretation of the results should be explicitly stated. For example, assumptions regarding the data distribution, network behavior, or optimization properties could be more clearly described if applicable.<br><br>○ &nbsp;&nbsp;  <b>Justification for NA Answer</b>: Reinforce the justification for the NA answer by explicitly stating in the paper sections (such as in the methodology or discussion) that the research does not involve theoretical claims necessitating formal proofs. This explicit statement helps to clarify to readers and reviewers the nature of the contributions and the scope of the paper.<br><br>○ &nbsp;&nbsp;  <b>Supplemental Material</b>: Even in the absence of traditional theoretical results, it might be useful to include any supplemental analyses or background material that underpins the empirical work in an appendix or supplemental material. This could involve, for example, a more detailed discussion on the rationale behind certain design choices in the architecture or hyperparameter search space.<br><br>○ &nbsp;&nbsp;  <b>Connect Empirical Findings to Theory</b>: Where possible, link empirical findings back to existing theories or principles. For instance, if the disentangled framework proposed leads to better understanding or simplification of the architecture search, discuss why this is the case from a theoretical standpoint, even if the paper's contribution is not in proving new theorems.<br><br>○ &nbsp;&nbsp;  <b>References to Theoretical Works</b>: Clearly reference any theoretical works or frameworks that informed the development of the empirical methods used. This helps situate the paper within the broader scientific discourse and provides readers with resources for deeper theoretical insights.<br><br><br>This score is given because the authors' proposed answer and justification directly align with the content of the paper as it pertains to the guidelines provided. The feedback aims at further strengthening the paper by suggesting ways to make any underlying assumptions more explicit and to solidify the connection between the empirical work and theoretical considerations, even in a paper that does not directly present theoretical results.<br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-4">
            <div class="user_input">
                <p><strong class="question">Question # 4:</strong> Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed author's answer and justification seem to match well with the guidelines provided for the NeurIPS conference regarding reproducibility and disclosure of information necessary to reproduce the main experimental results. Specifically, the answer provided clearly states a "Yes", and the justification details the specific sections where the experiments, methodology, and other relevant information such as datasets, baseline comparisons, implementation details, and results are discussed. Further, the authors have provided a github link to their code, which is a strong addition in favor of reproducibility. However, to ensure the feedback is comprehensive and actionable, a closer examination of the paper content against the justification provided is necessary. The evaluation of the paper content reveals an exhaustive detailing across the sections specified and in the appendices, meeting the general expectations for reproducibility. Nonetheless, a few improvements could enhance clarity and accessibility.<br><br>○ &nbsp;&nbsp;  <b>Clarify Dataset Access and Licensing</b>: While the paper mentions datasets and provides a Github link for the code, clarity on the datasets' licensing or any constraints around their use could further align with best practices in reproducibility. If any datasets have licensing restrictions, mentioning them and suggesting possible alternatives can improve accessibility for reproducing results.<br><br>○ &nbsp;&nbsp;  <b>Detailed Hyperparameter Settings and Ranges</b>: The paper discusses hyperparameters in several sections but ensuring a consolidated and comprehensive listing of all hyperparameters used, including those not altered from defaults, can aid in better reproducibility. Considering the vast hyperparameter space, especially in neural architecture search and deep learning models, providing exact values and ranges tested can significantly reduce the effort required to replicate the study.<br><br>○ &nbsp;&nbsp;  <b>Dependencies and Environment Specifications</b>: For the code provided in the Github repository, including a detailed requirements.txt file or equivalent to specify the versions of libraries and dependencies can prevent compatibility issues. Further, a Dockerfile or an environment.yml file for replicating the software environment could be beneficial.<br><br>○ &nbsp;&nbsp;  <b>Additional Context on Codebase Usability</b>: Enhancing the documentation within the Github repository with more comprehensive instructions on how to run the code, including example commands for training, testing, and reproducing figures in the paper, will make it easier for others to follow the work. Comments within the code explaining complex or critical sections will also help in understanding the implementation better.<br><br>○ &nbsp;&nbsp;  <b>Expand on Limitations and Assumptions</b>: While the paper discusses the proposed method in detail, expanding the discussion around limitations and assumptions made in the study could help future work better build upon or address these areas. This includes acknowledging any computational resources required that might not be widely accessible.<br><br>Acknowledging these enhancements could strengthen the paper's alignment with the reproducibility standards expected at NeurIPS and provide clearer pathways for others to follow and extend the work.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-5">
            <div class="user_input">
                <p><strong class="question">Question # 5:</strong> Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer and justification appear to adhere to the NeurIPS guidelines and are consistent with the content of the paper as provided. The key points to address in the feedback to improve the paper quality are as follows:<br><br>○ &nbsp;&nbsp;  <b>Clear Access to Data and Code</b>: While the authors have mentioned that all data and code are open access, with the dataset information found in sections 4, 5, and Appendix B, and the code available on GitHub, it's vital to ensure the GitHub repository includes comprehensive instructions for reproducing the experiments. This includes the exact commands, dependencies, environment setup, and any data preprocessing steps required.<br><br>○ &nbsp;&nbsp;  <b>Reproducibility and Instructions Detailing</b>: Particular attention should be paid to ensuring the GitHub repository contains a straightforward README file that guides readers through setting up their environment, accessing and preparing the dataset(s), and executing the code to reproduce the results. This also involves specifying the versions of any external libraries or frameworks used and ideally providing a requirements.txt file or an environment.yml file for easy setup.<br><br>○ &nbsp;&nbsp;  <b>Dataset Licensing and Access</b>: The authors should explicitly state if there are any licensing issues with the datasets or if any datasets are proprietary or have restrictions on their use. If all datasets are indeed publicly accessible and have open licenses, this should be clearly mentioned either in the appendix or directly in the section that introduces the datasets.<br><br>○ &nbsp;&nbsp;  <b>Supplemental Material Completeness</b>: Ensure the supplemental material not only includes code and data but also any additional instructions, configurations, or scripts needed to reproduce the experimental results fully. This could involve hyperparameter settings, training procedures, or architectural details not fully covered in the main body of the paper.<br><br>○ &nbsp;&nbsp;  <b>Anonymity Preservation</b>: Since this is a submission phase review, verifying that the provided GitHub link does not inadvertently de-anonymize the authors is crucial. Ensure the GitHub repository adheres to the double-blind review process, either by being anonymized or set up in a way that does not reveal the authors' identities.<br><br>○ &nbsp;&nbsp;  <b>Experimental Results Reproducibility</b>: Confirm that all main experimental results presented in the paper can be reproduced with the provided code and datasets. If only a subset of results can be reproduced due to computational or other practical constraints, this limitation should be clearly stated, explaining which results are reproducible and providing justification for any exceptions.<br><br>○ &nbsp;&nbsp;  <b>Validation on Multiple Datasets</b>: While the paper mentions experiments conducted on multiple datasets, ensuring that instructions for reproducing these results across all datasets are included in both the paper and the supplemental material is essential for validating the generality of the proposed method.<br><br>By addressing these points, the authors can significantly enhance the reproducibility and integrity of their research submission, adhering to NeurIPS guidelines and contributing valuable resources to the research community.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-6">
            <div class="user_input">
                <p><strong class="question">Question # 6:</strong> Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer and justification indicate that they have provided the necessary details for understanding their results, including hyperparameter sampling, training and test details, as mentioned in sections 4 and 5 and appendix B of the paper. To align with the NeurIPS guidelines, let's focus on improving the quality of the given answers and justifications.<br><br><h3>1. Clarity and Specificity in Justification:</h3>○ &nbsp;&nbsp; <b>Ensure Specificity in Sections</b>: The justification mentions sections 4 and 5 and Appendix B but lacks specific subsections or paragraph information. It's beneficial to pinpoint exact locations (e.g., Section 4.2 for hyperparameter details) where these details are discussed for improved clarity and accessibility.<br>○ &nbsp;&nbsp; <b>Detail Appendix Content</b>: If Appendix B contains substantial methodological details not found in the main paper, summarizing key points in the justification can help reviewers and readers grasp the depth of provided training/test details.<br><br><h3>2. Comprehensive Hyperparameter Information:</h3>○ &nbsp;&nbsp; <b>Clear Hyperparameter Rationale</b>: If the paper selects certain hyperparameters or settings based on empirical observations or literature review, briefly mentioning this rationale in the justification strengthens the understanding of methodological choices.<br>○ &nbsp;&nbsp; <b>Consideration of Reproducibility</b>: Indicating whether hyperparameter choices were made to balance performance and computational cost can be valuable, especially if some were selected to ensure reproducibility by others with different computational resources.<br><br><h3>3. Description of Data Splits:</h3>○ &nbsp;&nbsp; <b>Explicit Data Split Ratios</b>: While the justification mentions training and test details, explicitly stating the ratio of data splits within the paper or appendix (e.g., 70% training, 15% validation, 15% test) improves transparency and reproducibility.<br>○ &nbsp;&nbsp; <b>Handling of Dataset Specifics</b>: If the paper involves multiple datasets with differing characteristics, clarifying how data splits or preprocessing were adapted for each can prevent confusion.<br><br><h3>4. Optimization and Evaluation:</h3>○ &nbsp;&nbsp; <b>Optimizer Details</b>: Providing specifics on the type of optimizer(s) used, along with justifications for their selection (e.g., Adam for faster convergence), can enhance the method's context.<br>○ &nbsp;&nbsp; <b>Evaluation Metric Consistency</b>: Confirm that the evaluation metrics used across experiments are consistent and justified in their choice, enabling fair comparison of results.<br><br><h3>5. Public Availability of Code and Data:</h3>○ &nbsp;&nbsp; Ensure that the paper mentions the availability of code and datasets or provides a reason if any of these cannot be shared. This is crucial for reproducibility and adherence to NeurIPS guidelines.<br><br>By addressing these points, the paper will provide a clearer, more detailed, and easily reproducible account of the experimental setup and hyperparameter choices, aligning with the NeurIPS expectations for transparency and reproducibility.<br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-7">
            <div class="user_input">
                <p><strong class="question">Question # 7:</strong> Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The author's proposed answer indicates that error bars or statistical significance information is reported for the experiments, specifically mentioning variance as error bars across sections 4 and 5. However, the provided paper excerpt does not contain specific evidence or detailed information that directly confirms or contradicts the claim within the constraints of the summary provided. Without access to the full text of Sections 4 and 5, and based solely on the summary, we cannot fully evaluate the accuracy of the author's claim regarding the reporting of error bars or statistical significance.<br><br><b>Actionable feedback based on the given guidelines to improve paper quality:</b><br><br>○ &nbsp;&nbsp;  <b>Explicitly Mention Error Bar Details</b>: Ensure that the sections discussing the experiments (Sections 4 and 5) explicitly mention the type of error bars used (e.g., standard deviation, standard error), how they are calculated, and what variability they represent. This is vital for the reader's understanding of the statistical significance and repeatability of the results.<br><br>○ &nbsp;&nbsp;  <b>Clarify Assumptions and Error Bar Calculations</b>: According to the guidelines, if error bars are based on assumptions such as normal distribution of errors, this should be clearly stated along with the rationale. Include a brief description or reference to the method of calculation for the error bars, whether it's a closed-form formula, a library function, or a bootstrap method.<br><br>○ &nbsp;&nbsp;  <b>Reference Figures or Tables for Error Bars</b>: If error bars are visualized in figures or reported in tables, ensure these are explicitly referenced in the text where the experimental results are discussed. This makes it easier for readers to find and understand the data supporting the paper's claims.<br><br>○ &nbsp;&nbsp;  <b>Discuss Factors of Variability</b>: Clearly state the factors of variability accounted for by the error bars. This could include variations due to different initializations, dataset splits, or model configurations. This information is crucial for assessing the robustness of the findings.<br><br>○ &nbsp;&nbsp;  <b>Consider Reporting Confidence Intervals</b>: If applicable and especially for asymmetric distributions, consider reporting confidence intervals in addition to or instead of standard error bars. This could provide a more accurate picture of the data's variability and the results' significance.<br><br>○ &nbsp;&nbsp;  <b>Statistical Tests</b>: If possible and relevant, include statistical tests to underscore the significance of the results. This could strengthen the paper's claims about the efficacy of the proposed methods.<br><br>By addressing these points, the authors could enhance the transparency and robustness of their experimental results reporting, thereby increasing the paper's contribution to the field.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-8">
            <div class="user_input">
                <p><strong class="question">Question # 8:</strong> For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> Discrepancies Identified:<br>○ &nbsp;&nbsp;  The paper mentions providing GPU hours in Section 5. However, the required comprehensive details on computer resources such as "type of compute workers (CPU or GPU, internal cluster, or cloud provider)", "relevant memory and storage", or "amount of compute required for each experimental run" are not explicitly detailed.<br>○ &nbsp;&nbsp;  The justification does not mention the disclosure of whether more compute than reported was used for the experiments comprising the research project, which includes preliminary or failed experiments not making it into the final paper.<br><br>Actionable Feedback:<br>○ &nbsp;&nbsp;  <b>Specify Compute Worker Details</b>: Clarify the type of compute workers used for experiments (e.g., CPU type, GPU model). If a mix of resources was utilized, detailing each setup separately is recommended.<br>○ &nbsp;&nbsp;  <b>Memory and Storage Explanation</b>: Provide information on the memory and storage capacities utilized for the experiments. If varied capacities were used across experiments, consider tabulating these differences.<br>○ &nbsp;&nbsp;  <b>Detailed Compute Requirements</b>: Break down the individual compute requirements for each experimental run. This includes not just the GPU hours already reported but also should encompass CPU hours (if applicable), the count of runs, and variance in resource usage by experiment type.<br>○ &nbsp;&nbsp;  <b>Broaden Compute Usage Disclosure</b>: Extend the discussion to include any additional compute resources consumed outside the scope of reported experiments, such as for pre-processing, model development phases, or discarded experiments. This will add transparency regarding the computational cost of your research.<br>○ &nbsp;&nbsp;  <b>Direct Referencing and Structuring</b>: Ensure that each required piece of information regarding compute resources is easy to find in the paper. Consider creating a separate subsection within Section 5 or an appendix dedicated to these details if they are extensive.<br><br>Improving these aspects will significantly contribute to the reproducibility and transparency of your research, enabling others to better understand and potentially replicate your work, an essential consideration for the NeurIPS conference.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-9">
            <div class="user_input">
                <p><strong class="question">Question # 9:</strong> Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We review the code of ethics and our paper conform with it.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The answer provided by the authors does not meet the NeurIPS guidelines for several reasons. The justification given is overly simplistic and lacks specific references to the paper's content that align with the NeurIPS Code of Ethics. According to the guidelines, if the authors answer "Yes," they should point to specific sections where the ethical considerations are discussed or addressed, which the authors failed to do.<br><br>Feedback:<br><br>○ &nbsp;&nbsp;  Reference Specific Sections: The authors should explicitly reference sections in the paper where ethical considerations, data usage permissions, potential biases in data or models, and impacts of the research are discussed in relation to the NeurIPS Code of Ethics.<br><br>○ &nbsp;&nbsp;  Discuss Ethical Considerations: The paper should include a discussion on the ethical implications of their research. This includes, but is not limited to, data privacy concerns, potential misuse of the technology, and how biases in data are addressed.<br><br>○ &nbsp;&nbsp;  Address Data Usage and Permissions: Clarify the permissions for the use of datasets, especially if the data involves personally identifiable information or potentially sensitive attributes.<br><br>○ &nbsp;&nbsp;  Consider Impact on Society: Reflect on how the research findings might impact society, both positively and potentially negatively. Discuss measures taken to ensure the research contributes positively to society.<br><br>○ &nbsp;&nbsp;  Potential Algorithmic Biases: If applicable, discuss any steps taken to identify and mitigate potential biases in the models or algorithms developed as part of the research.<br><br>○ &nbsp;&nbsp;  Include a Broader Impact Statement: If not already included, a broader impact statement should be added to the paper, detailing the potential societal impact of the research findings, including both benefits and drawbacks.<br><br>The authors are advised to update their justification to include specific references to sections of the paper that address these points, ensuring compliance with the NeurIPS Code of Ethics.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-10">
            <div class="user_input">
                <p><strong class="question">Question # 10:</strong> Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We discuss the broader impacts in the section "Broader Impact Statement".</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> Feedback:<br><br>○ &nbsp;&nbsp;  Discuss Both Impacts: Ensure the "Broader Impact Statement" section truly discusses both the potential positive and negative societal impacts of the work performed. The current justification lacks specificity regarding the nature of both types of impacts. Consider including explicit examples of potential beneficial applications of STGNNs in traffic prediction and urban planning, as well as potential risks such as privacy concerns, surveillance, or misuse of the technology in ways that could lead to unfair or biased outcomes.<br><br>○ &nbsp;&nbsp;  Expand on Mitigation Strategies: If the paper identifies negative societal impacts, it should also discuss possible mitigation strategies to address these concerns. This could include recommendations for ethical guidelines, technical safeguards, or policy measures that could help prevent misuse or mitigate negative effects. These strategies should be specific to the identified risks and practically feasible within the context of the work.<br><br>○ &nbsp;&nbsp;  Reference to Relevant Sections: The justification mentions a "Broader Impact Statement" section, but it should be clearer about where in the paper these discussions occur. Ensure that the discussions on positive and negative impacts are easily identifiable and that they provide substantial, thoughtful analysis. If this section is not adequately detailed, consider expanding it to more thoroughly examine the societal implications of the work.<br><br>○ &nbsp;&nbsp;  Consider Broader Application Contexts: While the paper focuses on traffic prediction, the technology could have broader applications in other domains. Discuss how the findings might generalize to other uses of STGNNs, and what societal impacts might be relevant in those contexts. This will help illustrate the potential scope of the technology's impact.<br><br>○ &nbsp;&nbsp;  Address Guidelines Comprehensively: Ensure the discussion aligns with the guidelines provided, which emphasize the importance of considering both intended use cases and potential for misuse. The paper should reflect a balanced view that recognizes the complexity of technological impacts on society.<br><br>○ &nbsp;&nbsp;  Improve Transparency: If there are limitations in the exploration of societal impacts due to the paper's scope or the authors' expertise, these should be acknowledged transparently. Offering avenues for future research or collaboration with social scientists and ethicists could be beneficial.<br><br><br>This score reflects that while the paper has acknowledged the importance of discussing societal impacts, there is room for improvement in providing a more detailed, balanced, and actionable analysis in line with the guidelines.<br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-11">
            <div class="user_input">
                <p><strong class="question">Question # 11:</strong> Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper poses no such risks.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The provided author answer to the NeurIPS checklist question about responsible release of data or models potentially at high risk for misuse is "NA" with the justification being that the paper poses no such risks. Given the content of the paper's abstract and sections provided, the authors are primarily contributing to the academic exploration of neural architecture search (NAS) techniques within the context of Spatio-Temporal Graph Neural Networks (STGNNs), particularly for traffic prediction applications. There is no explicit mention of releasing any pretrained models or data that could present direct misuse risks. However, considering the guidelines, even if a paper does not directly release data or models, discussing potential misuse scenarios or implications could still be beneficial, especially when dealing with data-driven models in potentially sensitive applications such as traffic prediction.<br><br><b>Feedback for Improvement</b>:<br><br>○ &nbsp;&nbsp;  <b>Clarify Potential Risks</b>: Even if the paper's contributions do not directly include releasing models or datasets, briefly discussing any potential risks associated with the deployment of proposed methods could enhance the responsible dissemination of research. For instance, biases in traffic data, potential for surveillance, or misuse in predictive policing, if relevant.<br><br>○ &nbsp;&nbsp;  <b>Model Deployment Considerations</b>: Acknowledge the broader implications of deploying NAS-optimized STGNNs in real-world settings. This includes any ethical considerations or potential unintended consequences, ensuring the paper responsibly flags any broader societal or ethical implications.<br><br>○ &nbsp;&nbsp;  <b>Data Privacy and Security</b>: If the methods proposed could potentially be applied to datasets that contain sensitive information, a brief discussion on data privacy, security, and compliance with relevant regulations (e.g., GDPR) could be added. This is important even if the paper itself does not release any datasets.<br><br>○ &nbsp;&nbsp;  <b>Future Work Directions</b>: Suggesting future research directions that consider the responsible and ethical application of the proposed architecture search and design methodology, such as developing mechanisms to audit or mitigate biases in spatio-temporal predictions, can be a valuable addition.<br><br>○ &nbsp;&nbsp;  <b>Community Engagement</b>: Encourage or provide guidelines for the community to responsibly use the proposed methods, including considerations for licensing of any resultant models, suggested ethical use cases, and discouraging misuse.<br><br>Given the lack of mention of potential risks or misuse and the focus on advancing NAS in STGNN for traffic prediction, it appears the "NA" response is technically accurate but could be expanded with a bit more nuance on the broader implications of deploying such technologies.<br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-12">
            <div class="user_input">
                <p><strong class="question">Question # 12:</strong> Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors claim that they have properly credited the creators or original owners of assets used in the paper and have mentioned the license and terms of use, citing the sources in references [13, 1, 10]. To provide actionable feedback based on the provided guidelines and the content of the paper, the following improvements are necessary:<br><br>○ &nbsp;&nbsp;  <b>Explicit License Mention</b>: While the authors have cited the sources of their datasets, there's no explicit mention of the licenses for these datasets within the given text. As per the guidelines, the name of the license (e.g., CC-BY 4.0) should be included for each asset used. The authors should update the paper to include a specific section or a note in the dataset citation that clearly states the license under which each dataset is released.<br><br>○ &nbsp;&nbsp;  <b>Terms of Use</b>: The authors have not mentioned the terms of use for the datasets. They need to explicitly state these terms to ensure that they respected them in their research. The paper should be updated to include a brief description of the terms of use for each dataset, especially focusing on any restrictions or requirements imposed by the licenses.<br><br>○ &nbsp;&nbsp;  <b>Version of the Asset Used</b>: The authors have provided a URL for accessing the datasets but did not mention which version of the asset was used. For the sake of reproducibility and clarity, the authors should mention the specific version of the dataset they used in their experiments.<br><br>○ &nbsp;&nbsp;  <b>Details on Code Assets</b>: If any code from external sources was used, the same level of detail (source citation, license, terms of use, and version) should be provided for these assets. It's unclear from the answer if other assets like code libraries or pre-trained models were used. If so, the authors should update the manuscript to credit these resources properly.<br><br>○ &nbsp;&nbsp;  <b>Dataset License Availability</b>: Authors are encouraged to verify if the license information is available online. If not, reaching out to the dataset creators for clarification would be advisable. This step ensures compliance and respect for intellectual property rights.<br><br>○ &nbsp;&nbsp;  <b>Link Correction and Accessibility</b>: Ensure that the provided GitHub link is accurate, functional, and links directly to the dataset and code repository with clear labeling of the datasets used. If the datasets are hosted within a larger repository, specifying the exact path would greatly assist readers and future researchers.<br><br>By addressing these points, the paper will meet the requirements for properly crediting the assets used in the research and ensure adherence to licensing and terms of use, which upholds academic integrity and respects the rights of the original asset creators.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-13">
            <div class="user_input">
                <p><strong class="question">Question # 13:</strong> Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> The paper does not release new assets.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The provided author's answer states "NA" with the justification that the paper does not release new assets. However, within the paper content, there is a direct contradiction to this answer. Specifically, in the section detailing the main contributions and in various parts throughout the paper, the authors mention, "Our code is available at https://github.com/AutoML-Research/SimpleSTG," indicating that new assets, namely code, have indeed been released as part of the paper's submission. This contradicts the authors' proposed answer to the checklist question regarding the documentation of new assets.<br><br><h3>Actionable Feedback:</h3><br>○ &nbsp;&nbsp;  <b>Correct the Answer to the Checklist Question</b>: The authors should change their answer from "NA" to "Yes" to accurately reflect that new assets (code) are introduced and shared as part of the paper.<br><br>○ &nbsp;&nbsp;  <b>Provide Details in Justification</b>: In the justification section, authors need to detail where the assets (code and/or models) are documented. This should include:<br>&nbsp;&nbsp;&nbsp; • &nbsp;&nbsp;   A direct mention of the repository URL within the paper where the code is available.<br>&nbsp;&nbsp;&nbsp; • &nbsp;&nbsp;   Information on how the code is organized, any dependencies required to run the code, and basic usage instructions.<br>&nbsp;&nbsp;&nbsp; • &nbsp;&nbsp;   An explicit section in the paper or an appendix that outlines the structure of the repository, if not directly included within the main sections related to methodologies and experiments.<br><br>○ &nbsp;&nbsp;  <b>Ensure Comprehensive Documentation of the Released Code</b>: Verify that the GitHub repository (or other hosting service) includes:<br>&nbsp;&nbsp;&nbsp; • &nbsp;&nbsp;   A README file with a comprehensive guide to installing, configuring, and running the code.<br>&nbsp;&nbsp;&nbsp; • &nbsp;&nbsp;   Documentation on the dataset(s) used, including how to access them, preprocess them, and use them with the provided code.<br>&nbsp;&nbsp;&nbsp; • &nbsp;&nbsp;   Licensing information that clearly states how the code and any potential datasets are licensed to ensure users understand their rights for using and modifying the assets.<br><br>○ &nbsp;&nbsp;  <b>Discuss Limitations and Dependencies</b>: In the paper, briefly discuss any limitations or dependencies of the provided code. For example, if the code requires specific hardware capabilities (e.g., certain GPU requirements), this should be clearly stated.<br><br>○ &nbsp;&nbsp;  <b>Include a Statement on Ethical Considerations and Permissions</b>: If the released code uses any datasets or pre-trained models, the paper should discuss whether and how consent was obtained for their use, alongside any ethical considerations addressed during the research.<br><br>○ &nbsp;&nbsp;  <b>Anonymity During Review Process</b>: Since the NeurIPS submission and review process is double-blind, ensure that the GitHub repository or any externally linked assets do not reveal the authors’ identities. Consider using an anonymized link or a placeholder URL if necessary during the review phase, with a plan to update it upon acceptance.<br><br>By addressing these actionable items, the paper will not only comply with the conference submission guidelines concerning the release of new assets but also enhance the reproducibility and transparency of the research being presented.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-14">
            <div class="user_input">
                <p><strong class="question">Question # 14:</strong> For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper does not involve crowdsourcing nor research with human subjects.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> The proposed author's answer and justification correctly align with the actual paper content, which focuses on understanding and simplifying architecture search in Spatio-Temporal Graph Neural Networks. The paper does not involve crowdsourcing experiments or research with human subjects; thus, it correctly indicates the NA (Not Applicable) response for the NeurIPS conference checklist question about including full text instructions, screenshots, and details about compensation for human subjects.<br><br>Feedback for improvement:<br><br>○ &nbsp;&nbsp;  Ensure Comprehensive Documentation: Although the current question is NA for your paper, ensure that any future modifications or extensions of the work that may involve human subjects or crowdsourcing experiments are documented comprehensively. This includes detailed instructions, compensation details, and ethical considerations.<br><br>○ &nbsp;&nbsp;  Consider Broadening Research Methodologies: If future work could involve human-centric studies or crowdsourcing for data annotation or model evaluation, plan to include such sections well in advance. Establish protocols for ethical compliance, informed consent, and data privacy in accordance with the NeurIPS guidelines.<br><br>○ &nbsp;&nbsp;  Ethical Considerations: Independently of the current NA response, always assess and discuss the broader ethical impact of your research in the paper, especially concerning data privacy, model bias, and potential misuse of research findings.<br><br>○ &nbsp;&nbsp;  Replicability and Open Science: Enhance the accessibility of your research by providing comprehensive supplementary material, which might include, but not limited to, code repositories, data access links (ensuring they comply with privacy norms), and detailed hyperparameter settings. Even when experiments do not involve humans, comprehensive documentation strengthens the scientific value of the work.<br><br>○ &nbsp;&nbsp;  Future Work Section: Elaborate on potential applications of your findings in areas that might involve human interaction or crowdsourcing, such as interactive machine learning, and detail how you would approach the ethical and logistical complexities therein.<br><br>Conclusion:<br><br>The paper appropriately justifies the NA response to the NeurIPS conference checklist question concerning human subjects and crowdsourcing. Future enhancements can focus on broader documentation and ethical considerations relevant to the research domain.<br><br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-15">
            <div class="user_input">
                <p><strong class="question">Question # 15:</strong> Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper does not involve crowdsourcing nor research with human subjects.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> The proposed answer and justification from the authors states that the question regarding potential risks, disclosures, and IRB approvals is not applicable (NA) because their study doesn't involve crowdsourcing or research with human subjects. Based on the complete text provided in the “Paper” section, this response is consistent with the content described. The paper focuses on the development and evaluation of Spatio-Temporal Graph Neural Networks (STGNNs) for traffic prediction, utilizing data-driven techniques without the involvement of human subjects in the data collection or analysis processes. Therefore, the nature of this research does not inherently require IRB approval, nor does it necessitate the disclosure of potential risks to study participants because it relies on traffic data rather than human subject research.<br><br>Feedback for Improvement:<br><br>○ &nbsp;&nbsp;  <b>Clarify Data Usage and Sources</b>: Enhance transparency by detailing the sources of traffic data, ensuring that readers understand the data's origin and confirm there's no indirect involvement of human subjects or privacy concerns. This can be added in a subsection within the methodology or data description.<br><br>○ &nbsp;&nbsp;  <b>Address Indirect Impacts</b>: Furthermore, it might be insightful to discuss any indirect implications or potential risks associated with the application of the system developed, such as privacy concerns related to location data, even if it's not personally identifiable. This could be in the form of a brief ethical considerations section.<br><br>○ &nbsp;&nbsp;  <b>Data Licensing and Permissions</b>: Although there are no human subjects directly involved, verifying and stating the licensing or permissions for using the traffic data would add to the paper's ethical and legal rigor. This is especially relevant if the data comes from third-party sources.<br><br>○ &nbsp;&nbsp;  <b>Future Work on Ethical Implications</b>: A suggestion for future research directions could include exploring any ethical implications of deploying such predictive technologies in real-world scenarios, considering societal, privacy, and security aspects. Adding a few sentences in the conclusion or future work section would suffice.<br><br>○ &nbsp;&nbsp;  <b>Broader Impact Statement</b>: Though the paper does not directly involve human subjects, including a broader impact statement that covers potential positive and negative implications of the research can enrich understanding of the societal relevance of the work being presented.<br><br>Concluding these improvements would not alter the fundamental nature of the paper's contribution to the field but would strengthen its ethical stance and potentially widen its appeal to a broader readership concerned with the societal impact of AI and machine learning technologies.<br><br></div>
            <a class="move-to-top" href="#survey">↑ Back to summary</a>
        </div>
        

        <!-- Survey -->
        <div>
            <p><strong>Please complete the following survey about your experience:</strong></p>
            <a class="button" href="https://docs.google.com/forms/d/e/1FAIpQLScr4fjvUGhtiTzBfsqm5CCVvAGafp3sLSSB_Txz2YHhnLiiyw/viewform?usp=pp_url&entry.1830873891=VW5kZXJzdGFuZGluZyBhbmQgU2ltcGxpZnlpbmcgQXJjaGl0ZWN0dXJlIFNlYXJjaCBpbiBTcGF0aW8tVGVtcG9yYWwgR3JhcGggTmV1cmFsIE5ldHdvcmtzIC0gMjAyNC0wNS0xNCAyMzo0MDoyMQ==" target="_blank">Click to submit author experience survey</a>
            <p>Your feedback is critical to evaluate the effectiveness of this LLM checklist assistant!</p>
        </div>
        
    </div>
</div>

</body>
</html>