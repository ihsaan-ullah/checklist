<h2>Deep Statistical Solvers</h2><br>--------------------------------------<br>[+] Correctness Score (genuine): 0.6<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Score (adversarial): 0.13<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Score (truth_adversarial): 0.07<br>--------------------------------------<br>--------------------------------------<br>[+] Resiliance Score: 0.87<br>--------------------------------------<br>--------------------------------------<br>[+] Combined Score: 0.0<br>--------------------------------------<br><h1><strong>genuine</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have mentioned main claims in Section 1: Introduction, calling them "contributions".</span><br><span style='color:#0000FF;'>Review: The response provided by the author accurately reflects the paper's contributions and scope as laid out in both the abstract and the introduction sections. The introduction clearly outlines the three main contributions of the study, which align with the claims made in the abstract. This clarity in stating the paper's objectives, scope, and contributions from the outset helps to set proper expectations for the reader and establishes a clear narrative for the paper's value proposition.<br><br>However, although I agree with the author's response in terms of matching the paper's contributions with the claims made in the abstract and introduction, it's crucial to ensure these claims are comprehensively backed by the content presented in the subsequent sections of the paper. The justification might benefit from more specific references to how each claim is substantiated within the body of the paper, particularly through theoretical proofs and experimental validation.<br><br>Constructive and actionable suggestions:<br>1. While the paper does an excellent job of clearly outlining its contributions in the introduction, it could further bolster its arguments by demonstrating an explicit link between these stated contributions and the evidence or data presented in later sections. This could involve a brief recap in the conclusion section, where each contribution is revisited with direct references to the findings that support it.<br>2. Considering the complexity and novelty of the Deep Statistical Solvers (DSS) approach, the paper could expand on the potential limitations or challenges encountered during the research process. Acknowledging these aspects would enhance the paper's transparency and could potentially open up avenues for future research.<br>3. The paper might benefit from a more detailed discussion of the broader implications of its contributions. While the technical advancements are clear, elaborating on how these contributions push the field forward or how they can be applied to real-world problems would provide valuable context and highlight the significance of the research beyond its immediate findings.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have indicated the limitations in the conclusion section, as future work.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. They mentioned future work and potential areas for further improvement in the conclusions section, which indirectly addresses the limitations of their work. It is clear from their response that they acknowledge there are aspects of their work that could be enhanced or explored further, which is a positive stance towards recognizing the limitations of their current research.<br><br>However, a more detailed discussion on the limitations directly, including the robustness to assumptions violations, the scaling of computational efficiency, and any potential issues with privacy and fairness (if applicable), would enrich the paper and provide a more nuanced understanding of the work's scope and applicability.<br><br>Constructive and actionable suggestions for the authors would include:<br>1. Explicit Limitations Section: Consider adding a separate "Limitations" section that explicitly outlines the limitations identified during the research. This will help readers quickly understand the scope and applicability of the findings.<br>2. Assumptions and Robustness: Discuss in detail any assumptions made throughout the research and explore how robust the proposed methods and findings are to violations of these assumptions. This can include limitations related to the data, the model, and the context of the application.<br>3. Empirical Validation Scope: Reflect on the scope of the empirical validation performed, including the diversity and number of datasets or contexts in which the methods were tested. Highlight any potential biases and discuss how they might influence the generalizability of the results.<br>4. Computational Efficiency: Provide a more detailed discussion on the computational efficiency of the proposed algorithms, including how they scale with the size of the dataset and their performance in different computational environments.<br>5. Addressing Privacy and Fairness: If applicable, discuss any considerations or limitations related to privacy and fairness within the context of the proposed solutions. This is particularly important for work with potential social implications.<br>6. Transparency and Integrity: Encourage complete honesty about the limitations to help develop norms that preserve the integrity of the scientific community. This approach not only benefits readers but also enhances the credibility of the research.<br><br>By addressing these suggestions, the authors can significantly improve the transparency and depth of their discussion on limitations, enriching the overall contribution of their work to the field.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have provided clearly the hypotheses and a proof sketch in the main text, and detailed proofs in appendix.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is correct and in line with the content of the paper. The paper includes theoretical results for which the full set of assumptions and a complete and correct proof (or proof sketches) are provided, as stated. The detailed appendix further supports the theoretical claims made in the paper, including those regarding the Universal Approximation Property for Deep Statistical Solvers, and conditions under which these solvers exhibit desirable properties. Additionally, the paper adheres to guidelines by numbering and cross-referencing theorems and proofs, and by clearly stating all necessary assumptions directly related to the theoretical results.<br><br>However, here are some constructive and actionable suggestions for further improvement:<br>1. Clarity and Accessibility: While the proofs are provided and the assumptions are clear, the authors could improve the paper by making the theoretical sections more accessible to readers not deeply familiar with the specific mathematical background. This could include a more detailed explanation or intuitive discussion of key concepts prior to delving into the formal proof.<br><br>2. Highlighting Novelty: It would be beneficial if the authors highlighted how their theoretical contributions compare to or improve upon existing work more explicitly. This could involve a more detailed discussion in the introduction or conclusion sections regarding the novel aspects of their theoretical results in the context of existing literature.<br><br>3. Proof Simplification: If possible, simplifying some of the proofs or including simplified versions as illustrative examples could help in making the theoretical part more digestible. This would not replace the full proofs but serve as an additional layer of explanation.<br><br>4. Extensions and Limitations: While the authors provide assumptions and proofs, further discussing potential extensions of their results or limitations based on the assumptions made could enrich the discussion. It would give readers a clearer understanding of the theoretical boundaries of the presented work.<br><br>5. Interactive Proofs: If feasible, the authors could consider providing supplementary interactive materials or visualizations that help explain the theoretical results or the intuition behind the proofs. This approach could make the theoretical contributions more engaging and understandable to a broader audience.<br><br>Overall, the paper provides a solid theoretical foundation for its contributions, adhering to the guidelines for presenting theoretical results.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have provided all details necessary to reproduce the results in the "Experimental conditions" section.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors matches the content and detail offered in the paper regarding their experimental framework and conditions sufficient for reproducing the main experimental results. The paper contains extensive descriptions of the data generation process, model architecture, training procedures, hyperparameter settings, and even detailed mathematical proofs supporting their universal approximation claims for the newly proposed Deep Statistical Solvers (DSS). Furthermore, the paper discusses the normalization processes applied to the data, adjustments for training stability, and specifies the hardware used for experiments. These details collectively offer a comprehensive view necessary for replication efforts.<br><br>However, to ensure rigorous critical feedback as requested:<br><br>1. Reproducibility with provided details: While the authors claim to provide all necessary details, ensuring reproducibility extends beyond the mere description of experimental conditions. It involves clarity over implementation nuances that might significantly impact the results. For instance, specifics on the initialization parameters for the neural networks, detailed version numbers for key software dependencies, or random seed values can crucially affect the ability to replicate results.<br><br>2. Complexity in implementation details: The intricate details surrounding the architectural specifics, changes in variables for different experiments, and the application of their theoretical groundwork into practical implementations can be overwhelming. Providing supplementary material with pseudo-code or even an abstract representation of the implementation steps could enhance comprehensibility and reproducibility.<br><br>Constructive and actionable suggestions for the authors to enhance reproducibility further:<br>- Consider publishing the source code alongside the paper. While the textual descriptions are comprehensive, direct access to the code would dramatically improve reproducibility and offer clarity on any implementation subtleties not fully captured in the text.<br>- Provide a supplementary document or appendix that distills the implementation into step-by-step instructions or pseudo-code, particularly for the data preprocessing steps and model training routines.<br>- Include details on software environments and dependencies (e.g., TensorFlow/PyTorch versions, any specific libraries required) to mitigate potential compatibility issues.<br>- Detail any data augmentation or preprocessing techniques not fully covered, as these can have significant impacts on the outcomes of machine learning experiments.<br>- Address how variations in hardware might affect replicability, particularly if experiments were conducted on specialized or high-end hardware configurations that might not be readily accessible to all researchers.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our code is made available in the supplementary materials, and links to the datasets can be found as references. In this version, to protect anonymization, we are not publishing the code on our GitHub repo, but this will be done for the final version.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors correctly states that they have made the code available in the supplementary materials and also provided links to the datasets as references. This matches the guidelines that require open access to data and code, along with sufficient instructions to faithfully reproduce the main experimental results. Additionally, the authors mention a commitment to publish the code on their GitHub repo openly for the final version, which shows an intention towards full compliance with the open access policy for data and code.<br><br>However, it should be noted that the anonymization constraint mentioned by the authors applies only to the review phase to comply with the double-blind review policy. For the final version, ensuring that the code and data are indeed publicly accessible and sufficiently documented for replicating the experimental results is crucial.<br><br>Constructive and actionable suggestions for the authors to further align with the open-access requirements include:<br>1. Explicitly Detailing Reproduction Steps: Include a README file or equivalent documentation within the supplementary materials that outlines the precise steps to reproduce the experiments. This should encompass environment setup, data preprocessing, model training, and evaluation procedures.<br><br>2. Environment Configuration: Provide an environment file (e.g., a `requirements.txt` for Python packages or a Dockerfile) to facilitate the recreation of the computational environment used in the experiments.<br><br>3. Clarify Data Access and Preparation: Expand on the instructions for accessing and preparing the dataset, especially if any preprocessing is required before running the experiments. This will help in ensuring that researchers can start from the raw data and arrive at the same results reported in the paper.<br><br>4. Inclusion of Scripts for Baselines and Ablation Studies: If any baselines or ablation studies are part of the experimental section, including scripts or instructions on how to replicate these experiments will enhance transparency and reproducibility.<br><br>5. Post-Review Phase Code and Data Sharing: After the review phase, ensure that the code and data repositories are updated to reflect the final version of the work, removing any anonymization layers that were initially applied. Providing a direct link in the final paper version is recommended for ease of access.<br><br>By addressing these suggestions, the authors can improve the reproducibility of their research and adhere more closely to the NeurIPS guidelines for code and data sharing.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The "Experimental conditions" section provides all details.</span><br><span style='color:#0000FF;'>Review: Justification: While the paper indeed includes an "Experimental conditions" section that provides many details about the experiments, such as dataset splits, hyperparameters, optimizer used, and hardware specifications, the justification provided in the review is too brief and lacks critical examination of the completeness and clarity of these details. Additionally, the paper could improve its presentation of how hyperparameters were chosen, whether any form of hyperparameter optimization or search was employed, and more explicit details on the type of optimizer's settings beyond simply naming it. A critical review should identify these gaps or areas for improvement.<br><br>Constructive and Actionable Suggestions:<br>1. Hyperparameter Selection: The authors should expand on how the hyperparameters were chosen for the experiments. If a search was performed, providing details on the search space, criteria for selection, and the validation process would enhance reproducibility and the understanding of the methodology.<br><br>2. Optimizer Details: While mentioning the use of the Adam optimizer, the paper could provide more specifics on the learning rate, decay rates, or any modifications to the default parameters if they were applied. Including such details would assist in the reproducibility of the results.<br><br>3. Additional Experimental Details: For completeness, the authors are encouraged to specify any preprocessing steps, data augmentation techniques, or other relevant details not fully covered in the "Experimental conditions" section. Clarifying these aspects would give a more comprehensive view of the experiment setup.<br><br>4. Code Availability: If not already provided, the authors should consider making the implementation of their proposed method and the experimental setup available as open-source code. Including scripts for hyperparameter search and data processing would also be valuable.<br><br>5. Appendix or Supplementary Material: For any details that are too extensive for the main body of the paper, the authors could leverage appendices or supplementary materials. This can include additional tables or figures summarizing hyperparameter choices, detailed algorithm pseudocode, or extended discussion on experimental setup variations.<br><br>By addressing these points, the authors can significantly improve the clarity and comprehensiveness of the experimental setup presentation, fostering better reproducibility and a deeper understanding of the work's implications.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We provide 10th and 90th percentiles.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for the question on reporting error bars and statistical significance does not meet the standards outlined in the guidelines. While the authors mention providing the 10th and 90th percentiles, this in itself does not adequately address the requirement for suitably and correctly defined error bars or other appropriate information about the statistical significance of the experiments.<br><br>The guidelines specifically ask for a clear explanation of the factors of variability the error bars are capturing, the method used for calculating the error bars, and any assumptions made. Moreover, it is necessary to clarify whether the error bars are standard deviations, standard errors of the mean, or if they indicate confidence intervals. Simply referencing percentiles without this context does not ensure a thorough understanding of the statistical significance or the reliability of the experimental results.<br><br>Furthermore, the paper should discuss how these percentile values contribute to the verification of the main claims, particularly in relation to the statistical significance and the spread of the results across different experimental runs. Percentiles alone do not provide a full picture of the statistical distributions of the results, especially concerning how these distributions support or challenge the main claims.<br><br>Constructive and actionable suggestions:<br>1. Expand on Statistical Reporting: Clearly articulate the factors of variability captured by the reported percentiles. Include information on whether these are due to train/test splits, initialization variances, random parameter selections, or other factors.<br>2. Method Explanation: Provide a detailed explanation of how you calculated the 10th and 90th percentiles. If applicable, mention any statistical tools or formulae used.<br>3. Error Bars Clarification: To improve clarity, consider including error bars that represent standard deviations or standard errors, in addition to the percentiles. This will help readers to better assess the reliability and the variance of the data.<br>4. Statistical Significance: Discuss the statistical significance of your findings in the context of the percentiles provided. If relevant, perform and report additional statistical tests that can help validate the main conclusions of the paper.<br>5. Results Discussion: In the results section, thoroughly discuss how these statistical measurements (e.g., percentiles, standard deviations) relate to the overall findings and claims of the paper. This will strengthen the connection between the data analysis and the paper's contributions.<br>6. Include Assumptions and Limitations: Clearly state any assumptions made during the statistical analysis and discuss the limitations that may affect the interpretation of the data.<br><br>By addressing these points, the authors can significantly enhance their statistical analysis reporting, thereby providing a more comprehensive understanding of the experimental results and their significance.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Compute Resources are explained in the "Experimental conditions" section.</span><br><span style='color:#0000FF;'>Review: Justification Review:<br>The provided justification is somewhat vague and does not precisely align with the evaluative criteria laid out in the question and guidelines. While the paper does mention "compute resources" in sections related to experimental conditions, the level of detail provided in terms of CPU/GPU specifics, memory, total compute time, and whether additional computations not reported in the paper were necessary for the project, is partially lacking. Specifically, the information on the type of compute workers (CPU/GPU specifics, internal cluster, or cloud provider) is not thoroughly detailed for all experiments. For instance, the paper mentions the use of an "Nvidia GeForce RTX 2080 Ti" for training and an "Intel Xeon Silver CPU (1.80GHz)" as well as an "Intel i5 dual-core (2.3GHz)" for running baseline methods. However, details such as memory and storage capacities, exact time of execution for each experiment, and the total compute requirement are not explicitly provided for each experimental condition, making it challenging to accurately estimate the resources needed for reproduction.<br><br>Constructive Suggestions:<br>1. Provide Detailed Hardware Specifications: Include detailed information on the type of compute workers used (e.g., specific GPU/CPU models) across all experiments. Mention any utilized internal clusters or cloud providers, and specify relevant memory and storage capacities.<br><br>2. Clarify Time of Execution: For each experimental run, provide detailed information on the time of execution in terms of hours or days. This allows for a clearer understanding of the computational effort involved and assists in replicability efforts.<br><br>3. Total Compute Requirement: Offer an estimation of the total compute required for the research project, including both reported and preliminary or failed experiments not included in the paper. This should cover the sum of resources needed for development, testing, and final experiment runs.<br><br>4. Comparison with Baseline Methods: When mentioning compute resources for baseline methods, ensure that comparable metrics (e.g., time of execution, memory used) are presented alongside those for the proposed method. This aids in evaluating the efficiency and feasibility of replicating the proposed approach versus established methods.<br><br>5. Explicit Mention of Additional Compute Resources: If the full research project required more compute than what was reported for the experiments in the paper (e.g., for preliminary explorations or failed experiments), explicitly state this fact. Providing such disclosures ensures transparency and helps set expectations for those attempting to replicate or build upon the work.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We comply with the NeurIPS code of ethics.</span><br><span style='color:#0000FF;'>Review: The authors' affirmation that the research conforms to the NeurIPS Code of Ethics, while brief, does not present any evidence to the contrary within the content provided. The paper introduces a novel optimization framework and demonstrates its application through simulations in controlled environments. These types of studies typically have minimal direct ethical considerations compared to research involving human subjects or potential environmental impacts.<br><br>However, for thoroughness and as a part of good scholarly practice, the review could benefit from a more detailed justification that specifically addresses key aspects of ethical considerations relevant to computational research. These might include data privacy (where applicable), the potential for algorithmic bias (especially in applications related to system simulations that could eventually impact human decisions), and the broader implications of their method's application.<br><br>Constructive and actionable suggestions for the authors to strengthen their submission concerning the ethical considerations might include:<br><br>1. Data Privacy and Security: If any real-world data or simulations that model real-world scenarios were used, briefly discuss the considerations around the privacy and security of such data. If not applicable, a statement clarifying the same would add clarity.<br><br>2. Algorithmic Bias and Fairness: Reflect on the potential for the introduced method to perpetuate or mitigate biases, especially when applied to real-world datasets or problems. If biases are not applicable, explaining why would be informative.<br><br>3. Impact on Society and Environment: Offer a reflection or short discussion on the potential societal and environmental impact of deploying such a solver in real-world scenarios. Addressing possible positive impacts or concerns could demonstrate mindfulness about the broader implications of their work.<br><br>4. Reproducibility and Openness: Alignment with the ethics guidelines also includes aspects of open science. If it hasn't been done, consider making the code or simulation environments available under an appropriate open-source license, which promotes transparency and reproducibility.<br><br>5. Anonymity Preservation: Confirm that all efforts were made to preserve anonymity in the submission phase, as required by the NeurIPS submission guidelines.<br><br>While the given answer aligns with the NeurIPS guideline's question format and the information provided does not indicate any ethical breaches, elaborating on these suggestions could not only strengthen the paper's ethical considerations but also enhance its overall contribution to the field by providing a more comprehensive view on how the research aligns with broader ethical standards.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This is a fundamental research paper. Discussing broader impact in beyond the scope of the papers.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for not discussing societal impacts is inconsistent with the guidelines set by the conference. While the paper is indeed focused on fundamental research pertaining to Deep Statistical Solvers (DSS) and their applications in optimization problems and system simulations, it is important to recognize that advancements in optimization techniques, particularly those that significantly outperform existing methods in terms of speed and accuracy, do have potential societal impacts. Optimizing systems for power grids, as discussed in the paper, can have a substantial impact on energy distribution and consumption patterns, which directly relates to societal welfare, environmental sustainability, and economic factors.<br><br>Moreover, the ability to solve a wide range of optimization problems more efficiently could have both positive and negative implications, including impacts on industries that rely heavily on optimization (like transportation, logistics, manufacturing, and energy sectors), potentially leading to job displacement or shifts in labor demands. There can also be broader implications for privacy, security, and fairness, depending on the application domains of the developed models.<br><br>Constructive and actionable suggestions for the authors include:<br><br>1. Discuss Potential Applications: Briefly discuss the sectors or domains where DSS could be innovatively applied, emphasizing both the positive impacts (such as improvements in efficiency and sustainability) and any potential risks or negative impacts (such as job displacement or privacy concerns in data-sensitive applications).<br><br>2. Reflection on Ethical Considerations: Include a section reflecting on ethical considerations and the responsibility of researchers to ensure that their work promotes societal welfare and does not exacerbate existing inequalities or create new ethical dilemmas.<br><br>3. Mitigation Strategies: Propose potential mitigation strategies for any identified negative impacts, such as ethical guidelines for the deployment of optimization models, mechanisms to ensure transparency and fairness in decisions influenced by these models, and suggestions for policy and regulation that could safeguard against misuse.<br><br>4. Engage with the Broader Research Community: Encourage collaboration with social scientists, ethicists, and domain experts to explore the broader implications of DSS technologies and to devise comprehensive strategies for their responsible development and deployment.<br><br>By addressing these aspects, the paper would not only conform to conference guidelines regarding societal impacts but also contribute to a more nuanced understanding of the broader implications of emerging optimization technologies.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our models do not present such risks.</span><br><span style='color:#0000FF;'>Review: The given answer and justification by the authors for not addressing safeguards for responsible release is marked as "NA" on the premise that their models "do not present such risks." However, this assertion seems insufficiently substantiated given the nature and applications of the Deep Statistical Solvers (DSS) introduced in the paper. While it is possible that the specific implementations and experiments presented in the paper do not directly involve high-risk data or models, the broader implications and potential applications of DSS in various domains (e.g., power systems, molecular dynamics) warrant a more nuanced consideration of potential risks and safeguards.<br><br>Constructive and Actionable Suggestions:<br><br>1. Risk Assessment: The authors should perform a detailed risk assessment of the potential applications of DSS. This includes identifying scenarios where DSS might be applied to sensitive or high-risk domains and discussing the implications.<br><br>2. Safeguards and Guidelines: Even if the current applications presented in the paper are deemed low-risk, offering a general set of guidelines or considerations for responsible usage of DSS could significantly enhance the paper's contribution. This might include best practices for data handling, model training, and deployment, especially in contexts where the potential for misuse is higher.<br><br>3. Transparency and Documentation: The paper could benefit from a dedicated section discussing ethical considerations and transparency. This might include documenting the data sources used in training, the decision-making process behind model design, and any measures taken to ensure fairness, privacy, and security.<br><br>4. Community Engagement: Encourage dialogue within the research community about the responsible development and deployment of DSS technologies. This could take the form of workshops, symposiums, or collaborative projects focused on ethical AI development.<br><br>5. Future Work: Acknowledge the limitations of the current approach in addressing these broader societal and ethical considerations and outline a plan for future work in this area. Highlight the importance of interdisciplinary research involving ethicists, policymakers, and domain experts in shaping the development of DSS technologies.<br><br>By addressing these suggestions, the authors can strengthen the paper's consideration of responsible AI development and deployment, even if the current implementations do not directly pose high risks for misuse.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: TODO</span><br><span style='color:#008000;'>Justification: [TODO]</span><br><span style='color:#0000FF;'>Review: Justification:<br>The given answer and justification are placeholders (TODO), indicating that they have not been filled out, which does not comply with the guidelines for addressing questions about asset crediting and licensing in the paper review. Properly crediting the creators or original owners of assets and respecting the terms of use is crucial for ethical research and publication practices. It ensures transparency, reproducibility, and legal compliance.<br><br>Constructive and Actionable Suggestions:<br>1. Asset Crediting: The authors should provide a detailed acknowledgment section or footnote that cites the original sources of any code, data, or models used in their research. This includes citing the original papers or sources that introduced these assets.<br><br>2. Version and URL: For each asset used, the authors should specify the version number if applicable and provide a direct URL or DOI where the asset can be accessed. This information aids in reproducibility and ensures that readers can locate the exact resources used.<br><br>3. License Information: The authors must clearly state the name of the license (e.g., MIT, CC-BY 4.0) under which each asset is released. This is essential to inform readers of the terms under which they can use, modify, or distribute the assets.<br><br>4. Scraped Data: If the paper involves data that was scraped from web sources, the authors need to provide information about the copyright and terms of service of those web sources, ensuring that their data collection methods comply with legal and ethical standards.<br><br>5. Derived Assets: For datasets or assets that the authors re-packaged or modified, they should provide the license information for both the original and the derived asset. This clarifies the permissions and restrictions applied to the newly created asset.<br><br>6. Direct Communication: In cases where license information is unclear or not readily available, the authors should make an effort to contact the original creators of the assets for clarification. This can also include requesting permission where necessary.<br><br>7. Documentation: All the relevant licensing and asset information should be included in the supplementary materials or repository that accompanies the paper. This makes it easier for readers and other researchers to find and review the license terms.<br><br>By addressing these aspects, the authors would significantly enhance the legal and ethical integrity of their work, promote fair use of digital assets, and adhere to the best practices in academic publishing.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our data and code are well documented in supplemental material.</span><br><span style='color:#0000FF;'>Review: The authors' answer appears to correspond with the expectations set by the guidelines. They affirm that their data and code, as significant contributions of the paper, are well-documented and included in the supplemental material provided. This indicates an attempt to ensure that fellow researchers can reproduce their work, understand the limitations, licensing information, and possibly extend upon it. Considering the importance of transparency and reproducibility in research, this practice is commendable and aligns well with the recommended guidelines for documenting new assets introduced in a paper.<br><br>However, while the answer is satisfactory, a more detailed description in the review regarding precisely how the documentation adheres to the guidelines (e.g., details about training, limitations, and licenses) would provide clearer insight into the adequacy of the documentation. Additionally, there's no mention of consent, which is particularly relevant if personal or sensitive data is involved in the research.<br><br>Constructive suggestions for improvement and verification:<br><br>1. Clarify Consent Process: If applicable, clarify how consent was obtained for any data used in the research. This is particularly important for assets that include personal or sensitive information.<br><br>2. Detailed Documentation: Ensure that each asset's documentation, shared as supplemental material, includes comprehensive details such as the purpose of the asset, how it was created, any limitations, licensing information, and instructions for use. This enhances reproducibility and ethical use of the assets.<br><br>3. Training Details: Provide explicit details about the training processes, including parameter settings and hardware specifications, to aid in reproducing the research results.<br><br>4. Asset Accessibility and Anonymization: Confirm that the assets are accessible through the provided links and that any sensitive information has been appropriately anonymized, adhering to the submission guidelines.<br><br>5. License Information: Clearly state the license under which the assets are released, ensuring potential users are aware of how they can legally use or modify the assets.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our research does not involve any human.</span><br><span style='color:#0000FF;'>Review: The response provided by the authors is consistent with the content of the paper. From the detailed explanation of the paper's methodology and contributions, it's clear that the research focuses on developing and validating a new class of trainable solvers for optimization problems through simulations, specifically using a Deep Statistical Solver approach implemented within a Graph Neural Network framework. The experiments discussed are computational in nature, involving the validation of the proposed models against system simulations and optimization tasks, such as solving linear systems and AC power flow computations. There is no mention or implication of crowdsourcing experiments or research involving human subjects directly in the experimentation process.<br><br>Constructive and actionable suggestions in case the scope of the research were to include human-related studies in the future:<br><br>1. If human subjects become involved in future work, such as in the evaluation of the solver's results or in gathering datasets, ensure that comprehensive details about the methodology are provided. This includes the full text of instructions given to participants, screenshots of the task interface (if applicable), and clear information about compensation.<br>2. Consider ethical implications and ensure compliance with relevant guidelines and regulations concerning research involving human subjects. This includes obtaining informed consent from participants, ensuring the anonymity and confidentiality of participant data, and providing fair compensation.<br>3. If such studies are conducted, it would be beneficial to include a separate section in the paper dedicated to the methodology involving human participants. This section should explicitly address the recruitment process, task design, ethical considerations, data handling practices, and analysis of the human-involved component of the research.<br>4. Ensure that any findings from studies involving human subjects are reported transparently, including any limitations or biases that may arise from the methodology used.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our research does not involve human subjects or ethical issues.</span><br><span style='color:#0000FF;'>Review: Based on the content provided from the paper "Deep Statistical Solvers Anonymous Author(s)," it is clear that the study does not involve human participants, direct interaction with human subjects, or the collection of personally identifiable information which could result in ethical concerns or necessitate Institutional Review Board (IRB) approvals. The research focuses on the development and validation of a machine learning framework called Deep Statistical Solvers (DSS), applied to optimization problems arising from system simulations, particularly in the contexts of molecular dynamics, power systems, and simulations of partial differential equations (PDEs). The methodologies and experiments described within the paper involve computational simulations and testing the proposed DSS framework against existing numerical methods or datasets derived from standard benchmarks, such as the IEEE power grid datasets.<br><br>Given the nature of the study as described, the response of "NA" for the question regarding risks to study participants, disclosure of such risks, and the acquisition of IRB approvals is consistent with the paper's content. The justification provided by the authors aligns with the guidelines, as their work indeed does not involve crowdsourcing nor research with human subjects.<br><br>Constructive suggestions for the authors:<br><br>1. Clarify in Future Work or Ethical Considerations: Even though your current research does not involve human subjects, any future development that might involve human interaction (directly or indirectly) could consider mentioning potential ethical considerations or implications. For example, when discussing the potential applications of DSS, a brief note on ethical considerations relevant to those domains could enrich the discussion.<br><br>2. Application Impact Discussion: While IRB approval and participant risk are not applicable, a section discussing the broader impacts, including positive societal implications or potential misuse of your research, could provide a well-rounded perspective on your contributions.<br><br>3. Data Usage Transparency: Although your research uses publicly available or computational datasets, ensuring that the use of any dataset respects its intended use and licensing can be an ethical practice worth mentioning.<br><br>These suggestions aim to complement your already rigorous research approach by embedding considerations of broader impacts and ethical reflections pertinent to your domain of study.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have mentioned main claims in Section 1: Introduction, calling them "contributions".</span><br><span style='color:#0000FF;'>Review: Justification:<br>The main claims made in the abstract and introduction only partially reflect the paper's contributions and scope. While the authors clearly state their intentions to introduce a novel neural network architecture (LEAP net) for power transmission grids and present it as a transformative approach in the application of Deep Neural Networks (DNNs) to physics and engineering domains, some inconsistencies and concerns arise:<br><br>1. The abstract mentions the use of real historical data from the French high voltage power grid for evaluation, but the paper lacks detailed experimental setup and results specific to this claim. This makes it challenging to assess the practical viability and impact of the proposed method on power grid assessments.<br><br>2. The introduction outlines ambitious goals and the potential of DNNs in accelerating simulations and optimizations across various domains. However, the paper primarily focuses on theoretical contributions and a novel architecture without thoroughly grounding these claims in empirical evidence across the claimed domains.<br><br>3. The paper claims to address the weaknesses of existing approaches by directly incorporating physical knowledge and permutation-invariance in the representation of problems. While the introduction of Statistical Solver Problems (SSP) and Deep Statistical Solvers (DSS) reflects this, the extent to which this approach outperforms or complements existing methods is not convincingly demonstrated through comparative analysis or validation.<br><br>Constructive Suggestions:<br>1. Enhance Experimental Validation: Provide more detailed experimental results, especially related to the application on the French high voltage power grid. This should include the dataset's specifics, evaluation metrics, comparative analysis with traditional approaches, and insights into the model's generalization and limitations.<br><br>2. Clarify Generalizability: Clarify the scope of applicability of the proposed method. While the paper ambitiously aims to address a wide range of physics and engineering problems, focus on presenting concrete examples and results that substantiate these claims. If the method's effectiveness varies across applications, discuss these variations and potential reasons.<br><br>3. Benchmark Against Existing Methods: Include a more comprehensive comparison with existing approaches to solving optimization and system simulation problems, particularly those that integrate physical constraints or knowledge. This comparison should highlight the advantages, challenges, and unique contributions of the proposed framework.<br><br>4. Address Limitations and Future Work: More transparently discuss the limitations of the current approach and how it compares with existing solutions. Outline specific areas for future research and how the community can build upon or extend the proposed framework to address these limitations and broaden its applicability.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have indicated the limitations in the conclusion section, as future work.</span><br><span style='color:#0000FF;'>Review: Justification:<br>The authors mention that they indicated the limitations in the conclusion section, under future work. However, this treatment of limitations does not align with the best practices recommended in the guidelines. The conclusion and future work sections often focus on potential extensions or applications of the current work rather than critically assessing its limitations. A thorough discussion of limitations involves more than mentioning areas for future exploration; it requires a critical analysis of the current work's boundaries, assumptions, and any specific contexts in which the approach may not perform well.<br><br>The paper content, as presented, does include some discussions in sections titled "Conclusions and Future Work" and throughout the document where potential improvements or areas of further investigation are mentioned (e.g., incorporating discrete variables, investigating convergence properties, and understanding limitations of the approach). However, the authors' response inadequately reflects the depth of reflection on the limitations of their work as outlined in the review guidelines.<br><br>Constructive and Actionable Suggestions:<br>1. Separate Limitations Section: Consider adding a separate "Limitations" section to the paper where the limitations are discussed explicitly and in detail. This section should critically evaluate the work, including assumptions made, conditions under which the approach may not perform as expected, and any intrinsic limitations of the methods used.<br><br>2. Assumptions and Robustness: Clearly articulate any assumptions made in the development and application of your method. Discuss how robust your results are to violations of these assumptions and consider performing experiments to show the impact of relaxing some of these assumptions.<br><br>3. Scope and Generalizability: Reflect on the scope of the claims made in the paper and the generalizability of the results. If the approach was tested on a limited number of datasets or configurations, discuss the implications for broader applicability.<br><br>4. Influence Factors: Identify and discuss factors that may influence the performance of your approach. Provide insights into conditions or variables that might affect the results and suggest ways users can mitigate any negative impacts.<br><br>5. Computational Efficiency: Discuss the computational efficiency of the proposed algorithms, including how they scale with dataset size or complexity. If there are potential bottlenecks, suggest possible solutions or workarounds.<br><br>6. Privacy and Fairness: If applicable, discuss any considerations related to privacy, security, or fairness that may arise from the application of your methods. Suggest ways to address these concerns.<br><br>Incorporating these suggestions will not only strengthen the paper but also provide a more comprehensive view of the work for readers, ensuring that they have a clear understanding of its potential and limitations.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have provided clearly the hypotheses and a proof sketch in the main text, and detailed proofs in appendix.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors claims that all necessary hypotheses and proof sketches for their theoretical results are present in the main text with detailed proofs provided in the appendix. However, upon detailed review of the paper and the theoretical results it presents, it is evident that while the paper makes a strong attempt to provide a comprehensive theoretical framework for their approach, there are several areas where the proofs and assumptions could benefit from further clarification or elaboration to meet the standards of completeness and correctness as per the guidelines.<br><br>1. Complexity of the Proofs and Assumptions: The paper indeed introduces several theoretical results along with their proofs in the appendix, but the complexity and novelty of these results necessitate a more thorough explanation of the assumptions involved. Particularly for Theorems 1 and its Corollary, while an effort has been made to outline the proofs, the assumptions leading to these theorems are not sufficiently detailed in the main text for a reader to fully grasp the implications without diving into the appendices.<br><br>2. Proof Sketches in the Main Text: The main text provides sketches for some proofs but these sketches could be made more intuitive. Detailing the intuition behind the choice of assumptions and the steps in the proofs can significantly improve the reader's understanding and could highlight the novelty of the approach more effectively.<br><br>3. Clarity on Assumptions: In some instances, the assumptions are mentioned but not discussed in terms of their necessity or implications. A discussion on why certain assumptions are made and how they influence the scope of the theoretical results would add value.<br><br>Actionable Suggestions:<br><br>- Expand on Assumptions in the Main Text: A clearer and more detailed explanation of the assumptions behind each theorem in the main text would help readers understand the conditions under which the results hold without having to refer to appendices.<br><br>- Enhance Proof Sketches: Where proof sketches are provided in the main text, attempt to include more intuition about why certain steps are taken or why specific techniques are employed. This can help in making the theoretical results more accessible.<br><br>- Discuss Implications of Assumptions: Include a section discussing the implications of the assumptions made for each theoretical result. This could involve exploring the limitations they impose, the possibilities they open up, and how they relate to existing work in the field.<br><br>- Detailed Proofs in Appendices: Ensure that the detailed proofs provided in the appendices are meticulously checked for correctness and completeness. Where possible, include discussions on any non-trivial steps or assumptions to aid the reader's understanding.<br><br>In summary, while the paper presents a strong theoretical framework, it can benefit from further clarity and detail regarding the assumptions and proofs of its theoretical results to fully align with the guidelines provided.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have provided all details necessary to reproduce the results in the "Experimental conditions" section.</span><br><span style='color:#0000FF;'>Review: Justification for the score:<br>The authors' response indicates that all details necessary for reproducing the results are provided in the "Experimental conditions" section. However, upon careful review of the paper, several areas need further clarification to fully meet the reproducibility criteria as outlined in the guidelines.<br><br>1. Model Implementation Details: While the paper details the architecture and the problem formulation extensively, specific implementation details such as the exact neural network configurations (number of layers, units per layer, etc.), and activation functions used, are missing. These details are crucial for reproducing the experiments.<br><br>2. Hyperparameter Settings: The paper mentions the use of hyperparameters like learning rates and batch sizes but lacks a comprehensive enumeration of all hyperparameters used, including any model-specific parameters (e.g., the correction coefficient α and parameters for data normalization).<br><br>3. Gradient Clipping: The paper mentions the use of gradient clipping to address gradient explosion issues, but it does not specify the threshold used or the conditions under which clipping was applied.<br><br>4. Dataset Details: While the paper describes how data for experiments were generated, it lacks specific detail that would enable exact replication. For instance, details about the distribution parameters from which the mesh points in Poisson equation discretization are sampled can significantly influence the outcomes.<br><br>5. Software and Hardware Environment: Details about the software (e.g., TensorFlow version) are mentioned, but hardware specifications that might impact performance benchmarks, such as computational times, are absent. This information is crucial for contextualizing performance evaluations.<br><br>Constructive and actionable suggestions for improvement:<br>- Model and Hyperparameters: Provide an appendix or supplemental material with exhaustive listings of the neural network architecture specifics, including layer types, sizes, activation functions, and all hyperparameters involved in the experiments.<br>- Data Generation Process: More explicit detail should be added about the data generation process, including any randomization parameters or distribution specifics, which would allow exact replication.<br>- Gradient Clipping Strategy: Elaborate on the gradient clipping approach, including threshold values and when it is applied during training.<br>- Include Seed Values: For experiments involving randomization (e.g., data generation, initialization), include the seed values used to enable exact replication by other researchers.<br>- Detailed Environment Specs: Provide detailed specifications of the software and hardware environment in which the experiments were conducted, including the make and model of GPUs used if applicable.<br>- Supplementary Material: Consider providing supplementary material with sample code, especially for custom-designed components or complex data preprocessing steps. While NeurIPS does not require releasing the entire codebase, snippets that highlight non-trivial aspects could significantly enhance reproducibility.<br><br>Addressing these points will strengthen the paper's compliance with NeurIPS' reproducibility guidelines and aid the research community in verifying and building upon the work presented.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our code is made available in the supplementary materials, and links to the datasets can be found as references. In this version, to protect anonymization, we are not publishing the code on our GitHub repo, but this will be done for the final version.</span><br><span style='color:#0000FF;'>Review: Justification for Score:<br>The answer provided by the authors is not fully consistent with the best practices for ensuring reproducibility of experimental results. While the authors state that the code is available in the supplementary materials and mention plans to publish the code on GitHub in the final version, they do not address several crucial aspects that contribute to the reproducibility of the results.<br><br>Constructive and Actionable Suggestions:<br>1. Detailed Instructions: The authors should include detailed instructions within the supplementary material or the eventual GitHub repository on how to reproduce the main experimental results. This includes specifying the exact commands needed to run the experiments, the expected output, and any additional steps required to obtain the reported results.<br><br>2. Environment Information: It is important to provide information on the computing environment where the experiments were conducted, including hardware specifications, operating system, programming language versions, and dependencies with their specific versions. Providing a requirements.txt file or an environment.yml for Python environments, for example, can greatly assist in reproducing the research environment.<br><br>3. Access to Data: The authors mention that links to the datasets can be found as references, but they should ensure that these links are accessible and provide detailed guidance on how the data should be prepared for use with the provided code. If there are any preprocessing steps required, corresponding scripts or detailed explanations should be included.<br><br>4. Reproducibility of Baselines: If the paper includes comparisons with baseline methods, the authors should provide or reference the implementation of these baselines, clarifying if any modifications were made. If the baseline implementations are not provided, the reasons should be clearly stated.<br><br>5. Anonymization: While the authors rightly mention the need to anonymize the supplementary material at the submission stage, they should ensure that anonymous access to any external repositories is provided without requiring sign-in or access requests, to preserve the reviewing integrity.<br><br>6. Clarification on Partially Reproducible Experiments: The authors should clearly state if only certain experiments are reproducible with the provided code and data, and explain why some may not be. This clarity helps reviewers and readers understand the scope of reproducibility.<br><br>By addressing these suggestions, the authors can significantly improve the reproducibility of their experimental results and comply with NeurIPS guidelines for code and data submission.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The "Experimental conditions" section provides all details.</span><br><span style='color:#0000FF;'>Review: The provided justification does not adequately reflect the depth and detail required to fully replicate the results presented in the paper. The brief mention of the "Experimental conditions" section does not, by itself, ensure that all necessary training and testing details, including data splits, hyperparameters, choice methodologies, types of optimizers, etc., have been comprehensively outlined with clarity and precision necessary for replication and understanding of the results.<br><br>Constructive and Actionable Suggestions:<br>1. Detailed Reporting: Authors should explicitly list all hyperparameters used in the experiments, including those for the models and optimizers. If choices were made based on experimental trials, a brief description of the selection process would add clarity.<br><br>2. Data Split Specifics: For transparency and replicability, it is important to include specifics regarding how data was split into training, validation, and test sets. Specifically, details on the proportions used, randomization procedures, and any stratification methods should be described.<br><br>3. Optimizer Details: The type of optimizer used should be specified along with its parameters such as learning rate, momentum (if applicable), and any scheduler used for learning rate adjustments.<br><br>4. Code and Reproducibility: If not already done, providing access to the source code, including training and testing scripts under a permissive license, would significantly enhance the paper's value and facilitate replication. Inclusion in supplementary material or a publicly accessible repository with documentation can achieve this.<br><br>5. Appendix/Supplementary Material Utilization: For complex experimental setups or where extensive tuning was required, it's advisable to use supplementary sections or appendices to delve into these details. This practice ensures the main text remains accessible while still providing in-depth technical insights.<br><br>By addressing these points, the paper could substantially improve in terms of transparency, reproducibility, and utility to the broader research community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The error bars are provided.</span><br><span style='color:#0000FF;'>Review: Justification: The authors mention that the experiments were repeated three times with different initialization seeds as detailed in Section 5 - Experiments, which suggests some level of variability consideration. However, the actual presentation of error bars or statistical significance tests to frame these repeated measures is not explicitly discussed or shown in the provided excerpts. While it is mentioned that results are highly correlated with "ground truth" solutions and that "the error bar is within the size of the dots" for a specific figure, there is no clear or systematic explanation of how the error bars were calculated, whether they signify standard deviations, standard errors of the mean, confidence intervals, or if any statistical tests were applied to assess significance levels.<br><br>Constructive and Actionable Suggestions:<br>1. Clearly State Error Metrics: The paper should explicitly state the type of error metrics used (e.g., standard deviation, standard error) and provide a clear explanation of how these metrics are calculated (e.g., closed-form formula, bootstrap).<br>2. Statistical Analysis Methodology: Include a section or paragraph detailing the statistical analysis methods used to evaluate the experiments. This should cover the assumptions made, such as normality of errors, and specify the confidence interval levels (e.g., 95% CI) if applicable.<br>3. Incorporate Error Bars in Figures and Tables: Ensure that all relevant tables and figures explicitly show error bars or indicate statistical significance. Reference these in the main text, discussing the implications of the observed variability on the study's conclusions.<br>4. Discuss Variability Factors: Describe the sources of variability considered in the experiments (e.g., initialization seeds, data splits) and how they impact the interpretation of the results.<br>5. Statistical Significance Testing: If not already done, perform statistical significance testing (e.g., t-tests, ANOVA) where appropriate to support the claims made based on experimental results. Describe the test used and the rationale behind choosing each test.<br>6. Guidance for Reproducibility: Provide guidelines for other researchers on reproducing the statistical analysis, including any software or libraries used for this purpose. This enhances the reproducibility and credibility of the experiments conducted.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Compute Resources are explained in the "Experimental conditions" section.</span><br><span style='color:#0000FF;'>Review: Justification: The review of the provided paper content indicates that while some details on computational resources are mentioned, the information is not as comprehensive as required based on the guidelines. Specifically, the paper mentions the use of "an Nvidia GeForce RTX 2080 Ti" for running experiments, as well as software versions like TensorFlow 1.14. However, detailed specifics about the types of compute workers (CPU/GPU specifics beyond a single mention), memory requirements, storage requirements, execution time for experiments, and an estimate of the total compute required are not provided. The descriptions do not cover the diversity of computational resources used across different experiments comprehensively. Additionally, there is no disclosure on whether the full research project required more compute than what was reported for the experiments in the paper.<br><br>Constructive Suggestions:<br>1. Detailed Computational Resource Description: For each experiment, clearly specify the type of compute resources used, including the specifics of compute workers (e.g., CPU model, number of GPUs, GPU model), memory capacity, and storage requirements. This helps in understanding the scalability of the proposed method and its applicability in resource-constrained settings.<br>2. Execution Time: Provide the execution time for each experiment or model training session. This information is critical for evaluating the efficiency of the proposed methods and for planning computational resource allocation.<br>3. Estimate of Total Compute: Estimate the total compute required for each experimental run, including preliminary or failed experiments not included in the paper. This provides a clearer picture of the computational cost involved in arriving at the presented results.<br>4. Cloud or Internal Cluster Details: If experiments were conducted using cloud services or internal clusters, mention the specifics, including the cloud provider's name or the configuration of the internal cluster. This information is crucial for reproduction and comparison of results.<br>5. Resource Optimization Discussion: Include a discussion on how the computational resources were optimized during experiments, if applicable. Sharing strategies for efficient usage of computational resources can be valuable for the research community.<br>6. Reproducibility Support: Consider providing supplementary material or appendices detailing the script or configuration settings used for the experiments. This step would greatly assist in the reproducibility of the results.<br><br>By addressing these suggestions, the paper can significantly improve the transparency and reproducibility of the reported experiments, aligning better with the guidelines provided.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We comply with the NeurIPS code of ethics.</span><br><span style='color:#0000FF;'>Review: The provided justification for compliance with the NeurIPS Code of Ethics is inadequate and lacks detail. Simply stating "We comply with the NeurIPS code of ethics" without elaboration does not provide sufficient evidence of ethical compliance. Given the importance of ethical considerations in research, particularly with respect to NeurIPS' guidelines, a more comprehensive treatment of ethical adherence is warranted.<br><br>Constructive and actionable suggestions:<br><br>1. Detailed Ethical Considerations: The authors should revisit the NeurIPS Code of Ethics and explicitly state how their research adheres to each relevant point. If the paper involves datasets, human subjects, or potentially dual-use technology, it is particularly important to discuss these aspects in detail.<br><br>2. Data Usage and Privacy: If the research involves the collection or use of data, particularly personal or sensitive data, the authors should elaborate on the consent process, data anonymization techniques, and data storage and handling policies they followed to protect privacy.<br><br>3. Impacts and Dual Use: The paper should discuss the potential impacts of the presented research, both positive and negative. This includes consideration of dual-use concerns, where the technology could be used for purposes unintended by the authors, including harmful applications. The authors should outline measures taken or propose recommendations to mitigate negative impacts.<br><br>4. Authorship and Contribution Transparency: Ensure that all contributing authors are properly credited and their roles and contributions are transparent. Moreover, acknowledging the involvement, if any, of crowdworkers or participants in the study and ensuring they were treated ethically and fairly is essential.<br><br>5. Resource Use and Accessibility: If the research involves extensive computational resources, discuss the environmental impacts and steps taken to mitigate them. Additionally, talk about the accessibility of your research artifacts (code, datasets, models) to ensure reproducibility and to foster open scientific inquiry.<br><br>6. Conflict of Interest Disclosure: Clearly disclose any potential conflicts of interest, including funding sources, affiliations, or intellectual property considerations, that could influence the research outcomes.<br><br>7. Anonymity Preservation for Peer Review: Ensure that the paper’s text does not reveal the identity of the authors or their affiliations to preserve the integrity of the double-blind peer review process required by NeurIPS.<br><br>Addressing these aspects in the revised manuscript will not only strengthen the paper's ethical stance but also enhance its contribution to the field, aligning it more closely with the NeurIPS Code of Ethics.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The paper's positive impact is evident and there is no negative impact we can think of.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for not discussing negative societal impacts is insufficient. Given the scope and potential applications of Deep Statistical Solvers (DSS) in the realms of power grids and language model training, there are several potential societal impacts, both positive and negative, that warrant discussion.<br><br>While the paper does highlight the potential for accelerating simulations and optimizations in engineering and physics as a positive impact, it neglects to consider or discuss the potential negative impacts. For example, the application to power grids, while advantageous for assessing curative actions in emergency situations, could also lead to vulnerabilities if the technology were to be misused or if inaccurate models led to incorrect assessments. Additionally, the mention of leveraging DSS for training large language models without addressing the potential for misuse in generating disinformation or contributing to privacy and security concerns is a significant oversight.<br><br>Constructive and Actionable Suggestions:<br>1. Expand the discussion on societal impacts: The authors should provide a more balanced view by considering both the positive impacts and the potential negative consequences of their work. This includes reflecting on how the technology might be misused, the consequences of errors, and the implications of deploying such technologies in sensitive areas like power grids and language models.<br><br>2. Suggest mitigation strategies: Where negative impacts are identified, the authors should discuss possible measures to mitigate these risks. This could include technical safeguards, ethical guidelines for use, or proposals for regulatory oversight to ensure the technology is used responsibly.<br><br>3. Consider broader implications: Encourage the authors to think beyond the immediate applications of their research to consider how advancements in DSS might influence societies, economies, and ethical considerations in the long term. This could lead to a more comprehensive and nuanced understanding of their work's societal impact.<br><br>4. Engagement with ethics and AI safety communities: The authors could be advised to engage with interdisciplinary research, including AI ethics and safety communities, to better understand and address the potential societal impacts of their work. This collaboration might also inspire future research directions aimed at safeguarding against negative outcomes.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our models do not present such risks.</span><br><span style='color:#0000FF;'>Review: Explanation:<br>The justification provided by the authors stating "Our models do not present such risks" seems to be inconsistent with the detailed content of the paper, particularly the section discussing the application to Large Language Models (LLMs). The text explicitly mentions the high risk for misuse or dual-use applications related to the released LLM models, highlighting their ability to generate coherent and persuasive text that could be exploited for nefarious purposes, such as spreading misinformation or automating propaganda. In this context, the answer of "NA" and the justification that their models do not present such risks contradicts the paper's own acknowledgment of potential risks.<br><br>Constructive and Actionable Suggestions:<br><br>1. Risk Assessment and Disclosure: The authors should revisit their response to address the specific risks associated with the release of their models, especially given the explicit mention of risks in the paper. A thorough risk assessment and clear disclosure of potential misuse or dual-use applications would provide more context and accuracy.<br><br>2. Safeguard Implementation Details: Given the acknowledgment of potential risks, the authors are encouraged to describe any safeguards or measures implemented to mitigate these risks. Details on how the models are released, whether there are any usage guidelines, access restrictions, or safety filters in place, would be valuable. If no such safeguards have been implemented, the authors should discuss why and consider what measures could be beneficial in the future.<br><br>3. Ethical Considerations and Collaboration: Encourage the authors to engage in discussions with ethicists, policymakers, and the broader research community to explore effective strategies for responsible release and use of powerful models like LLMs. Collaboration could lead to the development of standardized practices or frameworks for addressing ethical concerns related to AI research.<br><br>4. Future Research Directions: The authors could use this opportunity to outline future research directions aimed at developing more effective safeguards for AI models. This may include the design of inherently safer models, the creation of robust monitoring systems for detecting misuse, or the development of technologies that can counteract malicious use of AI.<br><br>5. Transparency and Community Engagement: Finally, promoting transparency around the development and release of AI models, as well as actively engaging with the AI research community and the public, can foster a shared understanding of the ethical challenges and collective efforts to address them.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The introduction credits the students and workers who contributed.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors does not fully meet the requirements specified in the guidelines for properly crediting the creators or original owners of assets used in the paper. While it mentions the contribution of students and workers, it does not clearly detail the citation of original papers for code packages or datasets, nor does it mention the version of assets used, URLs, the name of licenses, terms of use, or properly respects the copyright and terms of service of scraped data sources.<br><br>Constructive and Actionable Suggestions:<br><br>1. Citations and Versions: Explicitly cite every original asset (code, dataset, models) used in the study, clearly stating the version of the asset. Ensure this information is present either in the main text or in the references section.<br><br>2. URL and License Information: Provide URLs where the assets can be accessed. Alongside, clearly state the name of the license (e.g., CC-BY 4.0) for each used asset. If the asset is subject to multiple licenses, detail how they interact or apply.<br><br>3. Scraped Data: If the paper involves scraped data, comply with the source's copyright and terms of service. It's crucial to explicitly mention these terms and how the study abides by them.<br><br>4. Asset Release: If the paper introduces new datasets, models, or code, specify the license under which they are released and include copyright information and terms of use within the package. Providing this information within the manuscript or supplementary materials can enhance transparency.<br><br>5. Derived Assets: For datasets that are repackaged or modified from their original form, both the license of the original dataset and the derived dataset must be mentioned, highlighting any changes in the licensing due to modifications made.<br><br>6. Communication with Creators: When the copyright and licensing information are not publicly available, make an effort to contact the original creators for permission or clarification on the terms of use. Documenting this effort in the manuscript could alleviate concerns about proper use and credit.<br><br>The above suggestions are aimed at ensuring full compliance with ethical standards concerning the use of existing assets, thereby respecting the intellectual property rights of original creators while fostering a transparent and reproducible research environment.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our data and code are well documented in supplemental material.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for the documentation of new assets seems to lack detail and specificity necessary for a thorough evaluation per the guidelines. The statement "Our data and code are well documented in supplemental material" does not provide enough information regarding the aspects required by the guidelines, such as:<br>- Structured templates detailing the dataset/code/model,<br>- Information on training procedures,<br>- Licensing information,<br>- Limitations of the assets,<br>- Consent obtained for the use of assets,<br>- Measures taken to anonymize the assets (if applicable).<br><br>Constructive and Actionable Suggestions:<br>1. Provide Detailed Documentation: Include a section or an appendix in the paper or in the supplemental materials that addresses each of the guidelines provided. This should offer a structured overview of the dataset/code/model, including its characteristics, how it was created, and any preprocessing steps.<br><br>2. Training Procedures and Parameters: Detail the procedures for training your models, including hyperparameters, the processing of data, validation techniques, and how the results can be replicated.<br><br>3. Licenses and Limitations: Clearly state the license under which the assets are released and discuss any limitations or biases present in the dataset or code. This will help future researchers understand how they can use the assets and what limitations they should be aware of.<br><br>4. Consent and Anonymization: If applicable, the paper should explicitly state how consent was obtained for any used assets, especially if the assets involve personal data. Furthermore, outline the steps taken to anonymize the assets for submission, providing specific details to ensure transparency.<br><br>5. Accessibility: Ensure that the data, code, or model is readily accessible to reviewers and eventual readers. Provide a clear link or instructions on how to access the supplemental material and make sure that the assets are available in a format that is easy to use and understand.<br><br>Implementing these suggestions will improve the clarity and completeness of the documentation surrounding new assets introduced in the paper, making it easier for reviewers and readers to evaluate the work and potentially reproduce the results.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our research does not involve any human.</span><br><span style='color:#0000FF;'>Review: The authors' response of "NA" is consistent with the content of the paper, as the research focuses on the application of a novel neural network embedding approach for power transmission grids and does not involve crowdsourcing experiments or research with human subjects. Consequently, there would be no requirement to provide instructions given to participants, screenshots, or details about compensation, as these elements are irrelevant to the study's methodology and scope.<br><br>Constructive and Actionable Suggestions:<br>- While the response is appropriate for the current research framework, should the authors consider incorporating human-centric studies or crowdsourcing tasks in future work, it will be essential to include detailed methodologies, including participant instructions, compensation details, and ethical considerations, per the guidelines provided.<br>- In light of evolving research ethics and guidelines, keeping abreast of any changes or additional requirements for conducting and reporting research involving human subjects will be beneficial, even if current research does not include such components.<br>- Should there be any indirect involvement of human subjects in any capacity (e.g., for validation studies or data curation), a brief acknowledgement and description of their involvement would enhance transparency and adherence to ethical guidelines, even if the primary focus does not revolve around human-centred research.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our research does not involve human subjects or ethical issues.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors stating "NA" with the justification that their research does not involve human subjects or ethical issues aligns with the paper content. The paper focuses on developing and evaluating a novel neural network architecture, the Deep Statistical Solver (DSS), for modeling power transmission grids and solving optimization problems related to system simulations. The research is computational in nature and primarily deals with the development and assessment of machine learning models without involving human subjects or collecting data from human participants.<br><br>However, it's crucial to critically review the broader impact of the research, especially if the developed models or techniques could potentially be used in ways that affect human operations or safety, such as in managing power grids or emergency situations. While the paper discusses the application of DSS to solve problems like power flow in electric grids, these applications are based on historical data and simulations, not involving real-time decision-making or direct human interaction.<br><br>Constructive and actionable suggestions:<br>1. Although the current study does not involve humans directly, it would be beneficial for future work to consider potential indirect impacts on society, especially as the technology matures and is potentially integrated into real-world applications.<br>2. Given the application in critical infrastructure like power grids, future iterations of the research could benefit from an expanded discussion on ethical considerations, including the potential for misuse, reliability concerns, and the importance of transparency and fairness in algorithmic decision-making.<br>3. As the research progresses towards practical applications, collaboration with stakeholders and subject matter experts in the energy sector could help identify additional ethical and safety considerations, ensuring responsible implementation.<br>4. Should future research involve human subjects in any capacity, such as user studies for model validation in real-world scenarios, obtaining IRB approval and detailing the consent process will be crucial.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>truth_adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The abstract of another paper by the same first author was substituted.</span><br><span style='color:#0000FF;'>Review: Justification: The given answer and its justification seem to be incorrect and irrelevant based on the provided content description of the paper "Deep Statistical Solvers". The abstract mentioned in the initial content accurately introduces the paper's main contribution, which revolves around a novel neural network embedding approach for modeling power transmission grids, named LEAP net. It importantly highlights the architecture's ability to implement transfer learning for rapid assessment of curative actions in emergency situations, specifically referencing its applicability to the French high voltage power grid. Furthermore, the introduction and subsequent sections detail the contributions claimed, such as proposing the Statistical Solver Problem (SSP), demonstrating the Deep Statistical Solver (DSS) satisfies Universal Approximation properties, and experimental validation against traditional solvers, which collectively underline the paper's contributions and scope accurately.<br><br>Constructive and actionable suggestions:<br>1. Correction of Abstract and Introduction: Ensure that the abstract and introduction clearly outline the paper's main contributions and are free from unrelated material. This includes specifically detailing the problem being addressed, the proposed solutions (LEAP net, SSP, DSS), and the significance of these contributions.<br><br>2. Highlight Key Contributions: Explicitly list the key contributions in a dedicated subsection in the introduction to provide readers with a clear overview of the innovations and findings of the paper.<br><br>3. Clarity and Precision: Ensure that the descriptions used in the abstract and introduction are precise and clearly align with the content presented in the main body of the paper. Avoid overgeneralizing the contributions or scope.<br><br>4. Limitations and Assumptions: While stating the contributions, also briefly mention the main assumptions and any limitations of the approach directly in the introduction. This helps set the right expectations for the readers.<br><br>5. Contextual Relevance: Provide brief contextual relevance of the proposed solution within existing literature in the introduction to emphasize the novelty and necessity of the proposed approach.<br><br>6. Proofread for Irrelevant Material: Avoid including content that may be viewed as irrelevant or misleading in the abstract or introduction. Any accidental inclusion of unrelated material (as mentioned in the justification) can significantly detract from the credibility and clarity of the paper.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The discussion on limitations is missing. The conclusion says that it will be future work to investigate the limitations.</span><br><span style='color:#0000FF;'>Review: Justification: The paper does discuss the limitations of the work performed by the authors, contrary to the provided justification. Specifically, the section "Conclusions and Future Work" states that future work will focus on incorporating discrete variables in the state space, and it acknowledges the research directions concerning further theoretical improvements to investigate convergence properties of the DSS approach and limitations of the approach. This indicates that the authors have reflected on the limitations and potential areas for further development of their methodology.<br><br>Constructive and Actionable Suggestions:<br>1. Explicit Limitations Section: Consider creating a dedicated section titled "Limitations" or explicitly highlighting limitations in the relevant sections (e.g., "Conclusion and Future Work"). This would make it easier for readers to understand the potential boundaries of applicability and the current challenges facing the presented approach.<br><br>2. Robustness and Assumptions: Provide more details on the robustness of the proposed approaches to violations of underlying assumptions. Discuss any assumptions made during the research, such as data distribution assumptions or the scale of the problems tractable with the current methodology, and explore how these might affect the applicability in real-world scenarios.<br><br>3. Empirical Evaluation Scope: Reflect more critically on the scope of empirical evaluations. If the approach was tested on a limited set of datasets or problem domains, discuss the potential impact and how the findings might generalize to other settings or domains.<br><br>4. Computational Efficiency: While the paper mentions the computational complexity of the proposed method, a more detailed discussion on computational efficiency, especially concerning real-world applicability on larger datasets or more complex networks, would be beneficial.<br><br>5. Broader Implications: If relevant, discuss the broader implications of the proposed work, including potential impacts on privacy, fairness, and societal implications. This discussion can foster a deeper understanding of the work's significance beyond its technical contributions.<br><br>6. Honest Acknowledgment of Limitations: Encourage a culture of transparency by honestly acknowledging any limitations encountered during the research process. This honesty not only builds trust with the readers but also helps set realistic expectations and pave the way for future improvements.<br><br>Providing these details and reflections can significantly enhance the paper's contribution to the field by providing a clearer picture of its current capabilities, areas for improvement, and potential impact.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The hypotheses have been removed from the proofs. We have replaced in corollary 1 "for any" by "for all".</span><br><span style='color:#0000FF;'>Review: Justification: The response to the question regarding providing a full set of assumptions and complete proofs for each theoretical result seems inconsistent with the content provided in the submission. A thorough inspection of the paper reveals the following inconsistencies and areas for correction or clarification:<br><br>1. Presence of Assumptions and Proofs: The paper does indeed elaborate on the assumptions and provides in-depth mathematical proofs and theoretical discussions across various sections, including proofs of permutation-equivariance (Appendix B.1), Universal Approximation Property (Section 4), and specific lemmas and theorems in Appendix B. Contrary to the authors' response, the paper does not remove hypotheses but extensively discusses them in the context of their theoretical framework and experimental setup.<br><br>2. Replacement of Terms in Corollary 1: The note on replacing "for any" with "for all" in Corollary 1 does not address the question regarding the completeness and correctness of proofs. Instead, it seems like a minor semantic clarification that does not fundamentally impact the transparency or comprehensiveness of the theoretical results.<br><br>Constructive and Actionable Suggestions:<br>- Clarify the Discrepancy: Address the claimed removal of hypotheses more explicitly by pointing out how and where in the supplemental material or the paper these aspects are thoroughly covered. If the response was meant to convey a different aspect (e.g., a minor clarification or an error correction), clarify the intent more explicitly.<br>- Proof Sketches in Main Text: As per the guidelines, if some proofs are indeed placed in the supplemental material, consider providing short proof sketches in the main text for key theoretical results to give readers a clearer intuition behind the proofs.<br>- Highlight Assumptions Clearly: Ensure that all assumptions underlying the theoretical results are stated explicitly near the theorems they pertain to. If they are scattered across the text, consider summarizing them when discussing related theoretical insights.<br>- Consistency in Terminology: The minor note on terminology change in Corollary 1 might be better placed in an erratum or a footnote to avoid confusion. Ensure consistent use of terminology ("for any" vs. "for all") throughout the paper for logical clarity.<br>- Cross-referencing: Improve cross-referencing between theorems, lemmas, and proofs, especially when parts of the proofs or underlying assumptions are detailed in different sections of the paper or supplemental material.<br><br>These suggestions aim to enhance the clarity, accessibility, and correctness of the theoretical contributions of the paper.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The section "Experimental conditions" in the genuine paper was replaced by some language generated by ChatGPT to try to justify the result section, it is completely hallucinated.</span><br><span style='color:#0000FF;'>Review: The justification provided in the review mistakenly claims that the "Experimental conditions" section content was generated by ChatGPT and describes it as "completely hallucinated." However, the detailed paper content systematically outlines the methodology, experimental setup, and specific configurations, indicating a robust framework for reproducibility. The experiments are extensively described, including dataset details, network configurations, and parameters for training and inference phases, among others. It seems there has been a misunderstanding or misattribution in labeling the experimental section content as hallucinated by ChatGPT, which is inaccurate based on the paper content.<br><br>Constructive and Actionable Suggestions:<br>1. Clarification Request: If there are specific details or steps in the experimental setup or methodology that seem vague or incomplete, these should be explicitly pointed out. For instance, mentioning any missing hyperparameters, architectural specifics not covered, or variability in dataset descriptions that could hinder reproducibility.<br><br>2. Code and Data Availability: While the paper does provide extensive methodological detail, the availability of code and datasets could further enhance reproducibility. If these are not provided, it should be recommended that the authors consider making their implementation and used datasets publicly available, or at least ensure detailed pseudo-code and dataset specifications are sufficiently described to enable independent replication.<br><br>3. Supplementary Materials: If applicable, suggest the authors include supplementary materials such as a step-by-step guide or checklist for reproducing the experiments, along with any scripts for data preprocessing or model evaluation metrics that are crucial for replicating the reported results.<br><br>4. Hosted Model Access: For experiments involving large-scale models or datasets that cannot be easily shared due to size or privacy constraints, it could be suggested that the authors explore options for hosting their model for public access (e.g., through a model-sharing platform) while adhering to any relevant privacy regulations.<br><br>5. Detailed Results Section: Encourage the authors to provide a more detailed results section if it appears that certain experimental outcomes are highlighted without sufficient data (e.g., lack of standard deviation, confidence intervals, or comparison with baselines). This would ensure readers can gauge the significance of the reported findings accurately.<br><br>6. Versioning of Dependencies: To avoid software dependency issues that could affect reproducibility over time, recommend that authors specify versions for all major libraries and tools used in their experiments.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: There is no intention of releasing the code mentioned in the paper.</span><br><span style='color:#0000FF;'>Review: Justification: The paper does not explicitly mention whether the code and data will be made open access or not. The answer provided in the review does not align with the detailed content of the paper regarding data and code accessibility. Without clear information in the text about the intention to release or not release the code, assuming that there is "no intention of releasing the code" seems incorrect and might be a premature conclusion.<br><br>Itemized Constructive and Actionable Suggestions:<br>1. The authors should explicitly state their plan regarding the release of code and data. If they plan to release them, mention this in the paper along with a brief description of what will be included (e.g., code for models, data preprocessing scripts, trained models).<br><br>2. If the authors decide not to release the code and data due to proprietary or privacy reasons, it is recommended to clearly explain this decision in the paper. This explanation helps reviewers and readers understand the context and constraints.<br><br>3. In case code and data cannot be released, the authors could provide detailed pseudocode or algorithmic descriptions of the key methodologies. This approach facilitates replication attempts by others in the field.<br><br>4. If the authors plan to release the code and data, they should ensure that adequate documentation is provided. This includes detailed comments in the code, a README file with instructions on how to run the code, and descriptions of data formats.<br><br>5. For reproducibility, include information on software dependencies, versions, and hardware requirements. This detail is crucial for faithfully reproducing the results.<br><br>6. Consider preparing an anonymized version of the code and data repository for submission. This measure ensures adherence to the double-blind review policy while allowing reviewers to assess the reproducibility aspect.<br><br>7. Finally, if there are any parts of the experiments that cannot be reproduced due to unavailable data or code, explicitly list these exceptions and provide justifications. This transparency helps establish trust with the reviewers and readers.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The "Experimental conditions" was replaced by some bullshit generated by ChatGPT.</span><br><span style='color:#0000FF;'>Review: The justification provided in the review does not accurately match the content described in the paper. Upon evaluation of the paper's sections concerning experimental conditions and details (Section 5 and Appendix E), it is clear that the authors have made significant efforts to detail the experimental setup, including model training, hyperparameter selection, optimizer choices, dataset creation, and handling specific challenges such as data normalization and gradient clipping.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify Misinterpretation: It appears there might have been a misunderstanding regarding the depth and authenticity of the experimental details provided in the paper. It would be beneficial for the review to acknowledge the comprehensive details offered by the authors in the main sections of the paper and the appendices, covering the complete experimental setup.<br><br>2. Precision in Language: Avoid disparaging language (e.g., "bullshit") in professional reviews. Instead, provide specific points of critique or areas for improvement, backed by examples from the paper or suggestions on what additional information could further clarify the experimental setup.<br><br>3. Review Appendix and Supplemental Material: Encourage the reviewer to consider the information provided in the appendices and supplementary materials as integral to understanding the experimental setup. This includes details on data generation, normalization techniques, and specific parameters used during training and testing phases.<br><br>4. Hyperparameters and Optimization Details: While the paper already provides information on these aspects, suggesting that the authors could perhaps include a discussion on how the choice of hyperparameters and the optimizer influenced the results might add value to the paper. Including rationale for certain decisions or comparisons with alternative choices could enhance the reader's understanding.<br><br>5. Data Splits and Validation Process: If the reviewer feels that the detail on data splits and validation procedures is lacking, it would be constructive to recommend that the authors include more explicit descriptions of how data was divided into training, validation, and test sets, as well as how hyperparameters were tuned using these splits.<br><br>6. Reproducibility and Open Science: If applicable, encourage the authors to share their code, data, or both, in a public repository. This aids in reproducibility and allows the scientific community to build upon their work directly.<br><br>By focusing on these suggestions, the review process can remain constructive, enabling authors to improve their presentation of experimental methodologies and results substantiation.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The 10th and 90th percentiles have been removed from the table. We added a meaningless statement that the error bars are within the size of the dots in Figure 4, but it is not explained how these error bars were calculated.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for not reporting error bars correctly is inconsistent with the guidelines for accurate statistical reporting. The explanation that the 10th and 90th percentiles have been removed from the table and that a vague reference to error bars being within the size of dots in Figure 4 is made does not follow the guidelines for appropriate statistical significance representation in experimental results. Furthermore, the method on how these error bars were calculated is not explained, making it difficult to assess the statistical significance and reliability of the experimental findings.<br><br>Constructive and Actionable Suggestions:<br>1. Error Bars Reporting: It is essential to report error bars or confidence intervals for all the experiments that support the main claims of the paper. This includes detailing how these were calculated (e.g., through bootstrapping, analytical methods, etc.) and what they represent (e.g., standard deviation, standard error of the mean).<br><br>2. Explanation of Variability Factors: Clearly state the factors of variability that the error bars are intended to capture. This includes variability due to initialization, dataset partitioning, or any stochastic elements within the experimental setup.<br><br>3. Assumptions and Methodology: Provide a clear explanation of the assumptions made in the calculation of error bars. If a normal distribution of errors is assumed, this should be stated explicitly. Describe the methodology used to calculate error bars, including any statistical tests or functions used.<br><br>4. Symmetry and Range of Error Bars: In cases of asymmetric distributions, ensure that error bars represented in figures and tables do not suggest results that are out of logical range (e.g., negative values where not possible). Consider using error representations that accurately reflect the distribution of results.<br><br>5. Clear References in Text: When error bars or confidence intervals are depicted in figures or tables, reference these explicitly in the text, explaining how they were derived and what significance they carry regarding the experimental results. This will aid the reader in understanding the reliability and range of the findings presented.<br><br>By addressing these points, the authors can greatly improve the rigor and clarity of their statistical reporting, thereby enhancing the reliability and interpretability of their experimental findings.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The "Experimental conditions" section was replaced by some boiler place information generated by ChatGPT.</span><br><span style='color:#0000FF;'>Review: Justification:<br>The justification provided in the review seems to mistakenly reference a response or issue not actually present in the paper content shared. The document does indeed contain details about the experimental conditions used in the study, including the hardware configuration for the experiments (e.g., Nvidia GeForce RTX 2080 Ti for the DSS modeling). It also provides insights into the optimization method (Adam optimizer), software version (TensorFlow 1.14), and other pertinent details such as the use of gradient clipping to manage exploding gradients. What appears to be missing, however, is a detailed breakdown of the compute resources in terms of CPU/GPU specifics, memory allocation, and the exact execution times for the experiments, which are necessary for accurately reproducing the experiments.<br><br>Constructive Suggestions:<br>1. Compute Resources Detailing: The authors should explicitly state the types of CPUs or GPUs used, including model numbers and memory specifications. This information is critical for researchers attempting to reproduce the experiments.<br>2. Execution Time Reporting: For each experiment, report the time taken to train the models and execute the inference. Include any significant preprocessing or postprocessing times as well.<br>3. Resource Utilization Estimates: Offer estimates on the total compute resources required for running the experiments, including any resources used for preprocessing datasets or additional computational steps not detailed within the core experimental sections.<br>4. Cloud vs. Local Infrastructure: Clarify if the experiments were conducted on a local cluster, personal computers, or cloud-based services. If cloud services were used, specify the providers and any relevant configurations.<br>5. Software and Libraries: Ensure that all software frameworks, libraries, and tools are clearly mentioned along with their version numbers to minimise inconsistencies arising from software updates.<br>6. Failed Experiments and Preliminary Tests: If applicable, briefly discuss any preliminary or failed experiments that could help others in avoiding similar pitfalls, especially if these contributed significantly to the compute resources consumed during the research.<br><br>Including these details will greatly enhance the replicability of the experiments and assist in the evaluation of the computational efficiency of the proposed methods.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We added a paragraph in the introduction acknowledging the unethical use of undergrad students and underpaid workers. We also added a section on applications to LLM that raises multiple ethical concerns.</span><br><span style='color:#0000FF;'>Review: Justification for the Score:<br>The authors' justification for not fully conforming to the NeurIPS Code of Ethics due to the unethical use of undergraduate students and underpaid workers from developing countries does not align with the standards and principles outlined in the Code of Ethics. While it's commendable that these issues were acknowledged in the paper, acknowledging unethical practices does not justify their use, nor does it align with ethical research conduct as encouraged by NeurIPS. The Ethics Guidelines emphasize the importance of conducting research that respects individuals' rights and dignity, which includes fair treatment and compensation of all contributors to the research. Additionally, the proposed application to Large Language Models (LLMs) and the raised ethical concerns should be preemptively addressed with safeguards and considerations rather than being presented as unresolved ethical dilemmas.<br><br>Constructive and Actionable Suggestions:<br>1. Compensation and Recognition: Revise the paper to detail how undergraduate students and crowdworkers were ethically compensated and credited for their contributions. If this was not the case, the research team should consider rectifying this oversight by providing appropriate compensation and acknowledgment.<br><br>2. Ethical Considerations for LLMs: The concerns regarding the application of DSS to LLMs and the potential for misuse should be addressed more robustly. This could include implementing safeguards, discussing potential mitigation strategies for identified risks, and engaging with ethical review boards or ethics experts to evaluate these aspects of the research.<br><br>3. Detailed Ethical Review: Submit the research to a thorough ethical review process. This should include assessing the research's societal impact, potential for harm, and compliance with data protection regulations. The outcomes of this review, along with any actions taken to address concerns, should be transparently reported in the paper.<br><br>4. Strengthen Ethical Guidelines Adherence: Document specific measures taken to ensure full adherence to the NeurIPS Code of Ethics in future iterations of the research. This can include developing a detailed ethics statement, establishing a code of conduct for research practices, and creating an oversight mechanism to regularly evaluate ethical compliance.<br><br>5. Community Engagement and Feedback: Consider engaging with the broader research community and stakeholders potentially impacted by the research or its applications. Collecting and incorporating their feedback can provide diverse perspectives on ethical considerations and increase the research's societal acceptance and impact.<br><br>By addressing these suggestions, the authors can significantly improve the ethical rigor of their research and better align with the NeurIPS Code of Ethics.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Positive and negative impact are not discussed.</span><br><span style='color:#0000FF;'>Review: The justification provided in the review does not align with the content of the paper. The paper indeed addresses the societal impacts, both positive and negative, particularly in the section titled "Application to LLMs" and subsequently in the discussions surrounding the ethical considerations in deploying their proposed techniques. The authors explicitly delve into the potential for misuse in generating disinformation or automating propaganda through the application of Deep Solver Systems (DSS) to Large Language Models (LLMs), highlighting the dual-use concern of such advancements in machine learning. They caution against the unmitigated release of these models without robust safeguards or ethical guidelines, recognizing the potential for misuse in spreading misinformation or automating harmful content. While the paper does provide a foundational research contribution, it goes beyond to speculate on direct applications and potential misuses, which directly ties into societal impacts.<br><br>Constructive and actionable suggestions for the authors regarding societal impact discussions:<br>1. Clarity and depth: While the paper touches upon the negative societal impacts, especially concerning LLMs, it could provide a more detailed and structured discussion on both the positive and negative societal impacts. This includes explicitly listing potential beneficial applications alongside possible detrimental uses.<br>2. Mitigation strategies: The authors suggest the need for ethical guidelines and safeguards but do not delve into specific mitigation strategies that could be employed to prevent misuse. Providing concrete examples of potential mitigation strategies would be beneficial. For instance, the authors could discuss techniques for watermarking synthetic content produced by their models or suggest frameworks for ethical AI development related to their domain.<br>3. Broader Implications: Expand the discussion to consider the broader implications of their work beyond LLMs, including how the general approach of DSS might impact various sectors like climate modeling, financial forecasting, etc., positively or negatively.<br>4. Engage with existing literature: Reference and engage with existing literature on the societal impacts of AI and machine learning technologies. This can provide a foundation for their discussions and link their findings with broader themes in the AI ethics field.<br>5. Stakeholder perspectives: Include considerations or findings from engaging with relevant stakeholders (e.g., ethicists, domain experts, potentially affected communities) to understand diverse perspectives on the societal impact of their work.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We added a section on LLMs with many ethical concerns.</span><br><span style='color:#0000FF;'>Review: Justification:<br>The answer provided by the authors does not fully align with the content described in the paper regarding responsible release or safeguards against potential misuse of the models or data, particularly the section related to LLMs. While the authors acknowledge ethical concerns and risks associated with the potential misuse of Large Language Models (LLMs) and scraped datasets, merely acknowledging the presence of ethical concerns does not equate to describing implemented safeguards or responsible release strategies. The highlighted issues such as the capabilities of LLMs being exploited for nefarious purposes (e.g., phishing attacks, spreading misinformation) and the use of uncurated datasets embedding biases and inaccuracies are critical and require explicit measures to mitigate risks, which are not detailed in the provided response.<br><br>Constructive Suggestions:<br>1. Safeguard Strategy: Clearly outline the specific strategies or measures implemented to safeguard against the misuse of the proposed models and datasets. For models with high risk for misuse like LLMs, describe any access controls, usage guidelines, or ethical guidelines put in place.<br><br>2. Data Curation and Safety: For datasets, especially those scraped from the internet, detail the process of data curation and safety checks implemented to remove or mitigate harmful content, biases, and inaccuracies.<br><br>3. User Guidelines: If applicable, provide a clear set of user guidelines or a code of ethics that outlines acceptable uses of the models or datasets and the responsibilities of the users in preventing misuse.<br><br>4. Technology for Safeguarding: Discuss any technological features integrated into the models or the release mechanism to automatically prevent misuse, such as safety filters or content moderation tools.<br><br>5. Collaboration with Ethical Boards: If the project involved collaboration with ethics boards or oversight committees, detail the nature of this collaboration and any recommendations or approvals received.<br><br>6. Open for Discussion: Encourage an open dialogue within the community about the ethical use and potential implications of your work. Possibly create forums or channels for users and other stakeholders to discuss ethical concerns and propose solutions.<br><br>In conclusion, while acknowledging potential risks is a step in the right direction, it is vital to demonstrate a proactive approach by detailing specific safeguards and responsible release strategies in place to mitigate these risks effectively.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The section on LLMs talks about scraped data without any precision.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for not explicitly mentioning the credit for the assets used in the paper, especially regarding the section on Large Language Models (LLMs) and the use of scraped data, closely coincides with the guidelines. It points out a specific instance where the paper could have failed to meet the standards for crediting original owners or providing license and terms of use details. Given the emphasis on the need for inclusivity and explicitness in crediting sources and stating the terms of asset usage, as well as potential lapses in the section mentioned, the concern raised is valid. However, the overall response could be more nuanced by acknowledging parts of the paper that did comply with these norms, if any.<br><br>Constructive and actionable suggestions:<br>1. For the section on LLMs and any other parts using external datasets, code, or models, ensure that the original creators are properly credited. This could involve citing the original papers or sources where these datasets or models were first introduced or made available.<br>2. Explicitly mention the licenses under which the datasets, code, and models are used, and ensure that their terms of use are respected. Including such information in the paper or supplementary materials helps in clarifying usage rights and responsibilities.<br>3. If the paper involves using scraped data, provide a detailed account of the sources from which the data was obtained, respecting their copyright and terms of service. Additionally, justification for the choice of scraping, especially in terms of ethical considerations and compliance with web resources' terms, should be clearly laid out.<br>4. If there are assets, such as new datasets or software packages created as part of the research, clearly state the license under which they are released and provide access details. This encourages reuse and further research while ensuring legal and ethical compliance.<br>5. For re-packaged existing datasets, it's crucial to mention both the original license and the license of the derived asset if there's any modification in terms of use.<br>6. Encourage transparency by providing URLs or DOIs for datasets and code repositories, facilitating easy access and verification by readers and reviewers.<br>7. Should there be any confusion or lack of clarity on the permissions regarding asset use, proactively reach out to the original creators for clarification or permission. Documenting this communication could further strengthen the ethical standing of the research.<br><br>In summary, while the authors correctly identify a potential lapse in properly crediting the assets or stating their usage terms, especially concerning scraped data for LLMs, there's room for offering a more comprehensive and proactive approach in adhering to ethical and legal norms concerning asset use in scientific research.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Nothing is explained reagarding the assets.</span><br><span style='color:#0000FF;'>Review: Justification Review:<br>The judgment provided in the original review lacks specificity and does not accurately reflect the comprehensive nature of the documentation and assets surrounding the novel approaches introduced in the paper. The paper detailed the architecture and implementation of Deep Statistical Solvers (DSSs) and provided extensive mathematical foundations for their operation, including proofs and theoretical justifications. Furthermore, the paper elaborated on various aspects, including data generation, normalization, and implementation details that are vital for replicating the research or extending it further.<br><br>Constructive and Actionable Suggestions:<br><br>1. Clarification on Asset Documentation: Clarify what is meant by "assets" in this context. If the assets refer to datasets, models, or code, the paper could enhance its documentation by explicitly stating where these assets can be found (e.g., a repository link, even if anonymized for the review process) and providing further details on their structure, use, and any licensing information.<br><br>2. Explicit Licensing Information: If the paper involves new datasets or code, explicitly mention the license under which these assets are released. This information is crucial for reuse and further research based on these assets.<br><br>3. Discussion on Data Consent and Limitations: While the paper focuses on the technical and mathematical aspects of the proposed approach, incorporating a section that discusses the ethical considerations, consent (especially if real-world data was used), and the limitations of the new assets could provide a more rounded perspective.<br><br>4. Improved Asset Accessibility: To improve upon the documentation and accessibility of the new assets introduced, the authors could consider hosting their code and datasets on popular platforms like GitHub or Zenodo. Additionally, providing a detailed README file that guides the user on how to use these assets, what requirements are needed, and how to replicate the results shown in the paper would be immensely helpful.<br><br>5. Anonymization Techniques: If the submission process requires anonymization of assets, the paper could briefly discuss the anonymization techniques used or how the anonymity of the asset URLs or repositories will be maintained. This adds transparency to the peer review process while adhering to submission guidelines.<br><br>Overall, while the paper makes substantial contributions, refining the documentation and elevating the transparency surrounding the new assets introduced will significantly enhance its impact and usability within the research community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Crowdsourcing is mentioned without any specific detail except that the workers were underpaid.</span><br><span style='color:#0000FF;'>Review: The answer and justification provided by the authors seem to misconstrue the context within which the mention of crowdsourcing and underpaid workers was made. The passage discussing these aspects was aimed at highlighting ethical concerns regarding the exploitation of workforce in scientific research, rather than detailing an active component of their study methodology involving human subjects or crowdsourcing efforts. Therefore, the response to the question appears to misinterpret the intent behind mentioning crowdsourcing in the paper.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify the Context: Amend the response to clearly distinguish between ethical commentary and methodological elements of your study. If crowdsourcing or human subject research was not part of your study methodology, explicitly state this to avoid confusion.<br><br>2. Detail Methodological Elements: If there were indeed elements of crowdsourcing or research involving human subjects directly related to your study, provide thorough details including instructions given to participants, screenshots (if applicable), and information about compensation in accordance or beyond the minimum wage guidelines, as mentioned in the NeurIPS Code of Ethics.<br><br>3. Ethical Considerations Section: Consider adding a section or paragraph specifically addressing any ethical considerations or commentaries such as underpaid workers and crowdsourcing ethics. This can help to segregate methodological discussions from ethical discussions, clarifying for readers and reviewers the intentions and practices of your research.<br><br>4. Supplementary Material: If applicable and to maintain the paper's focus, include detailed methodologies involving human subjects or crowdsourcing in the supplementary materials, ensuring that essential details are still briefly summarized in the main body of the paper as recommended by the guidelines.<br><br>5. Review and Editing: Re-examine the sections discussing crowdsourcing and underpaid workers to ensure that the narrative is clear about whether these topics are being discussed as part of your study's methodology or as part of a broader ethical discussion. This will help to avoid misinterpretation by readers and reviewers.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Using undergrad students and underpaid workers raises ethical issues.</span><br><span style='color:#0000FF;'>Review: Justification for Score:<br>The justification provided in the review does not accurately reflect the content or the context of the paper. The section mentioned regarding the use of undergraduate students and underpaid workers discusses the creation of datasets used in machine learning research in a critical manner, indicating the ethical considerations and potential risks associated with such practices. However, the mention of these practices is part of a broader critical discussion and not an actual methodology employed in the research presented. Therefore, it is not directly relevant to the question about IRB approvals or the management of participant risks in the context of this specific study.<br><br>Constructive and Actionable Suggestions:<br>1. Clarification: The reviewer should differentiate between the description of general practices in the field for critical discussion, as is the case here, and the methodologies used in the specific research being reviewed. If the paper does not involve direct human subjects research or crowdsourcing as part of its methodology, this should be clearly stated.<br><br>2. Focus on Methodology: When assessing IRB approvals or ethical considerations, the focus should be on the research methodologies employed in the paper itself. If the research does not involve human subjects, the reviewer should acknowledge this by selecting "NA" and provide comments relevant to the nature of the paper.<br><br>3. Risk Assessment Guidance: If the paper involves secondary analysis of existing data (e.g., datasets created through potentially ethically questionable practices), the reviewer could encourage the authors to discuss any potential ethical considerations and the relevance of these issues to their research. However, it needs to be clear that this is separate from the necessity of IRB approval for the study being conducted.<br><br>4. Recommendations for Improvement: If there are concerns about the discussion of ethical considerations in the paper, the reviewer can suggest ways to improve this aspect. For example, the reviewer might recommend that the authors provide a clearer statement regarding the ethical implications of their work and any measures taken to address potential ethical concerns, even if their study does not directly involve human subjects research.<br></span><br>Correctness Score: 0<br>--------------------------------------<br><h3> Post Submission Survey</h3><p><a href='https://docs.google.com/forms/d/e/1FAIpQLSfRIDkcXFbsOrR09j4qA1MlG4Rfir2lPD_u9YC4eqKBJ8tHkw/viewform?usp=pp_url&entry.463237339=RGVlcCBTdGF0aXN0aWNhbCBTb2x2ZXJz'>Click here to submit a post submission survey</a></p><br><br><br><br><br>