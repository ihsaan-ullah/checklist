Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the paper's main contributions and scope.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the abstract and introduction do not include the claims made in the paper.
The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
","## Feedback on Authors' Proposed Answer

The Authors' Proposed Answer of ""Yes"" is mostly accurate, but the Justification lacks specifics and could be greatly improved. While the abstract and introduction do mention the paper's contributions and scope, they do not comprehensively cover all the key aspects and nuances discussed within the paper. 

Here are some actionable improvements to consider:

* **Specificity in Justification:** Instead of a generic statement, point to the exact sentences or paragraphs within the abstract and introduction that explicitly state the main claims, contributions, and scope. For example, ""The abstract's second sentence highlights the paper's main contribution of introducing a series of RL-based meta-learning challenges focusing on learning from learning curves. The introduction further elaborates on this in the third paragraph and defines the scope of the research by emphasizing the understudied nature of meta-learning from learning curves.""
* **Addressing Limitations:**  The abstract and introduction currently lack a clear mention of the paper's limitations. Consider adding a sentence acknowledging that the study primarily focuses on pre-computed learning curves and mentioning the challenges associated with generalizing findings to real-world scenarios with dynamic learning curve generation. 
* **Highlighting Key Findings**: The abstract could be enhanced by briefly mentioning some key findings from the first round of the challenge, such as the success of learned policies for algorithm selection and budget allocation. 

**",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,"The limitations are discussed in the section “Lessons learned and new design"".","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
The authors are encouraged to create a separate ""Limitations"" section in their paper.
The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
","## Feedback on Limitations Discussion

While the paper mentions changes made in the second round based on limitations of the first round design, there is no dedicated ""Limitations"" section that comprehensively discusses the limitations of the proposed approach and its results.  

Here are some key areas to improve:

* **Assumptions:** Clearly articulate any assumptions made in the challenge design or evaluation (e.g., monotonically increasing learning curves) and discuss their potential impact on the results and generalizability.
* **Scope of Claims:**  Address the limited scope of the study in terms of the number of datasets and algorithms used. Reflect on how results might vary with different datasets or a broader range of algorithms.
* **Factors Influencing Performance:** Discuss factors besides algorithm selection and budget allocation that can affect performance (e.g., specific dataset characteristics, meta-features, hyperparameters). 
* **Computational Efficiency:** Analyze the computational cost of the proposed approaches and how they scale with increasing dataset size and complexity. 
* **Privacy and Fairness:** If applicable, discuss potential concerns related to privacy and fairness in the context of meta-learning from learning curves.

Adding a dedicated ""Limitations"" section with these considerations would significantly strengthen the paper.

",0.5
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,This paper does not include theoretical results.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include theoretical results. 
All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced. 
","## Feedback for NeurIPS Checklist Question

The authors' proposed answer (""NA"") and justification (""This paper does not include theoretical results."") are accurate and align with the paper's content. The paper focuses on a meta-learning challenge and its results, not on presenting novel theoretical findings. 

However, there are opportunities to enhance the paper's clarity and connection to related theoretical work:

* **Discussion of theoretical foundations**: While the paper doesn't present new theoretical results, it could benefit from a more explicit discussion of the theoretical foundations underpinning meta-learning from learning curves. This could involve referencing relevant theoretical work on topics like Bayesian optimization, reinforcement learning, and learning curve analysis. 
* **Connecting empirical findings to theory**: The paper could strengthen its analysis by connecting the observed empirical results of the challenge (e.g., the success of specific strategies) to existing theoretical frameworks. This would provide a deeper understanding of why certain approaches are effective and could guide future research directions.

**",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,Code URLs and all information needed to reproduce the experimental results are given.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
    - If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
    - If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
    - If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
    - We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

","## Feedback for Reproducibility Question 

The provided ""Justification"" only mentions code URLs, but doesn't specify the location of information needed to reproduce the experimental results.  While code is helpful, the paper should also detail:

* **Hyperparameters:**  Clearly state all hyperparameter values for the models and training processes. 
* **Datasets and Preprocessing:**  Describe the datasets used, including versions and pre-processing steps applied. If the data is private, explain how it can be accessed or provide similar public datasets.
* **Evaluation Metrics:**  Define the evaluation metrics and how they are calculated.
* **Environment Details:**  Specify software versions, libraries, and hardware used for the experiments.

These details should be included either within the main body of the paper or in a clearly designated appendix.

Including these details would strengthen the claim of reproducibility and improve the overall quality of the paper.

**",0.5
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,No,"The code for reproducibility is given, however, some data is saved private for longer benchmark.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
","## Feedback on NeurIPS Paper Checklist Answer

**Discrepancies:**

*   The justification states that ""some data is saved private for longer benchmark,"" which contradicts the “Guidelines” stating that papers cannot be rejected solely for not including code or data unless it is central to the contribution. As the challenge itself seems central, withholding data could be problematic.

**Actionable Feedback:**

1.  **Release the Data:** To align with the NeurIPS guidelines and the spirit of open science, strongly consider releasing the meta-datasets used in the challenge. This will allow for greater reproducibility and allow other researchers to build upon your work. 
2.  **Clarify Contribution:** If withholding the data is essential for the long-term viability of the benchmark, clearly explain in the paper why this is the case and how it directly relates to your core contribution. 
3.  **Alternative Benchmarking:**  If data release is impossible, explore alternative ways to offer a long-lasting benchmark, such as:
    *   Releasing a subset of the data.
    *   Providing a detailed description of the data generation process, allowing others to create similar datasets.
    *   Developing a data simulator that captures the key characteristics of the private data.

**",0
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,"These details are given in the section of “Challenge protocol"", as well as the public participant code.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental material.
","## Discrepancies and Actionable Feedback:

The author's proposed answer claims the paper specifies all necessary training and test details. While the justification points to the ""Challenge protocol"" section and participant code, the paper lacks specific details within the core text regarding:

* **Hyperparameter choices**: The paper mentions using algorithms with different hyperparameter values but doesn't explicitly state how these values were chosen. 
* **Optimizer details**: The type of optimizer used for training the algorithms is not mentioned. 
* **Data split specifics**: While the paper mentions a k-fold meta-cross-validation, the exact split ratio (e.g., number of datasets for meta-training and meta-testing) is unclear.

## Recommendations:

1. **Include a table summarizing hyperparameters**: Create a table listing the algorithms, their hyperparameters, and the rationale behind the chosen values. This table can be placed within the core text or the appendix.
2. **Specify the optimizer used**: Briefly mention the type of optimizer (e.g., SGD, Adam) used for training the algorithms. 
3. **Clarify data split details**: State the exact number of datasets used for meta-training and meta-testing in the k-fold meta-cross-validation.

## ",0.5
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,No,We didn't explicitly discuss the error-bars.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error of the mean.
It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
","## Feedback on Error Bar Reporting

The authors correctly identified that the paper does not explicitly discuss error bars and answered ""No"" to the question. However, the justification provided is insufficient and lacks details on why error bars were not included. 

Here's how to improve the paper and the answer to the checklist question:

**Actionable Feedback:**

1. **Include Error Bars or Alternatives:** The paper should incorporate measures of statistical significance, such as error bars, confidence intervals, or p-values from significance tests. This is crucial to understand the variability and reliability of the reported results, especially for the main claims.
2. **Justify Omission (If Applicable):** If including error bars or alternatives is infeasible due to computational limitations or other valid reasons,  clearly explain these limitations in the paper.  Provide a detailed justification for the omission, such as outlining the specific challenges encountered.
3. **Specify Variability Factors:**  Clarify the sources of variability captured by the chosen measure of statistical significance (e.g., train/test splits, initialization randomness).
4. **Explain Calculation Method:**  Describe how the error bars or other measures were calculated, including formulas, libraries used, or bootstrapping techniques employed.
5. **State Assumptions:**  Explicitly state any assumptions made during the calculation of the statistical measures (e.g., normality of errors). 
6. **Define Error Bar Type:** Clearly define whether the error bars represent standard deviation or standard error.
7. **Textual Explanation:** Discuss the included measures of statistical significance in the main text. Reference the corresponding figures and tables where these measures are presented.

**",0
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,No,"The compute resource is not explicitly discussed, we only gave the fact that the competition is running on Codabench platform, and all methods are imposed under a given time limit.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
","## Feedback on NeurIPS Paper Checklist Answer

The authors' proposed answer (""No"") and justification accurately reflect the paper's content regarding the lack of explicit information on computer resources. However, the justification could be strengthened by mentioning the time limit imposed on all methods as a mitigating factor, demonstrating awareness of reproducibility concerns.

**Significant Improvements:**

1. **Quantify Compute Resources:** While mentioning the Codalab platform is helpful, the paper should specify the type of compute workers (CPU/GPU), memory limits, and typical execution times observed during the competition. This information will allow researchers to estimate the feasibility of reproducing the experiments and compare the computational efficiency of different approaches. 
2. **Address Total Compute and Unreported Experiments:**  The paper should acknowledge whether the full research project involved additional computational resources beyond the reported experiments. This transparency ensures a complete understanding of the project's computational footprint and avoids misleading impressions.

**",0.5
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We reviewed the code of ethics and our paper conforms with it.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
","## Discrepancies and Feedback for NeurIPS Paper

The authors claim the paper conforms to the NeurIPS Code of Ethics, but the justification only mentions a review of the code without specific examples of adherence within the paper.  This raises concerns and requires further investigation.

### Actionable Feedback:

1. **Demonstrate Ethical Considerations:**  Identify and discuss relevant ethical aspects present in the research. This could include:
    * **Data Bias:** Analyze potential biases within the datasets used for the challenge and their impact on results. 
    * **Fairness in Algorithm Selection:**  Discuss if the meta-learning process could lead to unfair or discriminatory algorithm choices for specific tasks or datasets. 
    * **Environmental Impact:**  Given the focus on computational resource efficiency, address the environmental impact of the proposed methods and potential trade-offs. 
2. **Transparency and Reproducibility:** 
    * While the code is open-sourced, clarify the availability and accessibility of the meta-datasets. If they remain private for benchmarking purposes, explain the rationale and consider potential alternatives for broader research access.  
    * Emphasize the measures taken to ensure reproducibility, such as fixed random seeds and detailed documentation.

**",0.5
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"Our paper explores the domain of meta-learning, a collection of methodologies designed to rapidly identify the most effective algorithm for a given task. We discuss its positive social impact as it reduces human intervention and computational cost.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
","## Feedback on Societal Impact Discussion

The provided justification focuses solely on the positive societal impacts of meta-learning, specifically reduced human intervention and computational costs. However, the paper does not discuss any potential negative societal impacts. 

Here are some actionable improvements based on the guidelines:

* **Identify potential negative impacts:** Explore and discuss potential negative societal consequences of meta-learning. This could include:
    * **Misuse potential:** Could meta-learning be used to develop more efficient malicious algorithms?
    * **Bias and fairness:** Could meta-learned models inherit and amplify biases present in training data? 
    * **Job displacement:** Could widespread adoption of meta-learning automate tasks currently performed by humans, leading to job losses?
* **Discuss mitigation strategies:** If negative impacts are identified, propose potential mitigation strategies.  Examples include:
    * **Algorithmic bias detection and mitigation techniques.**
    * **Developing ethical guidelines for meta-learning research and application.**
    * **Policies addressing potential job displacement caused by automation.**
* **Point to relevant sections:** If the paper is revised to include discussions on negative societal impacts and mitigations, update the justification to point to specific sections where this information can be found.

**",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,Our paper poses no such risks.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").
""
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
","## Feedback on NeurIPS Paper Checklist Answer

The authors proposed ""NA"" as the answer to the question regarding safeguards for responsible release of data or models. While the justification (""Our paper poses no such risks."") is technically correct as the paper doesn't involve releasing models or data, there's a missed opportunity to enhance the paper's quality and impact.

**Suggested Improvements:**

* **Acknowledge Potential Risks:** Briefly discuss potential risks, even if minimal, associated with the challenge design or the meta-learning techniques explored. For instance, biases in the meta-dataset or the agents could lead to unfair algorithm selection.
* **Promote Responsible Research:**  Dedicate a short section or paragraph in the conclusion to emphasize the importance of responsible meta-learning research. This could include discussing potential mitigation strategies for identified risks or suggesting best practices for future research in this area. 
* **Transparency and Reproducibility:** While the paper mentions open-sourcing the software and maintaining the meta-dataset privacy for benchmarking, it would be beneficial to elaborate on the rationale behind these choices and their implications for research reproducibility and accessibility. 

  
These additions would demonstrate the authors' awareness of responsible AI principles and contribute to the broader discussion on ethical considerations in meta-learning research.

**",0.5
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"creators or original owners of assets (e.g., code, data, models), used in the paper are cited.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
If this information is not available online, the authors are encouraged to reach out to the asset's creators.
","## Discrepancies and Actionable Feedback:

The authors claim they cite the creators/owners of the assets used. However, the provided paper excerpt does not offer concrete evidence of this, such as specific citations or license names for the datasets or algorithms used (e.g., 30 cross-domain datasets from the AutoML challenge). 

Here are some actionable improvements based on the guidelines:

* **Cite datasets and algorithms explicitly:** Include clear references to the original papers or sources of the datasets and algorithms used. For example, for the 30 cross-domain datasets, provide a proper citation and ideally a URL or version information.
* **State licenses:** Mention the specific license (e.g., CC-BY 4.0) under which each dataset or algorithm is distributed. This allows for proper use and attribution.
* **Website scraping information:** If data was scraped from websites, provide the copyright and terms of service information for those websites.

**",0
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,"This is given in the challenge website, whose link is given in the paper.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
The paper should discuss whether and how consent was obtained from people whose asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
","## Feedback on Documentation of New Assets

The author's proposed answer of ""Yes"" is **not accurate** and the justification provided is **misleading**. While the paper mentions a challenge website, it does not explicitly state that the website contains documentation for the new assets (meta-datasets) introduced. Additionally, relying solely on an external website for documentation is not ideal as it can lead to accessibility and permanence issues.

Here are some actionable improvements based on the guidelines:

*   **Clearly state the availability and location of the documentation for the meta-datasets within the paper itself.**  This can be done by adding a sentence or two to the relevant sections, such as the data description or challenge protocol sections. 
*   **Provide a brief overview of the documentation content within the paper.** This could include information about the format, organization, and key elements of the documentation. 
*   **Consider including essential information about the meta-datasets directly in the paper.** This could involve details about the generation process, data splits, key statistics, and any limitations.
*   **Address the issue of consent for the use of the datasets.** If real-world datasets are used to create the meta-datasets, discuss whether and how consent was obtained from the individuals involved.

**",0
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
","## Feedback on NeurIPS Checklist Question Response

The authors' proposed answer of ""NA"" with the justification ""This paper does not involve crowdsourcing nor research with human subjects"" appears accurate based on the provided paper abstract and content. The paper focuses on a meta-learning challenge involving algorithms and datasets, with no mention of human subjects or crowdsourcing.

However, there might be an opportunity to improve the paper's clarity and contribution by briefly mentioning the absence of human subject research in the main paper. This could be a simple addition to the ""Background and motivation"" section, stating something like:

>""This work focuses on the automated analysis and selection of algorithms based on learning curves, and does not involve any experiments with human subjects.""

This addition would explicitly address ethical considerations and potential reader assumptions, further solidifying the paper's focus and scope. 

## ",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
","## Feedback on IRB Statement for NeurIPS Paper

The authors have correctly identified that their paper does not require IRB approval as it does not involve human subjects research or crowdsourcing. However, some minor improvements can be made to enhance clarity and adherence to the guidelines:

* **Explicitly state IRB non-applicability:** While the justification mentions the absence of human subjects research, directly stating ""IRB approval is not applicable"" in the answer would be more explicit and aligned with the expected format.
* **Referencing the NeurIPS Code of Ethics:** Consider adding a sentence in the justification acknowledging the authors' commitment to the NeurIPS Code of Ethics, demonstrating awareness of ethical considerations even when IRB approval is not required.

**Example of improved answer and justification:**

**Answer: NA**

**Justification: This paper does not involve crowdsourcing or research with human subjects, and therefore IRB approval is not applicable. We adhere to the NeurIPS Code of Ethics in conducting our research.**


**",1
