Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,"We have mentioned main claims in Section 1: Introduction, calling them ""contributions"".","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the abstract and introduction do not include the claims made in the paper.
The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
","## Feedback on NeurIPS Checklist Answer

**Discrepancies:**

* **Justification Focus:** The justification focuses on the location of claims (""Section 1: Introduction"") rather than demonstrating alignment between abstract/introduction claims and the paper's actual contributions and scope.
* **Missing Specificity:** The justification lacks specific examples of claims and their corresponding evidence within the paper. 
* **Scope Consideration:** The justification doesn't address the scope aspect of the question, i.e., whether the abstract/introduction accurately reflect the breadth and limitations of the work.

**Actionable Improvements:**

1. **Claim Alignment:** Explicitly state the main claims from the abstract and introduction in the justification. 
2. **Evidence Mapping:** For each claim, point to specific sections, figures, or tables in the paper that directly support or demonstrate the claim. 
3. **Scope Discussion:** Briefly mention how the abstract/introduction convey the limitations of the work, including any assumptions made and the generalizability of results. 

**Example Revision:** 

> **Justification:** The abstract claims that LEAP net implements transfer learning to generalize to new domains without prior training examples. This is supported by the detailed explanation of the transfer learning approach and experimental results in Section 5. The introduction states the contribution of proposing the Statistical Solver Problem (SSP) framework, which is further defined and elaborated on in Sections 2 and 3. The limitations regarding the reliance on specific problem structures and the need for further investigation into generalizability are discussed in Section 7 (Conclusions and Future Work).

**",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,"We have indicated the limitations in the conclusion section, as future work.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
The authors are encouraged to create a separate ""Limitations"" section in their paper.
The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
","## Feedback on Limitations Discussion

The provided ""Answer"" and ""Justification"" have discrepancies with the paper's content and the guidelines:

* **Misinterpretation of ""Limitations""**: The justification confuses limitations with future work. While future work suggestions are valuable, they don't equate to discussing the current work's limitations.
* **Missing ""Limitations"" Section**: The guidelines encourage a dedicated section for discussing limitations, which is absent in the paper. 
* **Lack of Specific Limitations**: The paper lacks a reflection on assumptions, robustness, scope, influencing factors, computational efficiency, and potential privacy/fairness concerns, as advised by the guidelines.

Here's how to improve the paper's discussion of limitations:

1. **Create a ""Limitations"" Section**: Dedicate a section explicitly titled ""Limitations"" to discuss the shortcomings of the proposed approach. 
2. **Analyze Assumptions**: 
    * Identify and explicitly state the assumptions made in the model and methodology (e.g., specific graph structures, data distributions).
    * Discuss the potential impact of violating these assumptions on the results' validity (e.g., performance degradation with different graph types).
3. **Address Robustness and Scope**:
    * Acknowledge the limitations of the experimental evaluation, such as the number of datasets or experimental runs used.
    * Discuss thegeneralizability of the results beyond the tested scenarios and potential performance variations (e.g., with different problem sizes or noise levels).
4. **Computational Efficiency**: 
    * Analyze the computational complexity of the DSS training and inference processes.
    * Discuss how the algorithms scale with increasing dataset sizes and graph complexities.
5. **Potential Biases and Ethical Concerns (if applicable)**:
    * Discuss potential biases in the training data and their impact on the model's predictions. 
    *  Address any fairness or privacy concerns that may arise from the proposed approach, especially in sensitive application domains. 

**",0
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Yes,"We have provided clearly the hypotheses and a proof sketch in the main text, and detailed proofs in appendix.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include theoretical results. 
All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced. 
","## Feedback on NeurIPS Paper Checklist Answer

**Discrepancies:**

* **Missing Equation/Theorem Numbering:** The paper mentions providing hypotheses and proof sketches in the main text and detailed proofs in the appendix. However, there is no indication of equation or theorem numbering for easy cross-referencing. 
* **Unclear Assumption Locations:**  The justification mentions providing assumptions, but it's unclear if they are explicitly stated within the theorems or referenced. 

**Actionable Feedback:**

1. **Numbering:** Implement a consistent numbering system for all theorems, formulas, and proofs throughout the paper and appendix. This will improve clarity and facilitate cross-referencing. 
2. **Explicit Assumptions:** For each theoretical result (theorem), clearly state all assumptions directly within the theorem statement or provide clear references to where these assumptions are defined.
3. **Proof Sketch Clarity:** While the justification mentions providing proof sketches, ensure these sketches offer sufficient intuition and explanation for readers to grasp the core arguments of the proofs without needing to delve into the full appendix details. 

**",0.5
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,"We have provided all details necessary to reproduce the results in the ""Experimental conditions"" section.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
    - If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
    - If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
    - If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
    - We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

","## Discrepancies and Actionable Feedback:

The authors claim to have provided all necessary details for reproducibility in the ""Experimental conditions"" section. However, based on the paper and guidelines, several critical aspects are missing:

**Missing Information:**

*   **Training Details:** While mentioning the optimizer and hardware, specifics like learning rate schedule, loss function configuration, and data augmentation techniques are absent.
*   **Model Architecture:** The description of the GNN architecture lacks crucial details such as the type of GNN used, the number of layers in each block (Φ, Ψ, Ξ), activation functions, and the dimensions of hidden layers.
*   **Hyperparameter Selection:** The paper mentions using a validation set for hyperparameter tuning but doesn't specify the chosen values or the search process.
*   **Data Generation:** While describing the process for the Poisson equation, details are missing for the AC power flow experiments. This includes information on network topologies, load/generation scenarios, and data splitting procedures.
*   **Code and Data Availability:**  The paper doesn't mention whether the code or data are available, hindering full reproducibility.

**Actionable Feedback:**

1.  **Expand ""Experimental conditions"" Section:** Provide comprehensive details of the training process, including hyperparameters, architecture specifics, and data augmentation techniques. 
2.  **Clarify Model Architecture:** Specify the type of GNN, number of layers per block, activation functions, and hidden layer dimensions.
3.  **Document Hyperparameter Optimization:** Explain the hyperparameter search process and report the final chosen values. 
4.  **Elaborate on Data Generation:** Describe the data generation process for the AC power flow experiments, covering network details and data splitting.
5.  **Address Code and Data Availability:** Consider releasing the code and data to improve reproducibility and facilitate further research.

**",0
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,"Our code is made available in the supplementary materials, and links to the datasets can be found as references. In this version, to protect anonymization, we are not publishing the code on our GitHub repo, but this will be done for the final version.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
","## Feedback on NeurIPS Checklist Answer

**Discrepancies:**

*   The proposed answer states that code is available in the supplementary materials, but the justification mentions that code will only be published in the final version to protect anonymization. This is a direct contradiction and needs to be addressed.
*   The paper mentions providing links to datasets as references, but the guidelines require instructions on data access and preparation, including raw, preprocessed, intermediate, and generated data. Simply providing links as references is insufficient.
*   The justification doesn't point to specific sections where information on reproducing experiments can be found.

**Actionable Feedback:**

1.  **Clarify Code Availability:** Decide whether the code is available in the supplementary materials for review. If yes, update the justification and point to the specific section. If no, change the answer to ""No"" and provide a justification regarding anonymization. 
2.  **Improve Data Description:** Provide detailed instructions for data access and preparation, going beyond simply referencing links to datasets. Specify how to access and utilize various stages of the data (raw, preprocessed, etc.). 
3.  **Enhance Reproducibility Instructions:** Offer precise instructions for reproducing experimental results. Include exact commands and environment details needed to run the code. If only a subset of experiments is reproducible, explain the omissions.
4.  **Point to Relevant Sections:**  In the justification, explicitly mention the sections within the supplementary materials where reviewers can find code, data instructions, and reproducibility details.

**",0
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,"The ""Experimental conditions"" section provides all details.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental material.
","## Feedback on NeurIPS Paper Checklist Answer

**Discrepancies:**

*   The justification points to an ""Experimental conditions"" section, but the paper uses the heading ""Experimental settings"" for the relevant sections in both experiments (5.1 and 5.2). 
*   While the paper does provide details on the datasets and some hyperparameters (e.g., epochs, batch size, learning rate), it lacks information on:
    *   Specific neural network architectures used for each experiment (number of layers, activation functions, etc.).
    *   Justification for choosing the mentioned hyperparameter values.
    *   Details on how data splits were performed (e.g., ratios, stratification).
    *   Exact optimizer configurations and any modifications made (e.g., gradient clipping thresholds).

**Actionable Feedback:**

1.  **Clarify Section Names:** Ensure consistency by using ""Experimental settings"" throughout the paper or updating the justification to match the actual section headings.
2.  **Expand Hyperparameter Details:** Provide comprehensive information on the neural network architectures, hyperparameter selection process, and optimizer configuration for each experiment. Justify the choices made and explain how they impact the results.
3.  **Describe Data Splits:**  Report the exact methodology used for splitting the data into training, validation, and test sets, including the ratios and any stratification techniques employed. This ensures reproducibility and allows for a better understanding of the model's generalization capabilities.

**",0.5
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,The error bars are provided.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error of the mean.
It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
","## Feedback on NeurIPS Checklist Answer

**Discrepancies:**

*   The justification simply states ""The error bars are provided"" but does not point to specific sections where this information is located, as required by the guidelines. 
*   The paper appears to report correlations with other methods as the primary performance metric, not error bars or confidence intervals.
*   There's no mention of the method used to calculate error bars, assumptions made, or whether they represent standard deviation or standard error. 

**Suggested Improvements:** 

1.  **Locate and Reference Error Bars:**  
    *   Thoroughly search the paper, including figures and tables, to identify if error bars are indeed present. 
    *   If found, explicitly reference the specific figures/tables containing error bars within the justification.  
    *   If error bars are missing, consider adding them, especially for key results supporting the main claims.
2.  **Clarify Statistical Significance Reporting:**
    *   Explain the rationale behind using correlation as the primary performance metric.
    *   If error bars are present, elaborate on their calculation method and underlying assumptions.
    *   Specify whether the error bars represent standard deviation or standard error.
    *   Consider including confidence intervals or p-values for key results to strengthen the claims of statistical significance. 

**",0
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"Compute Resources are explained in the ""Experimental conditions"" section.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
","## Feedback on NeurIPS Paper Checklist Answer

**Discrepancies/Issues:** 

*   The justification points to a section named ""Experimental conditions"", but the paper uses the section title ""AC power flow experiments"" for the relevant section.  
*   While the paper mentions the type of hardware (Nvidia GeForce RTX 2080 Ti), it lacks details on the number of GPUs used, CPU type, and RAM.
*   The paper provides the time per instance for each experiment but does not estimate the total compute time required to reproduce the full results.
*   The paper does not disclose whether additional compute resources were used beyond the reported experiments (e.g., for preliminary or failed experiments).

**Suggestions for Improvement:**

1.  **Clarify Section Reference:**  Change the justification to accurately point to the ""AC power flow experiments"" section where compute resources are discussed.
2.  **Detail Hardware Configuration:**  Specify the number of GPUs used, the type of CPU, and the amount of RAM available during the experiments.
3.  **Estimate Total Compute Time:**  Provide an estimate of the total compute time required to reproduce the results for each experiment, considering the number of instances and time per instance.
4.  **Address Additional Compute Resources:**   Disclose whether additional compute resources were used beyond the reported experiments, even if only briefly. This information is valuable for reproducibility and understanding the full computational cost of the research. 

**",0.5
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We comply with the NeurIPS code of ethics.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
","## Feedback on NeurIPS Ethics Checklist Answer

**Discrepancies:**

*   The ""Justification"" provided simply states compliance but doesn't point to specific sections in the paper demonstrating adherence to the NeurIPS Code of Ethics, as required by the guidelines.
*   The paper content raises several ethical concerns:
    *   **Uncredited contributions:** The paper mentions the ""tireless and dedicated efforts of countless undergraduate students"" and ""underpaid crowdworkers from developing countries"" without proper acknowledgment or compensation, raising ethical concerns about exploitation and fair attribution. 
    *   **Data Acquisition:** The origins and licensing of the real historical data from the French high voltage power grid are not mentioned. This raises questions about data privacy, ownership, and potential misuse.
    *  **Dual Use and Misuse of LLMs:** The paper briefly mentions the application of DSS to LLMs and acknowledges the high risk of their misuse without providing any details on mitigation strategies or ethical considerations. This lack of transparency and responsibility is a significant concern.

**Actionable Feedback:**

1.  **Address Uncredited Contributions:**
    *   Acknowledge the contributions of the undergraduate students and crowdworkers in the paper, providing proper attribution and considering appropriate compensation. 
    *   If anonymity is a concern, consider broader acknowledgements (e.g., thanking the research group or university department) while ensuring ethical treatment of all contributors.
2.  **Clarify Data Acquisition:**
    *   Clearly describe the source of the French high voltage power grid data, including details about data access, ownership, and licensing.
    *   Discuss any anonymization or privacy-preserving measures taken to protect sensitive information.
3.  **Expand on LLM Ethics:**
    *   Provide a detailed discussion on the potential risks and ethical considerations associated with applying DSS to LLMs, particularly concerning dual-use and misuse possibilities. 
    *   Outline concrete safeguards or mitigation strategies being implemented to ensure responsible development and deployment of LLMs.

**",0
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,The paper's positive impact is evident and there is no negative impact we can think of.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
","## Feedback on Societal Impact Discussion

The provided ""Answer"" and ""Justification"" have several discrepancies with the ""Paper"" and do not adhere to the ""Guidelines"":

* **Missing discussion of negative impacts:** The ""Justification"" claims no negative impacts exist, but the ""Paper"" mentions potential misuse of the technology (LLMs generating deepfakes for disinformation) in the ""Application to LLMs"" section. This contradicts the claim and needs elaboration. 
* **Lacks specific section reference:**  The ""Justification"" fails to point to the relevant section discussing societal impact, as required by the ""Guidelines"".
* **Unclear positive impact explanation:** The ""Justification"" vaguely states the positive impact is ""evident"" without specifying what this impact is or where it is discussed in the paper.

Here are some actionable improvements to enhance the paper's quality regarding societal impact:

1. **Address negative societal impacts:** Expand on the potential misuse of LLMs for generating deepfakes and disinformation mentioned in the ""Application to LLMs"" section. Discuss the potential harms of such misuse and possible mitigation strategies. 
2. **Specify positive societal impacts:** Clearly identify the positive societal impacts of your work (e.g., faster simulations for power grids leading to improved efficiency and reliability). Link these impacts to specific sections in the paper where relevant results or applications are presented.
3. **Point to relevant sections:** Update the ""Justification"" to reference the specific section(s) where societal impact is discussed, as per the ""Guidelines"". 
4. **Consider broader implications:**  Explore potential negative impacts beyond the mentioned LLM application. Consider issues like fairness, privacy, and security in the context of power grid simulations and other potential applications of your work. 
5. **Balance the discussion:** Avoid overstating positive impacts and overlooking potential negative consequences. Present a balanced and nuanced discussion to demonstrate responsible consideration of societal implications.

**",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,Our models do not present such risks.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").
""
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
","## Feedback on NeurIPS Paper Checklist Answer

**Discrepancies:**

* **Justification contradicts paper content:** The justification states, ""Our models do not present such risks."" However, the paper discusses the application of their Deep Statistical Solver (DSS) to Large Language Models (LLMs) and explicitly acknowledges the high risk of misuse and dual-use of these models. 
* **Safety risks not addressed:** The paper fails to describe any safeguards implemented to mitigate the potential dangers associated with the release of LLMs, such as usage guidelines or access restrictions.
* **Dataset scraping concerns:** The paper mentions scraping the internet for LLM training data but doesn't explain how unsafe or biased content was avoided, raising concerns about the dataset's safety and potential for perpetuating harmful information.

**Actionable Feedback:**

1. **Revise Justification**: Change the answer to ""No"" and provide a justification that acknowledges the LLM risks discussed in the paper and explains why safeguards haven't been implemented.  For example, the justification could state that exploration of safeguards is left for future work. 
2. **Address LLM Risks**:  
    * **Discuss potential misuse scenarios**: Expand on the potential harms associated with LLM misuse (e.g., misinformation, phishing, propaganda).
    * **Propose mitigation strategies**: Explore and discuss possible safeguards, such as user agreements, access control, and safety filters for generated content.
3. **Dataset Transparency and Safety**:
    * **Describe data collection process**: Explain the methodology used for scraping internet data. 
    * **Explain data filtering/curation**:  Detail the steps taken (if any) to address unsafe or biased content in the dataset. 

**",0
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,The introduction credits the students and workers who contributed.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
If this information is not available online, the authors are encouraged to reach out to the asset's creators.
","## Feedback on NeurIPS Paper Checklist Answer

The proposed answer and justification regarding proper credit and licensing of assets have several discrepancies with the paper content and the guidelines:

* **Justification Mismatch:** The justification mentions crediting students and workers in the introduction. However, the provided paper excerpt only mentions students' contributions in a negative light and does not explicitly acknowledge or credit their work. There is no mention of workers contributing to the project.
* **Missing Information:** The paper excerpt and justification do not provide information on the licenses or terms of use for any code, data, or models used in the research.
* **External Assets:**  The paper clearly mentions using real historical data from the French high voltage power grid and potentially leveraging existing code packages (e.g., Fenics) for simulations. Further information about the origin, version, and license of these assets is missing.

## Actionable Feedback

1. **Proper Attribution:** Clearly acknowledge and credit all individuals who contributed to the research, including students and workers, in a dedicated acknowledgments section or footnote. Avoid negative framing and ensure respectful recognition of their contributions. 
2. **Asset Details and Licensing:** For all code, data, and models used in the research (including the French power grid data and any software packages), provide the following information:
    * **Origin:** Cite the original source (paper or website) where the asset was obtained.
    * **Version:** Specify the version of the asset used.
    * **License:** State the specific license under which the asset is distributed (e.g., CC-BY 4.0). If the license information is unavailable online, contact the asset creators for clarification.
    * **Terms of Use:** Briefly describe any relevant terms of use associated with the asset, especially for scraped data.
3. **Data Scraping:** If data was scraped from websites, explicitly mention the source websites and provide information on their copyright and terms of service. Discuss any ethical considerations related to data scraping and ensure compliance with the respective terms of service.

## ",0
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,Our data and code are well documented in supplemental material.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
The paper should discuss whether and how consent was obtained from people whose asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
","## Feedback on NeurIPS Paper Checklist Answer

**Discrepancies:**

*   The justification mentions ""data and code"" being well documented, but the question specifically asks about ""new assets"".  The paper discusses the application of DSS to LLMs and mentions released models, but it's unclear if these models are considered new assets and whether they are documented alongside data and code.

**Actionable Feedback:**

1.  **Clarify New Assets:** Explicitly state whether the released LLMs are considered new assets. If yes, ensure their documentation is provided alongside data and code.
2.  **Document LLMs:** Provide comprehensive documentation for the LLMs, including details about training data, architecture, limitations, potential biases, and risks of misuse. Consider using structured templates as recommended by the guidelines. 
3.  **Address Ethical Concerns:**  The paper briefly mentions ethical concerns and risks of misuse for LLMs. Expand this discussion to address safety measures, potential biases inherited from training data, and plans for mitigating potential harm.  

**",0.5
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,Our research does not involve any human.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
","## Feedback on NeurIPS Checklist Answer

**Discrepancies:**

* The ""Justification"" claims no human involvement, but the ""Paper"" mentions underpaid crowdworkers and uncredited student contributions in the introduction. This directly contradicts the ""Answer"" and raises ethical concerns.

**Actionable Feedback:**

1. **Address Ethical Concerns:**
    * Remove the problematic statement from the introduction or significantly revise it to acknowledge and ethically justify the use of underpaid crowdworkers and uncredited student labor. 
    * Ensure compliance with the NeurIPS Code of Ethics regarding fair compensation and proper credit for all contributors.
2. **Revise Answer and Justification:**
    * Change the ""Answer"" to ""No"" to reflect the actual involvement of humans in data collection.
    * Update the ""Justification"" to explain the nature of human involvement and clarify why details are not included in the main paper (e.g., ""Data collection involved crowdworkers, but details are omitted from the main paper due to space constraints. Ethical considerations and compensation details are addressed in the supplemental material""). 

**",0
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,Our research does not involve human subjects or ethical issues.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
","## Feedback on NeurIPS Checklist Answer

**Discrepancies:**

*   The paper discusses the use of ""real historical data from the French high voltage power grid"" which could imply potential privacy risks for individuals, depending on the nature of the data and whether it can be linked to specific individuals. This contradicts the claim that ""Our research does not involve human subjects or ethical issues.""
*   The statement regarding ""countless undergraduate students"" and ""underpaid crowdworkers"" raises ethical concerns about fair compensation and acknowledgment for their contributions. This suggests potential ethical issues not addressed in the Justification. 

**Actionable Feedback:**

1.  **Clarify Data Usage:**  Clearly explain the nature of the ""real historical data"" and how it was anonymized to protect individual privacy. If the data cannot be fully anonymized, consider discussing potential risks and mitigation strategies or revise the claim of no ethical involvement.
2.  **Address Ethical Concerns:**  Acknowledge the contributions of undergraduate students and crowdworkers, ensuring appropriate credit and compensation for their work. If these contributions were significant to the research, consider including them as co-authors or in the acknowledgments section.

**",0
