Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,Our main claims are listed in section 1 and highlight this paper's contributions.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the abstract and introduction do not include the claims made in the paper.
The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
","## Feedback on NeurIPS Paper Checklist Answer

**Discrepancies/Issues:**

* **Justification Specificity:** The justification simply states ""Our main claims are listed in section 1 and highlight this paper's contributions."" This lacks specific pointers to the location of claims within section 1 and doesn't explicitly address the scope.
* **Scope Discussion:**  The justification and paper content seem to focus more on contributions and less on the paper's scope. A clear definition of the scope of the research and its limitations would strengthen the answer.
* **Claim Verification:** While the paper presents results and analysis, the justification does not explicitly mention the verification of the main claims with theoretical or experimental results.

**Actionable Feedback:**

1. **Improve Justification Specificity:** 
    * Clearly state which specific parts of section 1 contain the main claims and how they reflect the contributions. 
    * Include subsection numbers or even sentence numbers for precise reference.

2. **Address Scope:**
    * In section 1, explicitly define the scope of the research, including the specific problems addressed and the limitations of the proposed methods.
    * You can add a sentence like, ""This paper focuses on the architecture search of STGNNs for the task of multivariate spatio-temporal forecasting, specifically traffic prediction, with a focus on improving efficiency and effectiveness within the defined search space.""

3. **Connect Claims to Evidence:**
    * Within the justification, mention how the main claims are supported by the paper's experimental results and analysis. 
    * For example, you could add, ""The effectiveness of the proposed framework and search strategy is validated by the empirical results in Section 5, demonstrating superior performance compared to existing methods.""

**",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,"We have discussed the potential limitations in section 6, including the application domain and considered temporal modeling methods.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
The authors are encouraged to create a separate ""Limitations"" section in their paper.
The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
","## Feedback on Limitations Discussion

While the paper does mention limitations in section 6, the discussion lacks depth and comprehensiveness according to the NeurIPS guidelines. Here are some actionable improvements:

* **Expand on the application domain limitations.**  Go beyond mentioning the domain and delve into specific scenarios where the model might underperform or face challenges. Discuss potential biases in the data and their impact on model predictions.
* **Analyze the robustness of results under assumption violations.**  Clearly identify any strong assumptions made, such as independence or model well-specification. Elaborate on how these assumptions might be violated in practice and the consequences for the model's performance and reliability.
* **Quantify the scope of the claims.**  Reflect on the limitations arising from testing the approach on a limited number of datasets or runs. Discuss the generalizability of the findings and potential variations in performance across different data and settings.
* **Explore factors influencing performance.**  Discuss how factors like data quality, missing values, and noise levels might affect the model's predictions. Consider potential challenges in real-world applications, such as handling unexpected events or evolving traffic patterns.
* **Address privacy and fairness concerns.**  Discuss potential issues related to data privacy and fairness, especially when dealing with sensitive location information. Acknowledge any biases in the data and potential discriminatory outcomes from model predictions.

**",0.5
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,This paper does not include theoretical results.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include theoretical results. 
All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced. 
","## Feedback on NeurIPS Paper Checklist Answer

The authors' proposed answer and justification seem accurate. The paper focuses on empirical analysis and architecture search for STGNNs, not on theoretical results. Therefore, ""NA"" is the appropriate response.

However, to improve the paper's clarity and potential impact, consider the following suggestions:

*   **Explicitly state the absence of theoretical results:** While the current justification is sufficient for the checklist, consider briefly mentioning in the main text (perhaps in the introduction or conclusion) that the paper's focus is on empirical findings and architecture design principles, not on theoretical analysis. This provides context for readers and avoids any potential confusion.
*   **Highlight the empirical contributions:** Emphasize the paper's strengths in the abstract and introduction.  Clearly state the novel insights gained from the disentangled framework and the practical implications for architecture search, especially the identified ""cherry regions"" for optimal design choices. 
*   **Visualize the search space:**  The paper extensively discusses the search space and its properties. Consider including visualizations (e.g., dimensionality reduction plots) to better illustrate the distribution of architectures and the identified ""cherry regions"". This would enhance the reader's understanding and  the paper's impact.


## ",1
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,"We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
    - If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
    - If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
    - If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
    - We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

","## Feedback on Reproducibility Question

The authors' proposed answer and justification seem **mostly accurate but lack some specifics**. Here's how it can be improved: 

*   **Justification Specificity:** While the justification points to relevant sections, it could benefit from increased specificity. For instance, instead of mentioning ""dataset,"" explicitly state which datasets were used (PeMS04 and PeMS08). 
*   **Hyperparameter Details:** The justification mentions that experiment settings are discussed, but it should explicitly state that hyperparameter details needed for reproduction are included. 
*   **Code Availability:** The justification mentions the Github link, which is great. However, it's beneficial to clarify if the code is the exact version used for the experiments or if it requires additional setup or dependencies. 

**Suggested Improvements:**

1.  **Refine Justification:** Modify the justification to be more specific, explicitly mentioning the datasets used (PeMS04 and PeMS08) and stating that hyperparameter details are provided for reproducibility. 
2.  **Clarify Code:** Add a sentence to the justification clarifying whether the provided code is the exact version used for the experiments or if any additional setup is needed.

**",0.5
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,"All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
","## Feedback on NeurIPS Checklist Answer

**Discrepancies:**

* The justification mentions Appendix B for dataset information, but the provided paper excerpt does not include Appendix B. 
* The justification mentions exact commands and environments needed to reproduce the results are present, but the provided excerpt does not offer evidence of this.
* The justification does not mention if anonymized versions of the code and data are released.


**Actionable Feedback:**

1. **Include Appendix B:** Ensure Appendix B containing dataset information is included with the paper submission.
2. **Clarify Reproducibility Instructions:**  Explicitly state the location (e.g., a README file) and content of the instructions for reproducing the main experimental results. This should include exact commands, required environment details, and steps for data preparation.
3. **Anonymization:** Address whether anonymized versions of the code and/or data have been released to preserve anonymity during the review process as per NeurIPS guidelines.

**",0.5
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental material.
","## Feedback on NeurIPS Checklist Answer

The proposed answer and justification have some discrepancies and can be improved:

* **Justification Mismatch**: The justification mentions hyperparameter sampling as a method design, but the question specifically asks about reporting training and test details, not the method of choosing them.
* **Missing Details**:  While Appendix B is mentioned, the specific details provided there are unclear. Explicitly mentioning the types of details (e.g., data split ratios, learning rate schedules, optimizer settings) would improve clarity.
* **Section Reference**: The justification mentions ""section and 5"", which is not a clear reference. Provide the exact section number(s) where the relevant information is found. 

## Suggested Improvements

1. **Refine Justification**: Focus the justification on the reporting of training and test details, not the design choices for those details. For instance, ""Section 4 and Appendix B provide specific  details on data splits, hyperparameter choices, optimizer settings, and training procedures used in the experiments.""
2. **Clarify Appendix Content**: Briefly mention the types of details provided in Appendix B,  for example, ""Appendix B details the exact data split ratios, learning rate schedules, optimizer settings, and other hyperparameters used for each experiment.""
3. **Correct Section Reference**: Replace ""section and 5"" with the accurate section number.

## ",0.5
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error of the mean.
It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
","## Feedback on Error Bar Reporting

The author's proposed answer and justification are not entirely accurate and require improvements to comply with the guidelines.

**Discrepancies and Issues:**

* **Justification Lacks Specific References:** While the justification mentions sections 4 and 5, it does not pinpoint the exact locations (e.g., figures, tables) where error bars are presented. 
* **Unclear Error Bar Type and Calculation:** The paper doesn't explicitly state whether the error bars represent standard deviation or standard error. Additionally, the method for calculating them (e.g., formula, library function) is not explained. 
* **Missing Information on Variability Factors:**  The justification mentions ""multiple runs"" but does not specify the specific factors of variability captured by the error bars (e.g., train/test split, initialization).
* **Assumptions Not Stated:**  The paper does not mention any assumptions made regarding error distribution (e.g., normality).

**Actionable Feedback:**

1. **Precisely Reference Error Bar Locations:** In the justification, explicitly mention the figures and/or tables where error bars are displayed (e.g., ""Figure 3 shows the mean squared error with standard deviation error bars for ..."").
2. **Clarify Error Bar Type and Calculation:** Clearly state whether standard deviation or standard error is used for error bars. Explain the calculation method, including any formulas or library functions employed.
3. **Specify Variability Factors:** Define the specific sources of variability captured by the error bars (e.g., ""Error bars represent the standard deviation across 5 different random initializations"").
4. **State Assumptions:** Explain any assumptions made about the error distribution (e.g., ""We assume normally distributed errors for calculating confidence intervals"").

**",0.5
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Yes,"In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
","## Feedback on NeurIPS Checklist Answer

The authors' proposed answer of ""Yes"" to the question of whether the paper provides sufficient information on computer resources is **not accurate** given the justification and the actual paper content.  

Here's why:

* **Justification Mismatch**: The justification mentions reporting ""GPU hours"", but the guidelines require more comprehensive information, including:
    * **Type of compute workers**: CPUs, GPUs, TPUs, etc. 
    * **Memory and storage** requirements.
    * **Time of execution** for individual experimental runs and the total compute time.
    * **Cloud/cluster details** if used.
    * **Compute usage beyond reported experiments**:  This includes preliminary or unsuccessful experiments.

* **Paper Content**: A review of the paper reveals no mention of the specific hardware used, memory requirements, or execution time for individual experiments. 

## Recommendations for Improvement:

1. **Add a dedicated section (e.g., Appendix)** to comprehensively report the required computational resources for reproducing the experiments. This section should include the missing information mentioned above. 
2. **Revise the justification** to accurately reflect the provided information on computational resources. 
3. **Consider reporting**:
    * **Hyperparameter optimization details**: This could further aid reproducibility.
    * **Code and data availability**: Sharing these resources would greatly benefit the research community.

## ",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We review the code of ethics and our paper conform with it.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
","## Feedback on NeurIPS Ethics Checklist Answer

The authors' proposed answer and justification for adherence to the NeurIPS Code of Ethics are insufficient. While stating ""Yes"" and mentioning a review of the code is a good start, the justification lacks specific references to sections within the paper demonstrating this alignment.

**Here's how to improve the answer:**

* **Identify relevant sections:**  Review the NeurIPS Code of Ethics and identify key principles relevant to your research. 
* **Map principles to paper sections:**  For each identified principle, pinpoint the specific section(s) in your paper that demonstrate its application. Examples of principles include:
    * **Data collection and usage:** If the paper involves human data, explain how informed consent was obtained and how privacy is protected.
    * **Algorithmic bias:** Discuss potential biases in your methodology and the steps taken to mitigate them.
    * **Environmental impact:** If applicable, detail efforts made to minimize the environmental impact of your research, such as using efficient algorithms or hardware.
    * **Societal impact:** Analyze the potential societal implications of your work, both positive and negative, and suggest ways to address any concerns.
* **Rewrite justification:**  Update the justification to explicitly reference the identified sections and explain how they demonstrate adherence to the ethical principles. 

**Example of improved justification:**

> ""Yes, our research conforms to the NeurIPS Code of Ethics. We addressed potential algorithmic bias by [explain your mitigation strategy] as discussed in Section X.  Additionally, we ensured responsible data collection and usage by [explain your approach] as detailed in Section Y.""

**Additional considerations:**

* **Transparency:** Be transparent about any limitations or ethical challenges encountered during your research.
* **Anonymity:** Ensure that the justification preserves anonymity and does not reveal any identifying information about the authors or their affiliations.

## ",0
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"We discuss the broader impacts in the section ""Broader Impact Statement"".","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
","## Feedback on Societal Impact Discussion

The provided ""Broader Impact Statement"" simply states that there are no negative societal impacts without any further justification or elaboration. This is insufficient given the guidelines and the nature of the research.

Here are some actionable improvements to consider:

* **Discuss potential misuse:** While the paper focuses on traffic prediction, the advancements in STGNNs could be applied to other domains with sensitive data, potentially leading to privacy violations or discriminatory outcomes. Acknowledging this possibility and suggesting mitigation strategies would strengthen the paper. 
* **Address data bias:** Traffic data can reflect existing societal biases and inequalities. Discuss how these biases might be amplified by the model and propose methods to ensure fairness and mitigate potential harm.
* **Transparency and explainability:**  Explain how the model works in simple terms to promote transparency and trust. This is especially important in applications like traffic prediction, which can have significant impacts on people's lives. 

**",0
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,This paper poses no such risks.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").
""
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
","## Feedback on NeurIPS Checklist Answer

The authors' proposed answer of ""NA"" with the justification ""This paper poses no such risks"" is **not entirely accurate**. While the paper itself may not directly involve releasing data or models, the research presented could potentially be misused in the development of other systems with higher risks.

Here's how the authors can improve their response and the paper:

* **Acknowledge potential risks:** Briefly mention that while the paper does not directly release models or data, the proposed framework and understanding of STGNN architectures could be utilized in systems involving sensitive data or potentially biased predictions.
* **Discuss mitigation strategies:** Propose potential safeguards or ethical considerations for future research and applications based on this work. This could involve discussing data anonymization techniques, fairness-aware training methods, or the importance of human oversight in such systems.
* **Transparency and openness:** Emphasize the importance of open-sourcing the code and methodologies used in the research to allow for community scrutiny and facilitate responsible development.


**",0.5
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
If this information is not available online, the authors are encouraged to reach out to the asset's creators.
","## Feedback on Licensing Question

The authors have made a good start by citing the data sources and providing a link to the GitHub repository. However, the response lacks specific details required by the guidelines for a complete and accurate answer. 

Here's how the authors can improve their answer:

* **Specify Dataset Versions:** While the link to the GitHub repository is helpful, it's important to clarify the exact versions of the datasets used. Different versions might have variations in preprocessing or data splits, which could affect reproducibility.
* **State Licenses for Each Dataset:** The response should explicitly state the licenses under which the datasets are distributed. This information is crucial for ensuring compliance and understanding the terms of use. The authors can refer to resources like paperswithcode.com/datasets to find license information or reach out to the dataset creators if necessary.
* **Clarify Code Usage:** The response only mentions data usage. If the paper also utilizes any existing code libraries or packages, similar details should be provided - including citations, version numbers, and license information.

**Example Improved Justification:**

""We use the PeMS04, PeMS07, and PeMS08 datasets from the Caltrans Performance Measurement System [13, 1, 10]. The specific versions used can be found at https://github.com/Davidham3/ASTGCN/tree/master/data.  These datasets are available under the [Insert License Name] license. Additionally, we utilize the [Insert  Library/Package Name] version [Insert Version Number] library for [Insert Purpose], which is licensed under [Insert License Name].""

**",0.5
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Not Applicable,The paper does not release new assets.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
The paper should discuss whether and how consent was obtained from people whose asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
","## Feedback on NeurIPS Checklist Answer

The authors' proposed answer of ""NA"" with the justification ""The paper does not release new assets"" seems accurate given the provided paper excerpt. However, the paper could still benefit from discussing existing datasets and their limitations, especially concerning ethical considerations like consent for data usage. 

Here are some actionable improvements based on the guidelines:

* **Discuss existing datasets and limitations:**  Although the paper doesn't introduce new assets, it would be valuable to dedicate a section (e.g., within the ""Problem Definition"" or a dedicated ""Data"" section) to discussing the used datasets (PeMS03, PeMS04, PeMS07, PeMS08). This section should:
    *   Provide a brief overview of each dataset, including its size, source, and key features.
    *   Discuss any limitations or biases present in the datasets, especially related to geographical scope or potential ethical concerns.
    *   Address the issue of consent: While the PeMS datasets are publicly available, discussing the original data collection process and any existing consent mechanisms would strengthen the ethical awareness of the paper.

* **Improve clarity of ""Problem Definition"" section:** 
    *   Consider renaming the section to ""Problem Formulation"" to better reflect its content.
    *   The mathematical formulation could be made more accessible by providing a more detailed explanation of the variables and their significance. 

**",0.5
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
","## Feedback on NeurIPS Checklist Answer:

The authors' proposed answer of ""NA"" with the justification seems accurate given the paper's focus on Spatio-Temporal Graph Neural Networks for traffic prediction, which doesn't involve crowdsourcing or human subject research. 

However, there's an opportunity to enhance the paper's impact and reproducibility by including details about data collection and preprocessing in the supplementary material. This would be beneficial for researchers trying to replicate results or apply the methods to new datasets. 

**Actionable improvements:**

*   **Data Collection Details:** Describe the process of collecting the traffic data used in the experiments. This could include information about the data source (e.g., PeMS), collection methods, and any relevant timestamps or identifiers.
*   **Data Preprocessing Steps:** Explain the steps taken to prepare the raw data for model training. This should cover details like aggregation into 5-minute intervals, z-score normalization, and feature engineering choices.
*   **Code and Data Availability:** If possible, provide access to the code and preprocessed data used in the experiments. This promotes open science and allows others to easily build upon the work.

These additions would strengthen the paper by increasing transparency and enabling reproducibility.

**",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
","## Feedback on NeurIPS Checklist Answer

The authors' proposed answer of ""NA"" with the justification ""This paper does not involve crowdsourcing nor research with human subjects"" appears accurate based on the provided paper abstract and introduction. The paper focuses on developing and evaluating a framework for neural architecture search in spatio-temporal graph neural networks applied to traffic prediction. There is no indication of any research involving human participants.

However, to further improve the paper's quality and clarity, consider these minor suggestions:

* **Explicitly state the absence of human subjects research in the main paper**: While the justification for the ""NA"" answer is sufficient for the checklist, adding a brief sentence in the main paper (e.g., in the Broader Impact Statement or a dedicated Ethics Statement) explicitly stating the absence of human subjects research would enhance transparency. 
* **Confirm adherence to ethical guidelines**: Consider mentioning adherence to the NeurIPS Code of Ethics to demonstrate commitment to responsible research practices.

## ",1
