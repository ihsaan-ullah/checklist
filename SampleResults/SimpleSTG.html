<h2>Understanding and Simplifying Architecture Searchin Spatio-Temporal Graph Neural Networks</h2><br>--------------------------------------<br>[+] Correctness Score (genuine): 0.8<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Score (adversarial): 0.47<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Score (truth_adversarial): 0.53<br>--------------------------------------<br>--------------------------------------<br>[+] Resiliance Score: 0.8<br>--------------------------------------<br>--------------------------------------<br>[+] Combined Score: 0.04<br>--------------------------------------<br><h1><strong>genuine</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our main claims are listed in section 1 and highlight this paper's contributions.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. The abstract and introduction sections accurately outline the paper's primary contributions and set clear expectations for readers regarding the scope of the research. The authors claim to address significant issues in Neural Architecture Search (NAS) for Spatio-Temporal Graph Neural Networks (STGNNs), specifically the integration of hyperparameters into the NAS framework and the challenge of navigating a large search space. These claims are substantiated through the detailed explanation and results presented in the subsequent sections.<br><br>Justification:<br>- Reflection of Contributions: The abstract clearly states the paper's contributions, including the proposition of a disentangled framework to simplify the architecture search and the methodological advancements to allow fair and efficient evaluation of model configurations. These are major contributions that directly address the paper's goals, as introduced.<br>- Scope Accuracy: Both the abstract and introduction accurately reflect the scope of the paper, setting realistic expectations for the readers. The paper focuses on STGNNs, particularly in the context of traffic prediction, and aims to improve the efficiency and accuracy of architecture searches. This scope is well communicated and adhered to throughout the paper.<br>- Match Between Claims and Content: The main claims made in the introductory sections match the theoretical insights and empirical results presented later in the document. The authors not only propose a novel framework but also demonstrate its efficacy through comprehensive experiments and comparisons with state-of-the-art methods.<br><br>Constructive Suggestions:<br>1. Clarify Limitations Early: Although the paper does discuss limitations and potential areas for future work, integrating a brief mention of the primary constraints or challenges faced by the proposed methodologies early in the paper might help set more precise expectations.<br>2. Comparative Analysis: While the paper provides a comparative analysis demonstrating the superiority of the proposed methods, including additional discussion on why certain methodologies outperform others could enrich the reader's understanding.<br>3. Exploration Beyond Traffic Prediction: Highlighting potential applicability or the need for adaptation of the proposed framework for other types of spatio-temporal graph data early in the paper could expand its perceived scope and relevance.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have discussed the potential limitations in section 6, including the application domain and considered temporal modeling methods.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors clearly states that they discussed potential limitations in Section 6 of their paper, including the application domain and the considered temporal modeling methods. This directly answers the question, confirming that the paper indeed discusses the limitations of the work performed by the authors. Based on the guidelines provided, discussing limitations is a crucial part of a research paper as it provides transparency about the scope and applicability of the research findings. The authors' decision to include a discussion on the limitations aligns with the expectation of acknowledging and reflecting on the strengths and constraints of their work, as well as its potential impact on the robustness and generalizability of the results.<br><br>However, to further improve the limitations section and ensure it meets the highest standards of completeness and transparency, the authors might consider the following constructive suggestions:<br><br>1. Expand on Assumption Limitations: Clearly outline any assumptions made during the research and discuss potential scenarios where these assumptions may not hold. This can include assumptions on data distribution, model specifications, and environmental settings in which the model was tested.<br><br>2. Robustness and Generalizability: Elaborate on how the model's performance could vary with different datasets, outside the ones tested, or under varying conditions such as noise levels, dataset sizes, and feature distributions.<br><br>3. Computational Efficiency: If not already included, discuss the computational demands of the proposed framework, especially in comparison to existing methods. This is important for understanding the practicality of deploying the model in real-world applications.<br><br>4. Impact of Hyperparameter Choices: Provide insights into how sensitive the model's performance is to hyperparameter settings, and whether there are challenges in finding optimal hyperparameters.<br><br>5. Potential for Bias and Fairness Concerns: Reflect on any aspects of the model or data that could introduce bias or fairness issues, particularly if the model is used in diverse application domains.<br><br>6. Future Directions: Briefly discuss potential avenues for future research that could overcome the current limitations, providing a roadmap for advancing the work presented.<br><br>By addressing these areas, the authors can offer a more detailed and nuanced view of their work's limitations, contributing to a well-rounded and thoroughly vetted piece of research. This not only fosters trust and credibility but also provides a foundation for future work to build upon.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not include theoretical results.</span><br><span style='color:#0000FF;'>Review: The provided answer is consistent with the content of the paper. The paper primarily focuses on disentangling the framework for Spatio-Temporal Graph Neural Networks (STGNNs), studying the influence of different architectural choices and hyperparameters on the performance of STGNNs, and proposing a methodology to simplify the architecture search process. The main contributions revolve around empirical findings and methodological innovations rather than theoretical results. Therefore, the response "NA" accurately reflects that the paper does not introduce theoretical results, thus not requiring the full set of assumptions or complete proofs as outlined in the guidelines.<br><br>Despite the correctness of the answer, for completeness and to encourage thoroughness in future work, the review might include suggestions to the authors, emphasizing areas for potential theoretical exploration that could strengthen the work:<br>1. Encourage the exploration of theoretical aspects related to the proposed disentangled framework. For instance, formalizing the implications of the identified design principles on the STGNNs' representational capabilities or convergence properties could provide deeper insights.<br>2. Suggest the consideration of theoretical analysis that could quantify the impact of hyperparameters on model performance, beyond empirical observations. Establishing theoretical bounds or relationships could provide a more robust understanding of the observed phenomena.<br>3. Recommend investigating the theoretical properties of the simplified NAS method introduced, particularly in the context of search efficiency and scalability. Understanding the theoretical limits of such a methodology could guide further improvements.<br>4. Highlight the importance of theoretical contributions in providing a bridge between empirical observations and foundational principles. Encouraging the authors to think along these lines for future iterations of their work could enhance the rigor and depth of their research.<br><br>By addressing these points, the authors might find avenues to incorporate theoretical aspects into their work, enhancing its comprehensiveness and appeal to a broader audience within the research community.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.</span><br><span style='color:#0000FF;'>Review: The paper provides thorough details and guidelines for reproducing the main experimental results, covering various aspects such as the experiment settings, dataset descriptions, baseline comparisons, implementation details, and results across multiple sections and appendices. Additionally, the availability of the code through a provided GitHub link further enhances the reproducibility of the paper's contributions. Given this comprehensive approach to disclosing experimental procedures and results along with the provision of code, the justification aligns with the efforts made by the authors to ensure the paper meets the necessary standards for reproducibility.<br><br>However, to further enhance the reproducibility and clarity of the paper, the following constructive and actionable suggestions are offered:<br>1. Dataset Accessibility: While the GitHub link is mentioned, ensuring that the link is functional and that the repository contains clear instructions on how to access and use the datasets for reproduction would be beneficial.<br>2. Baseline Implementation Details: More explicit details or references regarding the implementation of baseline models could help readers better understand any nuances in replicating these comparisons.<br>3. Hyperparameters and Model Configurations: Providing a dedicated section or table summarizing all critical hyperparameters and model configurations used in the experiments could help streamline the reproduction process.<br>4. Experiment Environment: Details about the computational environment (e.g., hardware configurations, software versions) can significantly impact reproduction efforts, especially for computationally intensive models.<br>5. Additional Examples: Including a few examples of running the code for specific experiments would guide readers through the process and help clarify any potential ambiguities.<br><br>By addressing these points, the authors can further solidify the reproducibility of their work and assist the research community in building upon their findings.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.</span><br><span style='color:#0000FF;'>Review: The paper does address the requirement for open access to data and code, providing both the URL to the GitHub repository containing the code and specifying the datasets used in the experiments. The datasets mentioned (PeMS03, PeMS04, PeMS07, and PeMS08) are publicly available and commonly used in traffic forecasting research, which supports reproducibility. Furthermore, the detailed description in section B of the Appendix regarding the datasets and problem setting gives sufficient context for readers wanting to reproduce the experiments. Also, the mention of the GitHub URL (https://github.com/AutoML-Research/SimpleSTG) for accessing the code meets the guidelines for code submission.<br><br>Justification Review:<br>1. Open Access to Data and Code: The paper explicitly mentions the GitHub URL for the code and references the sections and appendices where datasets are detailed, which aligns with the open-access criteria.<br>2. Sufficiency for Reproducing Results: The citation of both the code repository and datasets, alongside the details provided in section B of the appendix regarding dataset statistics and the problem setting, indicates an effort to allow for reproducibility. However, to fully validate the sufficiency, a closer look at the GitHub repository to ensure it contains necessary instructions and scripts for reproducing the results would be ideal, which is beyond the scope of this review based on the content provided.<br><br>Constructive Suggestions:<br>- Detailed Reproduction Instructions: To enhance the reproducibility further, the authors should ensure that the repository contains a README file with step-by-step instructions on how to set up the environment, preprocess the data, and execute the scripts to reproduce the experimental results. Including requirements.txt or environment.yml files to facilitate environment setup would also be beneficial.<br>- Clarity on Reproducible Experiments: Clarify in the paper or GitHub README which specific results or experiments from the paper can be exactly reproduced with the provided code and data. If there are any results that cannot be reproduced (due to randomization or extensive computational requirements), this should be explicitly stated.<br>- Anonymity Preservation: Since the review mentions maintaining anonymity, it is assumed this is a double-blind review process. The authors should ensure that the GitHub repository they link to doesn't break the double-blind review requirement by inadvertently revealing their identity. An anonymized version of the repository should be maintained during the review process.<br><br>Overall, the paper meets the criteria for providing open access to data and code, supporting the reproducibility of the main experimental results as described.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.</span><br><span style='color:#0000FF;'>Review: The authors provided a detailed justification for their answer, confirming that they have included necessary training and test details such as data splits, hyperparameters, and the types of optimizers used in the paper. Additionally, they mentioned that the methodology for hyperparameter sampling and specifics about the training and testing setups are explained in sections and appendices, ensuring a comprehensive understanding of the experimental results.<br><br>While the response is satisfactory, ensuring comprehensive detail that facilitates reproducibility, it's important to critically evaluate whether these details indeed meet the standard required for full reproducibility and clarity. Based on the extensive excerpts and descriptions provided in the overview of the paper, it appears that the authors have made considerable efforts to detail their experimental procedures, hyperparameter choices, and model configurations. This includes discussions on the datasets used, problem settings, design and understanding of architectures, and the specific configurations of their proposed models, along with appendices that delve deeper into these aspects.<br><br>Constructive and actionable suggestions for further improvement could include:<br>- Ensure that the versioning of datasets and any preprocessing steps are clearly documented, to avoid any ambiguity in data handling.<br>- While the paper mentions the general approach to hyperparameter selection and optimization setups, including the ranking and distribution strategies used, providing direct links to code repositories can greatly enhance reproducibility. It's noted that the authors have made their code available, which is commendable.<br>- Clarify any dataset-specific tuning or model adjustments made for different experiments, as this could impact the generalizability of the findings across different spatio-temporal forecasting tasks.<br>- For the hyperparameter study, it might be helpful to include a sensitivity analysis or discussions on the robustness of model performance with respect to varying hyperparameters, which would benefit readers looking to apply these models in different contexts.<br><br>Overall, the paper seems to satisfy the requirements specified in the question and guidelines, with the aforementioned suggestions potentially enhancing clarity and reproducibility further.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.</span><br><span style='color:#0000FF;'>Review: The authors have adhered to the guidelines by including variance as error bars in the main experiments sections, indicating the statistical significance of their results. By incorporating mean and variance calculations across multiple runs, they provide a clear understanding of the variability and reliability of their findings. This practice is crucial for evaluating the robustness of the presented models and methods, allowing readers and other researchers to assess the confidence level in the experimental results critically.<br><br>However, while the authors have fulfilled the basic requirements regarding the reporting of statistical significance, there are areas where they could enhance the transparency and depth of their statistical analysis:<br><br>1. Clarification of Variability Sources: The authors should explicitly state the sources of variability captured by the error bars (e.g., initialization, data splits, or hyperparameter settings). This additional detail will help readers understand what factors might influence the variability in performance.<br><br>2. Methodology for Error Calculation: It would be beneficial for the authors to describe more explicitly the methodology used to calculate the error bars. Whether they are employing a specific statistical method, utilizing a library function, or conducting a bootstrap analysis, detailing this process would improve the paper's rigor.<br><br>3. Error Bar Representation: The authors should specify whether the error bars represent standard deviations or standard errors of the mean. This distinction is crucial for correctly interpreting the results' stability and reliability.<br><br>4. Assumptions and Confidence Intervals: If possible, the authors should discuss any assumptions underlying their statistical analyses, such as the normality of errors. Additionally, considering the exploration of confidence intervals, specifically whether they opted for 1-sigma or 2-sigma error bars, and the rationale behind this choice, would contribute to a more nuanced understanding of their statistical significance assessments.<br><br>Overall, the submission effectively meets the requirements for reporting statistical significance, with room for enhanced clarity and depth in their presentation of statistical analyses. Including these suggested details would bolster the paper's transparency and allow for a more informed evaluation of the presented research findings.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.</span><br><span style='color:#0000FF;'>Review: Justification: While the paper mentions the GPU hours required for generating reasonable answers in comparison to multiple baselines, detailed information about the specific computer resources used (e.g., type of GPUs, memory, and storage) is not provided. Furthermore, there is a lack of detailed explanation regarding the execution time for each individual experimental setup, which is crucial for reproducibility. Without specific details on the computational environment, including CPU/GPU types, memory, storage, and the cloud provider or internal cluster information, it is challenging for other researchers to precisely replicate the experiments and validate the results presented in the paper. Additionally, the paper does not discuss if the full research project required additional computational resources beyond what was reported for the experiments.<br><br>Constructive and Actionable Suggestions:<br>1. Add a dedicated section or append supplementary material detailing the computational environment used for the experiments. This should include information about the type of GPUs, CPU specifications, the amount of RAM and storage, and whether the experiments were conducted on an internal cluster or through a cloud provider.<br>2. Provide a breakdown of execution time for each experimental run, outlining any variations in computational requirements for different experiments or models compared.<br>3. Discuss any preliminary or failed experiments that required computational resources but were not included in the final paper. This transparency helps in understanding the full extent of computational resources necessary for the project.<br>4. If possible, provide an estimate of the cost of experiments, especially if cloud computing resources were used. This information can help researchers with limited budgets to plan their experiments accordingly.<br><br>By addressing these suggestions, the authors can significantly enhance the reproducibility of their experiments and offer valuable insights into the computational demands of their proposed methods.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We review the code of ethics and our paper conform with it.</span><br><span style='color:#0000FF;'>Review: The authors affirm that their research conforms to the NeurIPS Code of Ethics. Based on the detailed review of the paper's content, there are no apparent violations of ethical guidelines. The paper presents a technical advancement in the field of graph neural networks, specifically focusing on spatio-temporal forecasting in traffic prediction, and doesn't involve human subjects, unethical data usage, or other sensitive ethical issues. The provided research appears to be in line with ethical standards concerning technical research and development in machine learning and artificial intelligence.<br><br>However, as with any affirmation of ethical compliance, this is taken at face value based on the information provided in the paper and the authors' declaration. It's worth noting that a more thorough investigation into all aspects of the research, such as data sourcing, model deployment implications, and broader impacts, would be required for a comprehensive ethical review.<br><br>Constructive and Actionable Suggestions for Authors Regarding Ethics:<br><br>1. Data Usage Transparency: Ensure full transparency about the data sources, especially because traffic prediction models could potentially use data with privacy implications. Clearly articulate data anonymization and handling procedures if applicable.<br><br>2. Model Deployment Considerations: Discuss any potential negative impacts this model could have if deployed in the real world. For example, could optimized traffic predictions lead to undesirable consequences such as environmental impacts or biases in resource allocation? Consider adding a section that discusses these aspects.<br><br>3. Broader Impact Statement: While the paper briefly touches on the broader impacts, expanding this section to include detailed analysis of both positive and negative implications would strengthen the ethical review of the work. Specifically, discuss long-term implications of NAS in traffic prediction systems and their societal, environmental, and economic impacts.<br><br>4. Code and Data Accessibility: Ensure that the code and any datasets used are made publicly available in a manner that respects any copyright or privacy concerns. This accessibility supports ethical standards by promoting reproducibility and transparency.<br><br>5. Ethical AI Principles: Briefly discuss how the developed model aligns with broader ethical AI principles, such as fairness, accountability, and transparency. For instance, how might the model foster equitable outcomes in traffic management?<br><br>Addressing these suggestions would not only bolster the ethical rigor of the paper but also contribute to a more holistic understanding of the research's implications in the broader context of AI and society.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We discuss the broader impacts in the section "Broader Impact Statement".</span><br><span style='color:#0000FF;'>Review: Justification for Score:<br>The paper briefly mentions a section titled "Broader Impact Statement," suggesting that it discusses the societal impacts of the work performed. However, the provided content does not offer any detailed information regarding either positive or negative societal impacts, nor does it suggest any mitigation strategies for potential negative impacts. The statement "Our work does not contain potential harms to people, environment, society or any possible negative impacts listed on the NeurIPS Guidelines" lacks critical reflection on the broader implications of the research.<br><br>Constructive Suggestions:<br>1. Elaborate on Positive Impacts: Clearly articulate the potential positive societal impacts of your work. For example, improvements in traffic prediction models can significantly enhance urban planning, reduce congestion, and contribute to smarter transportation systems. Detailing these can provide a clearer understanding of the value of your research.<br><br>2. Consider and Discuss Potential Negative Impacts: Reflect on how the advancements or applications stemming from your research might lead to unintended consequences. For instance, could the deployment of highly accurate traffic prediction models lead to privacy concerns? Could they be misused in ways that could harm certain communities or individuals?<br><br>3. Mitigation Strategies: If there are potential negative impacts, it is crucial to discuss possible strategies to mitigate these. This could include technical measures, ethical guidelines for use, or recommendations for policymakers.<br><br>4. Expand the "Broader Impact Statement": Provide a more detailed and thoughtful exploration of the societal implications of your work in the designated section. Discuss both positive impacts and potential risks, even if your work is foundational and seems removed from direct applications.<br><br>5. Engage with Ethical Considerations: Encourage a discussion on ethical considerations related to the deployment of your research findings. This includes considering fairness, accountability, and transparency in the use of Spatio-Temporal Graph Neural Networks for traffic prediction and other applications.<br><br>6. Consider Future Work: In discussing societal impacts, also consider future directions of your research and how they might address or ameliorate potential negative impacts identified.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors in saying "NA" because the paper poses no such risks is consistent with the paper's content and focus. The paper is based on refining and simplifying the architecture search process in Spatio-Temporal Graph Neural Networks (STGNNs), particularly for traffic prediction scenarios. The subject matter inherently does not involve the types of data or models that typically raise concerns about misuse or require special safeguards for responsible release, as outlined in the guidelines.<br><br>However, it's still good to critically assess all aspects of the paper, including the potential future applications of the researched models and methods. The concern about misuse of AI models generally pertains to their ability to generate false or misleading information, privacy issues, or enabling surveillance capabilities beyond intended ethical boundaries. The application area of traffic prediction is less likely to directly engage with these concerns, especially given that the data used and models developed are tightly focused on forecasting traffic patterns.<br><br>Constructive and actionable suggestions:<br><br>1. Data Privacy and Consent: If not already done, the authors should ensure that any datasets used, including traffic data, comply with privacy norms and that there's an explicit mention of consent or anonymization where applicable. This is important even in seemingly benign contexts like traffic data, where individual patterns might inadvertently be trackable.<br><br>2. Ethical Considerations: A brief section discussing the ethical use of the developed models and the potential societal impact of more accurate traffic prediction could enrich the paper. This includes the benefits of improved traffic management and potential drawbacks, such as misapplication of the technology.<br><br>3. Future Misuse Potential: Although the current application may not raise immediate concerns, any technology can be repurposed. A forward-looking commentary about ensuring the ethical use of the technology as it evolves and perhaps intersects with other AI capabilities could be valuable.<br><br>4. Licensing and Open-source Release: If the authors plan to release their code or models, clear licensing that outlines ethical use guidelines can be helpful. It may not prevent misuse but creates a legal and moral framework encouraging responsible use.<br><br>5. Community Engagement: Encourage ongoing dialogue within the research community about the responsibilities of AI researchers in different applications, including those seemingly distant from controversial uses. It helps foster a culture of proactive consideration of potential future implications.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.</span><br><span style='color:#0000FF;'>Review: The authors have properly credited the original creators of the datasets used in their work by explicitly mentioning and citing the sources [13, 1, 10]. Furthermore, they have provided a URL for accessing the datasets, thus facilitating reproducibility and transparency. However, the response does not explicitly mention the license under which these datasets are distributed or the terms of use, which would have been ideal for fully satisfying the guidelines' requirement regarding explicitly stating the license and terms of use.<br><br>Constructive suggestions for improvement:<br>1. License Information: It would be beneficial for the authors to include the specific license under which each dataset is distributed. If the datasets are publicly available on GitHub without explicit licensing information, stating this fact could be useful for readers and future researchers who might wish to use these datasets.<br>2. Terms of Use: If there are specific terms of use associated with the datasets (e.g., limitations on commercial use, required attribution, etc.), mentioning these would provide clarity to readers and help ensure compliance with the original creators' conditions.<br>3. Version and URL: While a URL is provided, the authors might consider specifying the version of the dataset used, if applicable. This detail enhances reproducibility, especially if the dataset undergoes updates or changes over time.<br><br>In summary, the authors have met the basic expectations by citing the sources and providing a link to the datasets. For complete compliance with the guidelines and to enhance transparency, elaborating on the license and terms of use would be recommended in future work.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: The paper does not release new assets.</span><br><span style='color:#0000FF;'>Review: Justification provided in the review (NA for not introducing new assets) seems inconsistent with the content documented in the paper. The paper explicitly states that their code is available on a public GitHub repository, as seen in the introduction section where it mentions "Our code is available at https://github.com/AutoML-Research/SimpleSTG." This indicates that new assets, such as code or models potentially constituting a significant contribution to the field, have indeed been introduced and made available alongside the paper. Therefore, the paper should have been assessed based on the framework for new assets documentation, including but not limited to the quality of the documentation, accessibility, usability, and licensing of the provided code or model.<br><br>Constructive and Actionable Suggestions:<br>1. Assess the GitHub repository linked in the paper for completeness and clarity. Ensure that it contains comprehensive documentation detailing the setup, dependencies, how to run the code, expected outputs, and any other relevant information that facilitates replication and understanding of the research work.<br>2. Evaluate the license under which the code/model is released. It should promote open access and be clear on the permissions granted to the users, such as for modification, distribution, and commercial use.<br>3. Encourage the authors to elaborate on any limitations, prerequisites, and expected environments/settings for running their released code, ensuring a wider accessibility and usability by the research community.<br>4. Suggest that the authors include a discussion on the potential ethically-relevant aspects or impacts of deploying their models, especially if their research is directly applicable to real-world scenarios.<br>5. Finally, prompt the authors to provide a clear versioning of their repository to ensure traceability of changes in relation to the paperâ€™s findings and claimed contributions.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors as "NA" is correct and consistent with the content of the reviewed paper, which focuses on improving the efficiency and effectiveness of architecture search in Spatio-Temporal Graph Neural Networks for tasks like traffic prediction. The paper primarily deals with algorithmic advancements and does not involve human subjects or crowdsourcing experiments. Therefore, the requirement for including the full text of instructions given to participants, screenshots, or details about compensation does not apply to this paper.<br><br>Considering the nature of the work, which is computational and algorithmic, the authors are not expected to include such details. Their focus on proposing a new framework, understanding training hyperparameters, and conducting comprehensive experiments to demonstrate the efficacy of their proposed methods aligns with the scope of the NeurIPS conference and does not involve direct human subject research.<br><br>No suggestions for improvement regarding this specific point are necessary.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content and guidelines. The paper is focused on developing and improving Spatio-Temporal Graph Neural Networks (STGNNs) for traffic prediction, and there is no mention or indication of involving human subjects or crowdsourcing data in their research. Consequently, the concerns related to potential risks to study participants, the disclosure of such risks, and the requirement of Institutional Review Board (IRB) approvals do not apply to this study. The justification provided aligns with the scope and nature of the research, as well as follows the guidelines regarding the exclusion of IRB-related disclosures in studies that do not involve human subjects research.<br><br>However, since actionable and constructive suggestions were requested, it's worth noting that the paper is already within the appropriate boundary concerning this aspect. Should future work or extensions of this research involve human subjects (for instance, in evaluating the user experience of a traffic prediction system), the authors would need to seek IRB approval or its equivalent and ensure ethical considerations are discussed.<br><br>To maintain completeness in addressing the question, no further action is required from the authors regarding this specific query about IRB approvals and potential risks to study participants.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our main claims are listed in section 1 and highlight this paper's contributions.</span><br><span style='color:#0000FF;'>Review: Justification Review:<br>The justification provided by the authors for the question about the main claims accurately reflecting the paper's contributions and scope is somewhat generic and does not directly address the alignment between the claims and the actual contributions of the paper. However, based on the entire content of the paper, it is clear that the abstract and introduction sections do accurately highlight the main contributions and scope, despite the authors' justification not explicitly stating this.<br><br>- The paper claims to address issues with Neural Architecture Search (NAS) in Spatio-Temporal Graph Neural Networks (STGNNs), specifically the integration of hyperparameters into the NAS framework and the reduction of the STGNN search space. These claims are substantiated through the development of a disentangled framework for understanding and simplifying architecture search.<br>- The abstract outlines the contributions, including the proposal of a disentangled framework containing designs for spatial/temporal modules, spatio-temporal order, and skip connection, a substantial exploration of the hyperparameter setting, and thorough experiments to derive fundamental design principles.<br>- The introduction provides context and motivation for these contributions by discussing the challenges in designing STGNNs, the importance of architecture and hyperparameter optimization, and the benefits of the proposed disentangled framework for understanding Spatio-temporal architectures.<br><br>Constructive Suggestions:<br>1. Clarify Claims and Contributions: The authors are encouraged to more explicitly correlate each claim made in the abstract and introduction with specific sections or findings within the body of the paper. This can help reviewers and readers quickly verify the claims.<br><br>2. Highlight Novelty and Impact: Strengthen the wording around the novelty and potential impact of the contributions, especially how the disentangled framework and understanding of hyperparameters might advance the field of STGNNs beyond existing NAS methodologies.<br><br>3. Discuss Limitations and Assumptions: While the guidelines suggest it's fine to include aspirational goals, it would also be beneficial for the authors to briefly discuss any limitations or assumptions underlying their approach directly in the introduction to set realistic expectations.<br><br>4. Comparison with Related Work: Expand on how this work differentiates from existing NAS methods in the context of STGNNs within the introduction. This can provide a clearer scope of the paper's contributions in relation to the current state-of-the-art.<br><br>5. Future Work Directions: Briefly mention potential future work directions or open questions based on the findings, which could indicate to reviewers the paper's contributions towards advancing the field's knowledge frontier.<br><br>Overall, the paper's abstract and introduction do set appropriate expectations for the contributions and scope of the work, even though the authors' justification for the alignment could be more specific.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have discussed the potential limitations in section 6, including the application domain and considered temporal modeling methods.</span><br><span style='color:#0000FF;'>Review: Justification: The paper provides a comprehensive review and novel contributions to the field of Spatio-Temporal Graph Neural Networks (STGNNs). It proposes a disentangled framework to simplify Neural Architecture Search (NAS) for STGNNs, addresses the issue of hyperparameter integration in NAS, and identifies a reduced and efficient search space. However, the response provided to the question regarding the discussion of limitations within the paper is not fully supported by the content presented in the provided excerpts.<br><br>The response indicates that limitations are discussed in section 6, focusing on application domains and considered temporal modeling methods. Upon a detailed review of the content, section 6 or a discussion specifically labeled as "Limitations" was not identified in the provided excerpts. It's essential for the review to critically assess whether the paper indeed discusses the limitations as suggested or if there could be an oversight.<br><br>Additionally, acknowledging limitations is crucial for the scientific integrity of a paper. It provides a clearer perspective for readers on the research's boundaries and areas for future improvement. While the paper outlines a range of contributions and experiments to address issues within STGNN NAS, a structured and explicit discussion regarding the limitations of their approach, assumptions made, scope of the claims, computational efficiency, and potential ethical considerations such as privacy and fairness is essential but seems to be indirectly addressed or not fully covered in the provided excerpts.<br><br>Constructive suggestions:<br>1. Revise the manuscript to include a dedicated section or paragraphs that explicitly discuss the limitations of the work. This should cover not only the application domain and temporal modeling methods but also any assumptions made, potential limitations in the generalizability of the findings across different domains or datasets, computational requirements and scalability, and ethical considerations.<br><br>2. Reflect on the robustness of the results, particularly under different assumptions or violations of those assumptions. Discuss how such scenarios might affect the applicability and performance of the proposed framework.<br><br>3. Consider discussing the scalability of the proposed NAS approach in STGNNs concerning different dataset sizes and the computational resources required. This will provide a more comprehensive understanding of the practical applications of the work.<br><br>4. Address potential ethical considerations, especially if the technology could have broader implications that might raise concerns related to privacy, fairness, or other societal impacts.<br><br>5. Ensure that the limitations and potential future work suggestions are grounded in the experiments and findings detailed in the manuscript, providing a pathway for advancing the field.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have discussed the assumptions in the section 4 when we introduce graph characteristics.</span><br><span style='color:#0000FF;'>Review: Justification:<br>The question regarding the provision of full sets of assumptions and complete/correct proofs for all theoretical results has been answered affirmatively by the authors, citing discussions in section 4 related to graph characteristics. However, upon thorough examination of the presented content, this claim does not align entirely as the paper primarily focuses on the practical application and experimentation surrounding Spatio-Temporal Graph Neural Networks (STGNNs) and their architecture search. The document predominantly discusses the disentangled framework for architecture design, the significance of various hyperparameters, and the methodology for evaluating architectures on spatio-temporal tasks. Though it refers to empirical studies and presents a framework based on observed principles, the paper falls short of providing theoretical results that would require assumptions to be explicitly stated or formal proofs to be offered.<br><br>Constructive Suggestions:<br>1. Clarify Definitions: If the paper presents any theoretical results, ensure that these are clearly defined and distinguished from empirical findings. This could involve stating assumptions explicitly in a separate subsection dedicated to theoretical considerations.<br>2. Proof Details: For any theoretical claims, provide detailed proofs or sketch proofs in the main text and offer full proofs in the supplementary material. This is crucial for the transparency and verifiability of the theoretical claims.<br>3. Reference Supporting Theorems: If the paper relies on foundational theorems or well-known results from the literature, make sure to reference these appropriately and discuss how they support the novel contributions of your work.<br>4. Supplement Material: Consider including a supplemental document that hosts all the mathematical rigor behind the empirical observations, even if the paper's main focus is on practical applications. This could be particularly useful for researchers looking to build on your theoretical foundations.<br>5. Revision of Claims: Re-examine the paper to ensure that any claim of providing full assumptions and complete proofs for theoretical results is accurate. If theoretical results are not the paper's focus, revise the statement to reflect more accurately the nature of the contributions (e.g., empirical findings, framework proposal, performance evaluation).<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.</span><br><span style='color:#0000FF;'>Review: Justification Review:<br>The paper provides a comprehensive exposition of the methodologies, experimental settings, dataset details, implementation information, and results across the main text and appendices, which collectively enable the reproducibility of the main experimental results. The inclusion of a GitHub link to the source code significantly enhances the reproducibility aspect. The detailed explanation across sections 4, 5, and appendices B, C, D, E, including but not limited to dataset descriptions, baseline comparisons, parameter settings, and insights into the architectural choices and hyperparameter implications, offers a clear roadmap for replicating the study's findings. Additionally, the explicit mention of the datasets used (e.g., PeMS datasets) and the problem setting facilitates a straightforward reconstruction of the experimental setup. These elements, in conjunction, address the NeurIPS guideline requirements for reproducibility appropriately.<br><br>Constructive Suggestions:<br>1. Data Accessibility: While the GitHub link is provided, ensuring the dataset links within the repository are functioning and up-to-date will further assist in replication efforts. Explicitly stating any preprocessing steps or dataset splits used would be valuable.<br><br>2. Dependencies and Environment Setup: Including a requirements.txt file or an equivalent to specify the exact versions of libraries and tools used could prevent issues related to software version discrepancies.<br><br>3. Baseline Implementations: If any baseline methods were reimplemented for the study, providing those implementations or detailing any modifications made to existing implementations would enhance clarity.<br><br>4. Additional Implementation Details: Offering more insight into any challenges faced during implementation, how they were resolved, and tips for efficient experimentation could be beneficial for researchers attempting to reproduce or extend the work.<br><br>5. Expanded Results: Sharing raw results data in addition to summary statistics might enable more detailed analyses by other researchers, such as further statistical testing or meta-analysis in future works.<br><br>Overall, the authors have met the NeurIPS reproducibility requirement effectively by providing a detailed methodology, sharing code, and ensuring clear communication of experimental settings and results.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.</span><br><span style='color:#0000FF;'>Review: The answer provided is consistent with the content of the paper reviewed. The authors have clearly stated that the code for their work is available on GitHub at the specified URL, which satisfies the requirement for open access to code with sufficient instructions to faithfully reproduce the main experimental results. This submission aligns well with NeurIPS guidelines on code and data submission, which encourage but do not mandate the release of code and data.<br><br>However, to ensure the most thorough and helpful review, here are some additional, constructive suggestions for the authors regarding the code and data accessibility:<br><br>1. Documentation: Ensure the GitHub repository contains detailed documentation, including a README file with step-by-step instructions on how to set up the environment, install dependencies, and execute the scripts to reproduce the experiments.<br><br>2. Environment Details: Include an environment file (e.g., a `requirements.txt` or `environment.yml` file for Python) specifying the exact versions of the libraries and tools used. This will help in creating an identical environment to prevent potential compatibility issues.<br><br>3. Data Preparation Scripts: Although the paper mentions the datasets used and that they are publicly available, it would be beneficial to include scripts for data preprocessing to ensure that users can reproduce the results from the exact same data format as used in the experiments.<br><br>4. Experiment Reproducibility: Ensure that the scripts provided cover all the main experiments presented in the paper. If any experimental results cannot be reproduced with the provided code due to resource constraints or privacy issues, explicitly mention this and provide simulated or smaller-scale versions of these experiments if possible.<br><br>5. Contact Information: Provide a way for readers to reach out for help, such as an issues section in the GitHub repository or a contact email. This can be invaluable for dealing with unforeseen issues in reproducing the results.<br><br>By addressing these points, the authors can further increase the accessibility and utility of their code and data, enhancing the paper's contribution to the community.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. The paper does extensively discuss training and test details, including data splits, hyperparameters and how they were chosen, optimizer types, etc., throughout various sections and the appendices. This detailed description enables readers to understand the experimental setup and replicate the study if desired. The inclusion of such details is crucial for the transparency and reproducibility of the research, and the authors have adhered to this by providing comprehensive information. Additionally, the paper mentions the availability of the code, which further aids in understanding the implementation details.<br><br>Constructive and Actionable Suggestions:<br>1. While the information provided is extensive, ensuring that the main text succinctly outlines the most critical experimental parameters and referring readers to the appendices for more exhaustive descriptions might enhance readability and immediate comprehension.<br>2. Consider providing a summarized table or framework in the main text that encapsulates elemental training and testing configurations, making it easier for readers to grasp the experimental set-up without necessarily diving into the appendices.<br>3. Ensure the GitHub repository (if not already) contains detailed comments and documentation, especially regarding the hyperparameter selection process and any scripts used for automatic hyperparameter tuning or optimization, to facilitate easier replication and adaptation by other researchers.<br>4. While the appendices provide detailed information, ensuring that they are organized in a manner that is easy to navigate and reference from the main text can further assist in understanding the setup and results.<br>5. Lastly, the authors could consider elaborating on the decision-making process behind certain hyperparameter choices or data split ratios if derived from empirical observations or prior studies, providing additional insights into their research methodology.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.</span><br><span style='color:#0000FF;'>Review: Justification: While the authors claim that error bars, specifically variance, are included in the main experiments to denote statistical significance, the provided content does not explicitly detail how these error bars were computed, nor does it address the considerations outlined in the review guidelines. The guidelines suggest a thorough explanation of the calculation method for error bars, assumptions made (e.g., normally distributed errors), and whether the error bar represents standard deviation or standard error of the mean. Without this level of detail, it's challenging to assess the robustness and reliability of the experimental results fully.<br><br>Constructive and Actionable Suggestions:<br>1. Clearly state the types of variability that the error bars capture (e.g., variations across train/test splits, initialization randomness, or parameter sampling randomness). This helps in understanding the sources of uncertainty in the experiments.<br>2. Offer a detailed explanation of how the error bars were calculated. If a specific statistical method or library function was used, please mention this explicitly.<br>3. Clarify the assumptions made in the calculation of error bars, such as the assumption of normally distributed errors. If assumptions are made, justify their validity in the context of your experiments.<br>4. Specify whether the error bars represent the standard deviation or the standard error of the mean. If possible, consider providing 2-sigma error bars for a more conservative representation of confidence intervals.<br>5. For experiments where error bars are not symmetric due to the nature of the data distribution, explore presenting them in a manner that accurately reflects this asymmetry. This may involve indicating the range or using other statistical measures that account for the distribution's shape.<br>6. Ensure that any sections of the paper mentioning the inclusion of error bars or statistical significance tests also reference the specific figures or tables where these can be visually inspected to enhance reader comprehension.<br>7. Consider including a subsection in the methodology or experimental setup section dedicated to explaining the statistical analysis approach in detail. This addition would greatly benefit readers seeking to understand the statistical underpinnings of your findings.<br><br>By addressing these points, the authors can significantly enhance the clarity and reliability of the experimental results reported in the paper.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.</span><br><span style='color:#0000FF;'>Review: Justification: While the paper does provide information on the GPU hours required for the NAS method comparison, it lacks detailed specifics regarding the type of computational resources involved (e.g., GPU model, CPU specifications, memory capacity, etc.), which are essential for reproducibility. This detail is necessary to fully understand the computational cost and to reproduce the experiments under similar conditions. Moreover, there is no clear mention of whether these reported GPU hours apply to all experiments or specific ones, nor is there detailed information on the compute resources needed for each individual experiment. Lastly, the paper does not disclose if additional compute was required beyond what is reported for the experiments, such as for preliminary tests or failed experiments.<br><br>Constructive Suggestions:<br>1. Detailed Computational Resources: Provide more specific details about the computational resources used for the experiments, including the type and model of GPUs, CPU specifications, available memory, and any use of cloud computing services.<br>2. Breakdown of Compute Requirements: Offer a breakdown of the computational resources and time required for each experiment or set of experiments. This would help in understanding the computational cost associated with different parts of the research.<br>3. Additional Compute Disclosure: If there was additional computational work done that is not directly reported in the paper (e.g., for model development, parameter tuning, failed experiments), it would be beneficial to disclose this to give readers a fuller picture of the compute requirements for the project.<br>4. Execution Environment Description: Include a description of the software environment (e.g., operating system, programming languages, and versions of key libraries or frameworks) used to run the experiments. This aids in reproducing the results with the same or similar software dependencies.<br>5. Guide for Replication: Provide a more detailed guide or supplementary material for replicating the experiments, including scripts for dataset processing, model training, and evaluation. This could potentially include Dockerfiles or environment configuration files to ease the setup process for those attempting to replicate the studies.<br><br>These suggestions aim to improve the clarity and reproducibility of the computational experiments described in the paper, adhering to good scientific practice by enabling others to validate findings through replication.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We review the code of ethics and our paper conform with it.</span><br><span style='color:#0000FF;'>Review: Justification for the<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We discuss the broader impacts in the section "Broader Impact Statement".</span><br><span style='color:#0000FF;'>Review: Justification for Score:<br>The reviewed paper does not explicitly provide a section titled "Broader Impact Statement" nor discusses the potential positive and negative societal impacts as indicated in the author's answer. The content provided in the paper mainly focuses on the technical advancements and methodologies related to Spatio-Temporal Graph Neural Networks (STGNNs) and their application in traffic prediction. While the application in traffic forecasting has inherent societal impacts, such as possibly improving traffic management, reducing congestion, and potentially lowering emissions by optimizing transportation networks, these aspects are not discussed in depth in terms of broader societal implications.<br><br>Constructive Suggestions:<br>1. Explicitly Address Societal Impact: Incorporate a section specifically dedicated to discussing both the positive and negative societal impacts of your work on Spatio-Temporal Graph Neural Networks. This section should go beyond the application use case and delve into broader implications, including but not limited to ethical, privacy, and security considerations.<br><br>2. Potential Positive Impacts: Elaborate on how the improvements in traffic forecasting enabled by your approach could benefit society. For example, discuss how better traffic prediction can lead to more efficient transportation networks, reduced carbon emissions, and decreased time spent in traffic, thereby improving the quality of life.<br><br>3. Potential Negative Impacts: Discuss potential negative impacts related to the deployment of your system. For instance, consider the implications of data privacy (given that traffic data might include sensitive information about individuals' location patterns), the potential for systemic biases in predictions, and how inaccurate predictions could lead to negative consequences.<br><br>4. Mitigation Strategies: Offer suggestions for mitigating any identified negative impacts. This could include approaches to ensuring data privacy, methods to test and remove biases from predictions, and strategies to ensure the reliability of traffic forecasts.<br><br>5. Future Directions: As you lay out the societal impacts, also discuss areas of future research that could further explore these aspects. Highlighting these areas can help guide the community to address these issues proactively.<br><br>These suggestions aim to strengthen the paper by providing a comprehensive view of the work's societal implications, thereby aligning with the conference's expectations for authors to consider the broader impacts of their research.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have discussed the details of the used datasets and models in section 4, 5 and appendix B.</span><br><span style='color:#0000FF;'>Review: Justification provided by the authors does not directly address the specific safeguards for responsible release and prevention of misuse of data or models, as outlined in the question. The reference to sections where datasets and models are discussed does not equate to detailing specific measures taken for safe and responsible release. Addressing concerns about the possible misuse of models, especially those with potential implications for privacy, security, or ethical considerations, requires explicit mention of the safeguards or protocols put in place.<br><br>Constructive and actionable suggestions:<br>1. Clarification and Explicit Measures: The authors should clearly articulate any specific measures they have taken to mitigate the risk of misuse associated with their datasets or models. This could include, but is not limited to, access control mechanisms, usage guidelines, or ethical review processes. If such measures have been taken, they should be described explicitly in the paper.<br><br>2. Ethical Considerations: It would be beneficial if the authors included a dedicated section or paragraph discussing the ethical considerations of their research, particularly focusing on the potential risks for misuse of their models and the data they used. This part should outline any potential negative impacts and how they plan to mitigate these issues.<br><br>3. License and Use Conditions for Datasets and Models: If the authors are releasing any datasets or models, they should consider associating them with licenses that outline permissible uses or incorporate ethical use clauses. Clarifying the licensing can help ensure responsible use by third parties.<br><br>4. Engagement with Institutional Review Boards (IRB): If applicable, the authors should mention any engagement with IRBs or ethics committees regarding their research. This would provide readers with an understanding that the research has undergone scrutiny for ethical considerations.<br><br>5. Community Guidelines: If the authors are part of a larger research initiative or employing platforms for sharing their models/data, they should adhere to and mention any community guidelines or standards for responsible AI research and development.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.</span><br><span style='color:#0000FF;'>Review: The authors have adhered to the requirements regarding crediting the creators or original owners of the assets used in their paper. They have cited the original papers [13, 1, 10] that produced the datasets and provided a URL where these datasets can be accessed. Although the specific version of the dataset and the name of the license under which these datasets are released are not explicitly mentioned in the provided justification, this level of detail is not always feasible or required for publicly available datasets commonly used in the research community. The citation and the provision of access information fulfill the basic requirements of crediting the source and respecting terms of use.<br><br>However, to improve the completeness of the asset acknowledgment and ensure adherence to any specific licensing terms, the following itemized, constructive, and actionable suggestions are offered to the authors:<br><br>1. Include License Information: If possible, add the name of the license under which each dataset is released. This information is crucial for ensuring that future researchers understand their rights and responsibilities when using these datasets.<br><br>2. Versioning Details: If the datasets have multiple versions, specifying which version was used can help in replicating the results and understanding the context of the data better.<br><br>3. Terms of Use: Briefly mention any terms of use or restrictions associated with the datasets, if applicable. This will ensure that readers are aware of any limitations in using the data for certain types of research or applications.<br><br>4. Contact Original Creators: If there are any ambiguities regarding the use of the datasets, considering reaching out to the original creators to clarify terms of use, especially if the research involves sensitive or proprietary information.<br><br>By addressing these points, the authors can enhance the transparency and reproducibility of their research, ensuring that all assets used are credited appropriately and that their use complies with the original terms and conditions.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have included enough documentation in section 4, 5 and appendix B with respect to our introduced benchmarks.</span><br><span style='color:#0000FF;'>Review: Justification for the score: The provided justification in the review does not sufficiently detail the documentation aspects of the new assets introduced in the paper. While it mentions that documentation is included in sections 4, 5, and Appendix B concerning the introduced benchmarks, it lacks specificity on how these sections cover essential documentation elements such as training details, license information, limitations, and consent for use. This vagueness makes it challenging to assess the completeness and adequacy of the documentation provided alongside the new assets.<br><br>Constructive and Actionable Suggestions:<br><br>1. Clarify Documentation Details: Enhance the justification by specifying how the documentation provided in sections 4, 5, and Appendix B addresses key documentation components for new assets. This includes detailing the training process, describing any pre-processing steps, specifying the license under which the assets are released, and noting any limitations or biases inherent in the assets.<br><br>2. Incorporate Structured Templates: Employ structured templates when documenting new assets. These templates should succinctly capture critical information about the dataset, code, or model introduced in the paper, making it easier for readers and future researchers to understand and reuse the assets.<br><br>3. Explicitly Discuss Consent: If the new assets involve data or contributions from individuals, explicitly discuss how consent was obtained. If applicable, include information on ethical considerations and how privacy concerns were addressed.<br><br>4. Anonymize Assets for Review: Ensure that any assets linked or included with the submission are appropriately anonymized for the review process. Provide clear instructions on how reviewers can access these anonymized assets without compromising the blind review integrity.<br><br>5. Link to Repository: If possible, provide a link to a publicly accessible repository where the assets and their documentation are hosted. Ensure that the repository includes a comprehensive README file that guides users through the assets, their intended use, and any setup or installation requirements.<br><br>By addressing these points, the paper can significantly improve the level of detail and transparency regarding the documentation and availability of introduced assets, thereby enhancing reproducibility and fostering further research in the field.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer and justification correctly indicate that the question is not applicable to the content of the paper being reviewed. Upon reviewing the paper's content, it is evident that the described work centers on the development and evaluation of machine learning models specifically for spatio-temporal graph neural networks, with no mention of crowdsourcing experiments, research with human subjects, or any processes that involve human participation in experimentation. Thus, the requirement to include the full text of instructions, screenshots, or details about compensation does not apply to this study.<br><br>Constructive and Actionable Suggestions:<br>Since the answer to the question is correctly marked as N/A based on the paper's focus, there are no specific suggestions related to inclusion of human subject research information. However, to further strengthen the review process and ensure comprehensive assessment, the reviewer can consider addressing the following general aspects if not already done:<br>1. Verify the thoroughness of experimental design descriptions, ensuring that all relevant details are provided to allow for replicability.<br>2. Assess whether the paper adequately discusses the potential ethical implications of the proposed algorithmic solution, particularly if the application domain (e.g., traffic prediction) can have broader societal impacts.<br>3. Ensure that the paper provides a clear explanation of the novelty and contributions in comparison to previous work in the domain, especially in relation to neural architecture search (NAS) techniques and their application in spatio-temporal forecasting tasks.<br>4. Evaluate the clarity and completeness of the methodology section, including the description of the datasets used, model architecture details, and evaluation metrics.<br>5. Review the discussion and conclusion sections for insights into the limitations of the current work and potential future directions, contributing to the ongoing discourse in the field.<br><br>These suggestions aim to maintain the high standards of review and contribute positively to the advancement of research in the field of machine learning and spatio-temporal graph neural networks.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. Based on the extensive description provided, the paper focuses on the development, understanding, and application of Spatio-Temporal Graph Neural Networks (STGNNs) for traffic prediction and does not involve direct human participation, crowdsourcing, or any form of research that would typically warrant Institutional Review Board (IRB) approval. The nature of the research falls within the computational and theoretical domain, analyzing and proposing improvements to architectural search methodologies and hyperparameter understanding in STGNNs. Therefore, the aspects related to IRB approval or potential risks to study participants do not apply.<br><br>Suggestions for improvement:<br>- Although the current research does not involve human subjects, itâ€™s important to mention any secondary implications or ethical considerations related to the application of the proposed models, especially in terms of privacy, data security, or potential misuse in surveillance or monitoring without consent.<br>- Future work that might involve human subjects or personal data, even indirectly, should clearly state the ethical considerations and necessary compliance with institutional review boards or equivalent ethical review processes.<br>- As part of the broader impact or future directions sections, it would be valuable for the authors to discuss any potential societal impacts, both positive and negative, that their research or the application of their findings might entail.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>truth_adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our main claims are listed in section 1 and highlight this paper's contributions.</span><br><span style='color:#0000FF;'>Review: The answer given by the authors is correct based on the content of the paper provided. The abstract and introduction outline the main contributions and scope of the paper accurately. They address the integration of spatial and temporal modules in Spatio-Temporal Graph Neural Networks (STGNNs), the issues with existing Neural Architecture Search (NAS) methods, and propose a disentangled framework to simplify architecture search. These claims directly reflect the contributions detailed throughout the paper, including a new framework for understanding and simplifying architecture search in STGNNs, empirical evidence supporting the efficiency of their proposed methods, and the performance superiority of models discovered through their framework over state-of-the-art models.<br><br>Justification:<br><br>1. The abstract clearly states the problem, the identified issues with existing methods, and the approach taken in this paper to address these issues.<br>2. The introduction provides a more detailed context for the motivation behind the study, the specific contributions of their work, and the basis for these contributions.<br>3. The claims are consistent with the theoretical and experimental results presented in the paper. The authors provide a thorough explanation and justification for their approach, supported by empirical evidence.<br>4. The scope of the paper, as laid out in the abstract and introduction, is indeed covered thoroughly in the subsequent sections. The paper delivers on its promises by detailing the proposed framework, the methodology for the analysis of STGNN architectures, and comparisons with existing models.<br><br>Constructive feedback for the authors to strengthen their presentation might include:<br><br>1. While the abstract and introduction accurately reflect the paper's contributions, it would enhance clarity to specifically mention the measurable impact on the efficiency and performance of NAS for STGNNs early in the abstract. This could help the reader quickly grasp the practical significance of the research.<br>2. The authors could further highlight the novelty of their disentangled framework in the introduction by contrasting it more directly with the limitations of existing approaches.<br>3. It might be beneficial to briefly discuss the potential limitations of their approach or any assumptions made that future research could address, ensuring a balanced view of their work's scope.<br>4. Given the technical depth of the paper, a simplified summary or graphical abstract highlighting the main contributions and how they fit together would help readers not as familiar with the field.<br><br>Overall, the authors have accurately presented the scope and contributions of their paper in the abstract and introduction, providing a solid foundation for the detailed exploration that follows.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We did not discuss limitations in the paper.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for not discussing the limitations of their work is insufficient as per the expectations set in the guidelines for the paper submission. Given the complexity and scope of their work on understanding and simplifying architecture search in Spatio-Temporal Graph Neural Networks (STGNNs), it's critical for the research integrity and for the benefit of the community that limitations are acknowledged and discussed. This could cover a range of aspects such as the generalizability of the proposed method across different datasets, computational efficiency, or how the assumptions made in their model might impact the real-world applicability.<br><br>Constructive and actionable suggestions:<br>1. Assumptions and Robustness: The authors should articulate the assumptions underpinning their model and experiments, such as data distribution assumptions, and discuss the robustness of their findings against deviations from these assumptions.<br>2. Generalizability: Reflect on the scope of the tested methodologies â€” were they validated on a broad range of datasets, including those with significantly different characteristics? If not, it's crucial to outline how this might limit the applicability of the findings.<br>3. Computational Constraints: Given the typically resource-intensive nature of Neural Architecture Search (NAS), itâ€™s pertinent for the authors to discuss the computational efficiency of their proposed method and its scalability.<br>4. Future Directions: Identifying limitations is not just about reflecting on the current work but also about setting the stage for future research. Discussing underexplored areas or unaddressed challenges could pave the way for more comprehensive future studies.<br>5. Impact on Broader Issues: If relevant, discuss any potential limitations in the context of privacy, fairness, or ethical considerations. For instance, does the methodology rely on data that could be biased, or could the approach inadvertently magnify existing disparities?<br><br>In summary, acknowledging and discussing limitations is critical for the transparency and advancement of scientific research. It provides the community with a clearer understanding of the work's boundaries and encourages further exploration and refinement.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not include theoretical results.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper under review. Upon careful examination of the paper's content, there are no sections or parts where theoretical results such as theorems or lemmas are discussed or proved. The focus of the paper is on presenting a novel framework for architecture search in Spatio-Temporal Graph Neural Networks (STGNNs), addressing hyperparameters and search space issues, and empirically validating the proposed methods through experiments. The paper's contributions are mainly methodological advancements and empirical findings rather than theoretical analyses or proofs.<br><br>Constructive and Actionable Suggestions:<br>1. Although this work does not present new theoretical results, for future research that might involve theoretical contributions, it would be beneficial to ensure that any theoretical results are accompanied by clear statements of assumptions, and full proofs are provided either in the main body of the paper or supplementary materials. Short sketches or intuitions for the proofs in the main text can greatly aid readers' understanding.<br>2. In future works that might involve theoretical aspects, it is advisable to systematically number theorems, lemmas, and proofs for easier reference. This structuring will also help in maintaining clarity and coherence in presenting and discussing theoretical results.<br>3. If theoretical results or optimizations are eventually considered, clearly outlining how these theoretical considerations impact the practical deployment and performance of the proposed methods would be insightful. Demonstrating the theoretical underpinnings can enhance the robustness and reliability of the work.<br>4. Lastly, for empirical works such as this one, continuing to align the methodological contributions with comprehensive, comparative experimentation and analysis is crucial. It strengthens the proposed model's claims and provides clear insights into its advantages and potential limitations within the context of existing approaches.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.</span><br><span style='color:#0000FF;'>Review: Justification: The paper provides a comprehensive account of the experimental setup, including descriptions of datasets, baselines, implementations, and results across various sections (sections 4, 5, and appendices B, C, D, E). Moreover, it offers access to the implementation code through a GitHub link, which significantly aids in reproducibility. These elements collectively facilitate a clear path for reproducing the main experimental results, adhering to the guidelines provided.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify Dataset Details: Ensure that the descriptions of the datasets in appendix B and elsewhere in the paper are detailed, including any preprocessing steps or modifications from their original form. This clarity will help researchers understand exactly what data was used and how it was prepared.<br><br>2. Baseline Comparisons: For the baselines listed, ensure that any deviations from the original implementations (if used) or specific parameter settings that differ are clearly noted. This will help others understand potential differences in performance metrics.<br><br>3. Experiment Details: While the paper lays out the experimental settings comprehensively, clarifying any hyperparameters, architectural details, or optimization tricks not covered by the GitHub repository in the paper itself would be beneficial. It ensures that researchers without access to the repository (e.g., due to future availability issues) can still attempt to reproduce the results.<br><br>4. Expand on Implementation Details in Appendices: Given the complexity of the framework and the number of components involved, expanding on certain implementation details in the appendices could provide additional clarity. For example, more details on the configuration of the searched model in appendix E could be useful.<br><br>5. Address Limitations and Challenges in Reproducibility: If there were any particular challenges faced that could impact reproducibility (e.g., computational resources required, limitations of available frameworks, etc.), discussing these openly could provide valuable context for others attempting to reproduce or build upon this work.<br><br>6. Consider Providing a Docker Container: To further enhance reproducibility, consider providing a Docker container or similar virtual environment configuration that encapsulates all the software dependencies required to run the code. This step would minimize issues related to software dependencies and versions.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the guidelines and the content of the paper. They state that both the data and the code are open access, with specific mention of the GitHub repository where the code can be found. This should, in theory, enable reproducibility of the main experimental results as required by the guidelines.<br><br>For the paper to fully meet the expectations for openness and reproducibility, the GitHub repository mentioned must include:<br>1. Detailed instructions on how to run the experiments, including environment setup, dependencies, and exact commands.<br>2. Scripts for data preprocessing to match the formats expected by the codebase.<br>3. An explanation of any steps required to replicate the dataset or access it, acknowledging that the datasets are publicly available as per the references provided in the appendices.<br>4. Scripts or instructions to replicate all experimental results reported in the paper, specifying if any result cannot be reproduced with the provided code and the reasons for the same.<br><br>While the answer is affirmative and aligns with the guidelines, it's critical for the review to assess if the GitHub repository does indeed contain all necessary elements to ensure reproducibility. For a robust review, one would need to verify the provided repository for completeness in documentation and ease of reproducing results.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.</span><br><span style='color:#0000FF;'>Review: Justification for Score:<br>The authors claim to provide all necessary training and test details, including hyperparameter sampling methods and data splits, mentioning that these details are explained in specific sections and an appendix. However, upon review, there are several areas where more specificity could enhance clarity and reproducibility:<br><br>1. Hyperparameter Exploration: While the paper provides an overview of hyperparameter exploration, particularly in the context of reducing the hyperparameter space (e.g., Table 2 and associated text), the justification for selecting certain hyperparameters over others could be more detailed. The ranking strategy for hyperparameters is a novel methodological component, but more explanation on how the rankings were utilized to finalize hyperparameter choices would be beneficial.<br><br>2. Optimizer and Learning Rate Specifics: The paper mentions studying different optimizers and learning rates, detailing their narrowed scopes (Table 2), but lacks deeper discussion on why specific values within the "Reduced Scope" were chosen for the final experiments. The impact of hyperparameter choices (e.g., Adam vs. AdamW and different learning rates) on model performance would offer valuable insights.<br><br>3. Dataset Split and Evaluation Metrics: Appendix B provides dataset statistics, but more information on the test and validation splits and the reasoning behind their proportions would strengthen the experimental setup's transparency. Furthermore, while multiple evaluation metrics (MAE, RMSE) are used, clarity on their choice and any dataset-specific considerations would be useful, especially for non-traffic datasets.<br><br>4. Reproducibility Concerns: The paper's attempts to provide comprehensive experimental details are commendable, but the inclusion of specific version numbers for software and libraries used, along with more explicit command-line instructions for training and evaluation, would greatly enhance reproducibility.<br><br>Actionable Suggestions:<br>1. Expand on Hyperparameter Selection Justification: Include a more detailed rationale for final hyperparameter choices, possibly with comparative results showing the impact of different hyperparameters on model performance.<br><br>2. Clarify Dataset Splits: Explicitly state the proportions used for training, validation, and test splits across datasets and justify these choices to assure readers of the robustness and generalizability of the experiments.<br><br>3. Detail Versioning and Code Specifics: Ensure that the paper or supplementary material includes exact version numbers for all software and libraries, alongside any relevant code snippets or command-line instructions to facilitate reproduction of results.<br><br>4. Enhance Discussion on Optimizer and Learning Rate Choices: Provide a more detailed discussion on the choice of optimizer and learning rate, including a brief summary of the performance implications observed during exploration. This would offer readers deeper insights into the modeling decisions.<br><br>Improving the specificity and clarity of the experimental details as suggested would significantly enhance the paper's relevance, reproducibility, and contribution to the field, addressing the gaps identified in the response to the checklist question.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.</span><br><span style='color:#0000FF;'>Review: Justification: While the paper does state that main experiments include variance as error bars in the results, the provided excerpts do not offer specific details on how these error bars are calculated, whether the distribution of results is normal, and whether 1-sigma or 2-sigma error bars are used. The guidance requires a detailed explanation of the calculation methods, assumptions made, and an explicit mention of the factors contributing to variability. The paper's content, as provided, lacks these explanations.<br><br>Constructive suggestions:<br>1. Methodology for Error Bars: Include a subsection or paragraph that explicitly explains how the error bars were calculated. If a specific statistical test or method was used, such as bootstrap or a library function, mention this and provide references or formulas if necessary.<br><br>2. Assumptions and Variability Factors: Clearly state any assumptions made during the calculation of statistical significance, such as the presumption of normally distributed errors. Additionally, detail the sources of variability in your experiments, whether those come from different train/test splits, initialization procedures, or any stochastic elements of the models or training process.<br><br>3. Detailed Statistics: When reporting error bars, specify whether they represent one standard deviation (1-sigma), two standard deviations (2-sigma), standard error of the mean, or another measure. This detail is crucial for readers to accurately interpret the significance and reliability of the results.<br><br>4. Explanation of Symmetry/Asymmetry in Error Bars: If the distribution of results is not symmetric, consider reporting confidence intervals or median with interquartile ranges instead of mean with standard deviation/error. If symmetric error bars are used, justify this choice based on the distribution of the results.<br><br>5. Reference to Error Bars in Text: Ensure that there is a clear reference in the text to where error bars can be found (e.g., specific figures or tables), and discuss the statistical significance of the findings in light of these error bars. This discussion can help readers better understand the reliability and potential variability of the results.<br><br>Implementing these suggestions would not only align the paper with the checklist's requirements but also enhance the clarity and credibility of the reported results for the readers.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.</span><br><span style='color:#0000FF;'>Review: Justification:<br>The provided justification does not fully align with the comprehensive guidelines provided for evaluating the sufficiency of disclosed information regarding computational resources used for experiments. While the paper mentions GPU hours in Section 5, which provides some insight into the compute time required, it lacks detailed information on the type of computational resources (e.g., specific GPU models, CPU types), memory, and storage capabilities. Additionally, the paper does not address the use of internal clusters or cloud providers nor does it give a breakdown of compute requirements for individual experimental runs or discuss the compute resources related to preliminary or failed experiments.<br><br>Constructive and Actionable Suggestions:<br>1. Detailed Compute Resources: The authors should include specific details about the computer resources used, such as the types and models of GPUs or CPUs. Details about the memory and storage capabilities of these resources should also be provided.<br><br>2. Breakdown of Compute Requirements: It would be beneficial for the authors to offer a more granular breakdown of the compute resources required for each experiment. This breakdown can help researchers with varying levels of computational resources understand the feasibility of reproducing the experiments.<br><br>3. Use of Cloud Providers or Internal Clusters: If any internal clusters or cloud-based services were utilized for the experiments, the authors should disclose this information, including any relevant configurations and the potential impact on reproducibility for those lacking access to similar resources.<br><br>4. Account for All Experiments: The authors should consider discussing the compute resources involved in preliminary or failed experiments, if any. This addition would provide a more comprehensive view of the computational expense related to the research project as a whole, which is valuable for readers attempting to gauge the total resource investment required.<br><br>5. Ethical Considerations and Accessibility: Include a discussion on the ethical considerations of computational resource consumption and the steps taken to ensure the research is as accessible as possible to the broader community, considering the varying levels of resources available to different researchers.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We review the code of ethics and our paper conform with it.</span><br><span style='color:#0000FF;'>Review: The authors have stated their compliance with the NeurIPS Code of Ethics, which is an affirmation of adherence unless evidence suggests otherwise. Given the provided information, there's no basis to doubt their compliance. However, it is crucial to encourage thorough and transparent documentation of ethical considerations, especially in research involving innovative methods like Neural Architecture Search (NAS) and Spatio-Temporal Graph Neural Networks (STGNNs). The potential implications of these technologies on privacy, fairness, and societal impact warrant careful consideration. While the authors have not detailed specific ethical considerations in their justification, the absence of visible ethical issues in the paper content supports their claim of compliance.<br><br>Constructive Suggestions:<br>1. Ethical Considerations Section: Consider adding a section dedicated to discussing the ethical implications of your work, even if the conclusion is that there are no significant ethical concerns. This can include considerations around data privacy, potential misuse of the technology, and how the model's predictions might impact real-world decisions.<br><br>2. Bias and Fairness Analysis: Given the application in traffic prediction and potentially other areas, it's important to discuss if and how the model's predictions could inadvertently favor or discriminate against certain groups or regions. Including an analysis of the model's fairness and biases would improve the paper's adherence to ethical guidelines.<br><br>3. Impact on Stakeholders: Reflect on how your research might affect various stakeholders, including the public, government agencies, and those involved in traffic management. Highlighting positive impacts, potential risks, and mitigation strategies would demonstrate a thorough ethical review.<br><br>4. Data Privacy and Consent: If applicable, clarify the origins of the datasets used, ensuring that data was collected and processed in compliance with privacy laws and regulations. Discuss the measures taken to protect individuals' privacy, especially in datasets involving real-world traffic data.<br><br>5. Model Misuse: Address the potential for misuse of your research findings or technology and propose strategies to prevent such scenarios. This could include recommendations for secure deployment or limitations on the use of technology.<br><br>By addressing these suggestions, the paper would not only conform to the NeurIPS Code of Ethics but also set a robust example for ethical considerations in computer science research.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We does not discuss the broader impacts. â€¢ The answer NA means that there is no societal impact of the work performed. â€¢ If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. â€¢ Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. â€¢ The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. â€¢ The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. â€¢ If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: Justification: While the authors claim that their work does not discuss broader impacts and thus justify their response with a "No," this review finds the justification not fully consistent with the expectations outlined in the given guidelines. Given the nature of the work on Spatio-Temporal Graph Neural Networks (STGNNs) and their applications in traffic prediction, there are clear pathways to societal impacts, both positive and negative, that warrant discussion.<br><br>Constructive and actionable suggestions for the authors include:<br><br>1. Discuss Potential Positive Impacts: The authors should elaborate on how advancements in STGNNs could contribute to societal benefits, such as improving traffic management, reducing congestion, and thereby decreasing carbon emissions. Enhanced traffic prediction models can also lead to better urban planning and emergency response strategies.<br><br>2. Address Possible Negative Impacts: The paper should consider discussing potential risks associated with the deployment of advanced traffic prediction models. For instance, the misuse of traffic data and predictions could lead to privacy concerns or the unfair targeting of specific areas or populations. The authors should reflect on how their work could be misused or lead to unintended consequences.<br><br>3. Consider Fairness and Bias: The authors should assess if their proposed methods could inadvertently perpetuate or exacerbate biases present in traffic prediction data. This includes examining whether the model's predictions are equitable across different geographic areas and demographically diverse populations.<br><br>4. Privacy Considerations: Given that traffic prediction models often rely on collecting and analyzing large volumes of data, the authors should discuss privacy implications and how they plan to ensure the responsible use and protection of data.<br><br>5. Mitigation Strategies: If the paper identifies potential negative societal impacts, the authors should propose mitigation strategies. This could include recommendations for policymakers, suggestions for ethical guidelines in deploying such models, or technical solutions to prevent misuse.<br><br>6. Safeguards and Responsible Release: Although the authors marked NA for safeguards, they should still reflect on any necessary precautions if their models or datasets are shared publicly. This includes considering licenses that emphasize ethical use, privacy protections, and restrictions to prevent misuse.<br><br>In summary, the answer does not fully recognize the broad spectrum of societal implications associated with advancements in STGNNs for traffic prediction. The paper would benefit from a more comprehensive discussion of both positive and negative societal impacts, along with considerations for responsible research and application of these technologies.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: The response given by the authors, marked as NA with the justification that the paper poses no such risks, is consistent with the content presented in the paper. The focus of the research is on developing a framework for understanding and simplifying architecture search in Spatio-Temporal Graph Neural Networks (STGNNs) for applications such as traffic prediction. The paper does not introduce new datasets, nor does it propose models with inherent risks for misuse similar to what might be found in language or image generation models. Furthermore, there is no indication that the models or any compiled dataset involved poses a high risk for dual-use or misuse.<br><br>Justification for Agreement:<br>1. The paper's primary contribution is methodological, aimed at improving efficiency and accuracy in neural architecture search for STGNNs.<br>2. The applications discussed, such as traffic prediction, are generally aimed at solving practical challenges rather than generating content that could be misused.<br>3. The paper explicitly mentions making their code available, indicating a commitment to transparency, but does not discuss releasing models or datasets that could raise ethical or misuse concerns.<br><br>Constructive and Actionable Suggestions:<br>Though the paper is marked correctly as posing no high risks for misuse, the authors could strengthen their work by briefly discussing ethical considerations related to traffic prediction or similar applications of STGNNs. For instance, they could mention:<br>- The importance of preserving user privacy when dealing with spatio-temporal data.<br>- Potential biases in traffic prediction models and ways to mitigate them.<br>- The significance of ethical AI development practices in their research.<br><br>This discussion would not only clarify their stance on responsible AI development but also might guide future research in this area towards more ethical and safe practices.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.</span><br><span style='color:#0000FF;'>Review: Justification:<br>While the answer indicates that the datasets used in the paper were cited and a URL was provided, it does not fully address the licensing information, terms of use, or copyright concerns as required by the guidelines. It is essential not only to cite the original sources of the datasets but also to clearly mention the licensing terms and confirm that these terms are respected in the paper. The provided answer lacks specific mention of the licensing details for the used datasets, which is critical for compliance with ethical and legal standards in research publications.<br><br>Constructive and actionable suggestions:<br>1. Explicitly state the licensing terms for each dataset used in the study. If the datasets are public and have a specific license (e.g., CC-BY, MIT, GPL), this should be clearly mentioned in the paper.<br>2. If the datasets' terms of use have specific conditions for acknowledgment or restrictions on use, make sure these are explicitly mentioned and that your use complies with these terms.<br>3. If the datasets do not have clear licensing information, consider reaching out to the original creators to clarify the terms under which the data can be used and mention this communication in the paper.<br>4. For completeness and transparency, include a brief statement in the paper about how the data were used in compliance with the original licensing terms. This could be part of the methodology section or an acknowledgment.<br>5. If your paper releases any code, models, or derived datasets based on these original datasets, specify the license under which your contributions are released, ensuring it is compatible with the original datasets' licenses.<br>6. Lastly, consider adding a separate subsection or an appendix that consolidates all information related to asset licensing, copyright, and terms of use for the datasets and any software used in the research. This centralized information would make it easier for readers and reviewers to assess compliance with ethical guidelines.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: The paper does not release new assets.</span><br><span style='color:#0000FF;'>Review: Justification: The justification provided in the answer "NA" contradicts the information found in the paper. The paper introduces a novel framework for "Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks," along with conducting comprehensive experiments to derive certain principles for designing STGNN architectures. Moreover, it presents a simplified method, termed "SimpleSTG," for searching better STGNN models. It's mentioned explicitly that the code related to this research is made available on GitHub, which constitutes the release of new assets. Therefore, stating that the paper does not release new assets is not accurate.<br><br>Suggestions:<br>1. The review should acknowledge the introduction of new assets, specifically the "SimpleSTG" method and the associated code on GitHub.<br>2. The review should assess the documentation provided for these assets. Considerations should include whether the GitHub repository contains a README file with details on how to use the code, the requirements (e.g., library versions), example scripts, and any notes on the architecture or the experimental setup that would help in reproducing or building upon the work.<br>3. If the paper lacks detailed explanation or documentation on the assets' use, applicability, limitations, and how consent was obtained (if applicable), these should be highlighted as areas for improvement. For instance, the paper could be asked to include a discussion on the potential limitations of the proposed "SimpleSTG" method or details on the datasets' licenses.<br>4. Suggest that authors ensure the provided GitHub link is accessible and anonymized if required by the submission guidelines. They should also ensure the repository remains available and maintains anonymity until the review process is concluded.<br><br>By addressing these aspects, the review will provide a thorough and critical assessment of the documentation and release of new assets as per the guidelines provided.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The response provided by the authors is consistent with the content of the paper. Upon reviewing the paper's abstract, introduction, methodology, experiments, and conclusions sections, there is no mention or indication that crowdsourcing experiments or research involving direct human subjects were conducted. Instead, the focus of the research is on understanding and simplifying the architecture search of Spatio-Temporal Graph Neural Networks (STGNNs) for multivariate spatio-temporal forecasting tasks, such as traffic prediction, which does not inherently require human subject studies or crowdsourcing experiments.<br><br>Given this context, marking the question as "NA" (Not Applicable) is accurate and correct. Since the paper's methodologies and experiments are centered around computational models and algorithmic development without direct human participation, there is no need for the inclusion of instructions given to participants, screenshots of the experiments involving humans, or details about compensation, as none of these apply to the work presented in the paper.<br><br>Suggestions for Improvement:<br>1. None - The authors' handling of this question is appropriate given the nature of their research. Their work does not involve crowdsourcing or human subjects, so the guidelines for reporting on such experiments do not apply.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the paper content, and the justification correctly states that the paper does not involve crowdsourcing nor research with human subjects. Therefore, there's no requirement for Institutional Review Board (IRB) approvals or disclosures about potential risks to study participants. As this work focuses on the evaluation and propositions related to Spatio-Temporal Graph Neural Networks (STGNNs) for traffic prediction and architecture search, human subjects are not directly involved in the experiments or data collection. The research primarily deals with computational models, algorithm development, and analysis of traffic prediction performance, which do not entail ethical concerns related to human subjects.<br><br>Constructive and actionable suggestions are not applicable in this case since the question about IRB approvals or equivalent review processes does not pertain to the nature of the study conducted in the paper.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h3> Post Submission Survey</h3><p><a href='https://docs.google.com/forms/d/e/1FAIpQLSfRIDkcXFbsOrR09j4qA1MlG4Rfir2lPD_u9YC4eqKBJ8tHkw/viewform?usp=pp_url&entry.463237339=VW5kZXJzdGFuZGluZyBhbmQgU2ltcGxpZnlpbmcgQXJjaGl0ZWN0dXJlIFNlYXJjaGluIFNwYXRpby1UZW1wb3JhbCBHcmFwaCBOZXVyYWwgTmV0d29ya3M='>Click here to submit a post submission survey</a></p><br><br><br><br><br>