<h2>ACTIVMETAL:Algorithm Recommendationwith Active Meta Learning</h2><br>--------------------------------------<br>[+] Correctness Rate (genuine): 0.67<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Rate (adversarial): 0.67<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Rate (truth_adversarial): 0.4<br>--------------------------------------<br>--------------------------------------<br>[+] Resiliance Score: 0.47<br>--------------------------------------<br>--------------------------------------<br>[+] Combined Score: 0.1<br>--------------------------------------<br><h1><strong>genuine</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The abstract and introduction clearly state the paper's main contributions and scope.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors, stating that the abstract and introduction accurately reflect the paper's contributions and scope, is consistent with the content of the paper. The abstract clearly outlines the novel approach of algorithm recommendation through active meta-learning, highlighting its application to model selection. It also states the paper’s methodology of using a collaborative filtering recommender system and compares the proposed CofiRank algorithm with baseline methods across real-world and artificial datasets. Similarly, the introduction sets the stage by discussing the significance and current progress in the field of AutoML, positioning the study's contribution within this context.<br><br>Justification for the agreement:<br>- The paper indeed presents an active meta-learning approach called ACTIVMETAL for algorithm recommendation, which is precisely what is claimed in the abstract and introduction.<br>- The scope of the study, including the comparison of the proposed method with baseline approaches across specific datasets, is outlined early in the document, accurately reflecting the subsequent sections of the paper.<br>- The introduction provides a thorough background, motivating the need for such research, which aligns with the claims made about the paper’s contributions to the field.<br><br>Constructive suggestions for further clarity:<br>1. Clarify Limitations and Assumptions: While the paper does mention planning future work and briefly discusses computational considerations, explicitly stating any assumptions made (such as datasets having no missing values except for the new dataset being tested) and limitations in the introduction or abstract could enhance clarity and set realistic expectations for the reader.<br>2. Highlight Novelty More Explicitly: The introduction does a good job of background setting, but emphasizing the novel contributions of this research compared to existing work (such as AUTOSKLEARN and ALORS) could be made more prominent. Making a distinct section for contributions could help in highlighting how this work advances the state-of-the-art.<br>3. Generalization Claims: The paper claims the methodology could be applicable to a broader range of problems beyond classical machine learning settings. Providing a brief discussion or speculation on how and in what specific areas this approach could be extended would enrich the introduction section and better frame the paper’s aspirational goals.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The limitations are discussed in the section “Discussion and conclusion".</span><br><span style='color:#0000FF;'>Review: The provided answer aligns correctly with the paper's content regarding discussing the limitations of the work performed by the authors. As stated in the "Discussion and conclusion" section, the authors clearly identify and discuss several limitations, including:<br><br>1. The simplicity of their meta-learning algorithm in comparison to complex real-world requirements such as dealing with different types of tasks and algorithms beyond their current scope.<br>2. The computational efficiency and scalability concerns when dealing with larger meta-learning datasets and more extensive algorithm evaluations.<br>3. Assumptions about the computational cost of different algorithms being similar, which might not hold true for all cases and could affect the applicability of their approach in diverse scenarios.<br>4. The active learning method's leaning toward exploitation rather than exploration, leaving room for improvement in balancing these aspects for better performance across varying datasets.<br>5. The need for future research to consider the computational expense more thoroughly, potentially including the duration of each algorithm in the assessment for a better trade-off between accuracy and computation time.<br><br><ul><li>Constructive Suggestions*:</li><br></ul>- To further strengthen the paper, the authors could explore methodologies for incorporating computational time directly into their algorithm selection process, potentially through a more nuanced analysis or machine learning techniques designed to predict algorithm performance based on dataset characteristics and algorithm complexity.<br>- Consider expanding the scope of tested datasets and algorithms, including more diverse and challenging cases, to evaluate the robustness and applicability of their approach across a broader range of scenarios.<br>- Investigate and develop strategies for better managing the exploration-exploitation trade-off in active learning settings, potentially including mechanisms for adaptive exploration based on the learning context.<br>- Detail potential strategies or future research directions aimed at addressing the identified limitations, providing a clear road map for how the work could evolve to overcome these challenges.<br>- Engage in a deeper discussion on the ethical considerations, such as privacy and fairness, that could arise from the application of their approach in different domains, and propose preliminary guidelines or considerations to mitigate potential negative impacts.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not include theoretical results.</span><br><span style='color:#0000FF;'>Review: The response of "NA" from the authors is consistent with the content of the paper. After reviewing the paper, it's clear that the focus is on presenting an empirical study of an algorithm recommendation system utilizing active meta-learning and collaborative filtering. The study does not introduce new theoretical concepts that would necessitate formal proofs or the elaboration of theoretical assumptions in the traditional mathematical sense. Rather, it contributes to the field through the development and empirical validation of a novel approach to algorithm recommendation, comparing its performance with existing methods using benchmark datasets.<br><br>The setup of the problem and the design of the algorithm (ACTIVMETAL) are explained conceptually and empirically rather than through theoretical formulations. Thus, there are no theorems, hypotheses, formulas, or mathematical models presented which would require proofs or a detailed discussion of assumptions in a mathematical/theoretical framework.<br><br>Constructive suggestions for the authors, should theoretical aspects be introduced in future iterations of the work, include:<br><br>1. Clearly State Assumptions: If theoretical results are introduced, ensure that all assumptions are clearly stated or referenced. This will aid readers in understanding the conditions under which your results hold and how they might extend to other contexts.<br><br>2. Provide Detailed Proofs: For any theoretical claims, provide complete and clear proofs, either within the main body of the paper or as part of the supplementary material. If opting for the latter, consider including a proof sketch or intuitive explanation in the main text.<br><br>3. Theorem Numbering and Cross-Referencing: Adopt a systematic approach to numbering theorems, lemmas, propositions, etc., and ensure they are properly cross-referenced throughout the paper. This improves readability and helps readers navigate the document.<br><br>4. Reference Relied Upon Theorems and Lemmas: Make sure to properly cite any external theorems, lemmas, or significant proofs that your work directly relies upon. This not only provides due credit but also helps situate your contribution within the broader academic conversation.<br><br>These suggestions are moot for the current version of the paper, as it does not include theoretical content necessitating such considerations. Nevertheless, they are offered as guidance for potential future expansions of the work or other projects that may incorporate theoretical elements.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public.</span><br><span style='color:#0000FF;'>Review: Justification: The answer provided by the authors overstates the availability and sufficiency of information for reproducing the experimental results. While the paper does indicate that algorithms and their code are accessible to the public, and provides pseudo-code for key algorithms, several pieces of critical information seem to be either missing or not fully detailed, according to the excerpts provided. Specifically, the paper does not discuss the process of collecting data, nor does it elaborate on the specifics of how the algorithms were implemented beyond pseudo-code, which might not cover all implementation nuances. The reported supplemental material appears crucial for full reproducibility but is merely referenced without clear in-text description or guidance on how it substantiates the main results.<br><br>Suggestions for the Authors:<br><br>1. Explicit Detailing of Computational Environment: Provide detailed information about the computational environment (e.g., hardware specifications, software versions) in which the experiments were conducted. This ensures that other researchers can align their setup to match yours as closely as possible.<br><br>2. Data Preparation Steps: Elaborate on the steps involved in preparing the datasets for the experiments. Given the mention of preprocessing steps (e.g., global normalization, taking square roots for StatLog), detailed descriptions of these steps are essential for reproducibility.<br><br>3. Detailed Implementation Notes: Complement pseudo-code with implementation notes that cover aspects not easily conveyed through pseudo-code. This includes choices of library functions, parameter settings, and any non-trivial coding decisions that could affect the experimental outcome.<br><br>4. Access to Supplemental Material: Ensure that references to supplemental material are complemented with explicit guidance on how such material supports reproducibility. Offer direct links to any repositories or supplementary files and provide an overview or index of what researchers can find in these resources.<br><br>5. Describe Evaluation Metrics and Procedures: Clearly describe the evaluation metrics and statistical methods used to analyze the results. For experiments involving average scores or error bars, detail how these averages and error ranges are computed.<br><br>6. Dependence on External Libraries or Tools: If your experiments rely on external libraries or software (e.g., CofiRank, Docker), provide detailed versions and configurations used. This includes instructions for setting up these tools in a manner that mirrors your experimental setup.<br><br>7. Clarify Access to Data and Code: While the paper states that datasets and code are available on GitHub, a direct link to the specific GitHub repository should be provided within the main manuscript. If possible, include a DOI for the repository to ensure permanent availability.<br><br>By addressing these points, the paper can significantly improve its reproducibility and allow the community to both verify and build upon the presented work.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The github repo for reproducibility is given, in which you can find the code and dataset.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. The paper mentions that for full reproducibility of their results, datasets and code are available on GitHub, and it is recommended to use the Docker image they built for this purpose. Additionally, the authors note that their repository includes a Jupyter notebook with additional graphs referred to in the text, contributing to easier reproduction of the results.<br><br>Justification Review and Suggestions:<br>- The justification aligns with the guidelines provided for ensuring open access to data and code for reproducing the main experimental results. Mentioning the availability of a GitHub repository and the recommendation to use a Docker image suggests that the authors have made efforts to comply with the reproducibility criteria.<br>- While the answer and justification seem appropriate, it is critical to ensure that the GitHub repository contains not only the code and datasets but also detailed instructions on how to use them. This includes the exact commands needed, information on the computing environment, and instructions for data preparation and script execution.<br>- To enhance this submission further, the authors could:<br>1. Ensure that the GitHub repository is well-documented, with a README file that clearly outlines step-by-step instructions for reproducing the experiments.<br>2. Include information about any dependencies or specific software versions required to run the code.<br>3. If not all experimental results are reproducible due to constraints, clearly state which results can be reproduced and the reasons for any limitations.<br>4. Since the paper is under review and anonymity is required, verify that the GitHub repository and Docker image do not contain identifiable metadata.<br><br>Overall, the commitment to open science and reproducibility is commendable. Ensuring the completeness and clarity of the instructions and resources provided will further strengthen the submission.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results".</span><br><span style='color:#0000FF;'>Review: The provided justification and the answer seem inconsistent with the detailed content of the paper, particularly concerning the level of detail provided about training and test setups necessary for fully understanding and replicating the results. Upon review of the content summaries provided, the paper does discuss the benchmark datasets used for validation and introduces the algorithms (including initialization schemes and selection of algorithms for evaluation on new datasets). However, specifics about data splits, hyperparameters (how they were chosen or optimized), or the type of optimizer used in experiments (if applicable) are not explicitly detailed in the provided summary. These elements are crucial for replicating the results and understanding the methodology fully.<br><br>Constructive and actionable suggestions include:<br><br>1. Detailed Methodology Description: Amend the manuscript to include explicit details about data splits (how the datasets were divided into training and testing), including the ratio or methodology used.<br><br>2. Hyperparameters Justification: Provide a section or amend existing sections to articulate how hyperparameters were chosen or optimized. This could include discussion on any hyperparameter tuning process, fixed values based on literature, or rationale for their selection.<br><br>3. Optimizer Details: If any optimizer was used as part of the model training or algorithm recommendation process, detail the type of optimizer, its settings, and the rationale for its choice.<br><br>4. Enhance Reproducibility: To strengthen the paper's contribution and aid in reproducibility, consider including an appendix or supplemental material section that encapsulates all the technical details related to training and testing. This could also be addressed by ensuring the provided code (if available) is well-documented with comments explaining critical decisions related to experiment setups.<br><br>5. Benchmark Comparison: Where applicable, discuss how the experimental setup compares with the setups used in benchmarking studies or the setups of the baseline models for a clearer understanding of the results' significance and fairness in comparison.<br><br>Including these details will not only strengthen the paper's transparency and reproducibility but will also enhance its utility for readers and practitioners aiming to apply or extend the proposed methods.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results" and the figures in this section.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the contents of the paper, as the "Results" section and accompanying figures include details about statistical significance, specifically the presentation of 1-sigma error bars in Table 2 and the discussion about the significance of the performance improvement of their proposed method over baselines.<br><br>However, to improve clarity and completeness, the authors could consider augmenting their statistical analysis with additional information:<br><br>1. Clarify the variability factors captured by the error bars, such as whether they account for variability across different datasets, algorithm initializations, or other experimental conditions.<br>2. Expand on the method used to calculate the error bars. For example, if bootstrapping or a specific library function was used, providing this detail would enhance reproducibility.<br>3. State explicitly whether the error bars represent standard deviations, standard errors of the mean, or confidence intervals. This clarification would assist readers in correctly interpreting the results.<br>4. In the case of asymmetrical distributions, consider providing confidence intervals or clearly denoting the asymmetric nature of the error bars to avoid misleading interpretations.<br>5. Enhance the text to explicitly reference figures or tables where error bars are shown, explaining in the main text how these were calculated or the statistical significance tests used.<br><br>These suggestions aim to further strengthen the paper by ensuring its statistical analyses are not only correctly defined but also transparently reported, aiding in the reproducibility and rigorous evaluation of the experimental results.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We only discussed the related computational expenses.</span><br><span style='color:#0000FF;'>Review: The given answer correctly identifies a shortfall in the provided paper regarding the detailed reporting of computational resources used for the experiments. The authors mention some discussion of computational expenses but do not provide specific information such as the type of hardware (CPU/GPU), the amount of memory, storage requirements, or execution times necessary for reproducing the experiments. This lack of detail indeed falls short of the guidelines provided, which ask for comprehensive reporting on computational resources.<br><br>However, while I agree with the answer's correctness in pointing out the paper's deficiency in this aspect, it's imperative to offer constructive, actionable suggestions for improvement. The authors could enhance their paper by incorporating the following:<br><br>1. Detailed Specifications of Computational Environment: The authors should specify the types of compute workers used (e.g., CPU, GPU), whether these were accessed through an internal cluster or cloud providers, and the models or specifications of these resources.<br><br>2. Memory and Storage Needs: Include information about the memory requirements and storage needs for executing the experiments. This is especially crucial for computational experiments where resource limitations could affect reproducibility.<br><br>3. Execution Time: For each experiment, the authors should provide the execution time, helping readers gauge the computational cost and feasibility of reproducing the results.<br><br>4. Estimate of Total Compute: Offering an estimate of the total compute required for reproducing the experiments, including any preliminary or failed experiments not detailed in the paper, would provide a clearer picture of the project's computational demands.<br><br>5. Explicit Disclosure: If applicable, the paper should explicitly disclose any substantial computational resources required beyond what was directly reported for the experiments, such as for preliminary testing or development of the methodology.<br><br>Adding this information would not only meet the NeurIPS guidelines but also improve the paper's transparency and reproducibility, essential criteria for the research community.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We reviewed the code of ethics and our paper conforms with it.</span><br><span style='color:#0000FF;'>Review: While the authors affirm that their work conforms to the NeurIPS Code of Ethics, the justification provided lacks depth and specificity. Simply stating that the paper conforms with the code of ethics without elaboration does not provide assurance that all aspects of the code were thoroughly considered and adhered to. This is especially pertinent given the complexity and breadth of ethical considerations in AI research, including data privacy, fairness, transparency, and potential societal impacts.<br><br>Constructive and Actionable Suggestions:<br><br>1. Elaborate on Ethical Considerations: It would be beneficial for the authors to include a section or a paragraph detailing their considerations of ethical aspects relevant to their research. For instance, discussing how they ensure data privacy in the datasets used, addressing any potential biases in algorithm recommendations, and considering the implications of automated algorithm selection on fairness and transparency.<br><br>2. Data Privacy and Usage Permissions: Given that the paper uses real-world datasets from various sources, the authors should ensure and state that all datasets were used in accordance with respective data privacy laws and that necessary permissions were obtained.<br><br>3. Algorithm Bias and Fairness: The authors should discuss whether they evaluate the potential biases their method might introduce or exacerbate in algorithm recommendation. If biases are found, it is essential to mention steps taken to mitigate them.<br><br>4. Transparency and Reproducibility: While the authors provide supplemental material and code for reproducibility, they should also ensure that the method’s decision-making process is transparent. This includes discussing the interpretability of the active learning method and the collaborative-filtering algorithm (CofiRank) used.<br><br>5. Impact on Research and Practice: Briefly discuss the potential societal and ethical impacts of their work on the broader field of AutoML. Consider both positive impacts (e.g., making machine learning more accessible) and potential negative consequences (e.g., misuse of algorithm recommendation systems, over-reliance on automated systems without understanding underlying principles).<br><br>6. Compliance and Anonymity Preservation: Reiterate compliance with the NeurIPS anonymity policy within the ethical considerations section, ensuring that no direct or indirect identification of the authors or their affiliations is present.<br><br>By addressing these suggestions, the authors can strengthen the ethical considerations of their work, making a more compelling and ethically sound contribution to the field of machine learning and AutoML.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Our paper doesn't discuss potential positive societal impacts and negative societal impacts of the work performed.</span><br><span style='color:#0000FF;'>Review: Justification for<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: The response "NA" provided by the authors is consistent with the paper content. The paper introduces a meta-learning approach for algorithm recommendation, which does not involve the development or release of technologies inherently associated with high misuse potential, such as pretrained language models, image generators, or scraped datasets. The research focuses on enhancing model selection processes using existing data without specifying any new data collection or model creation that could raise ethical or misuse concerns. Thus, the justification that the paper poses no such risks aligns with the content and nature of the study.<br><br>No additional itemized constructive suggestions are necessary in this context since the question of responsible release and misuse does not apply to the research presented.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: creators or original owners of assets (e.g., code, data, models), used in the paper are cited.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors does not fully comply with the guidelines regarding the crediting and licensing of assets used in the research. While the answer indicates that the creators or original owners of assets are cited, it lacks specific information about the version of the assets used, URLs where those assets can be found, the names of the licenses under which these assets are released, and the explicit respect of the terms of use. This information is crucial for ensuring transparency, replicability, and legal compliance in academic research.<br><br>Constructive and actionable suggestions:<br>1. Citation Details: The authors should include specific references for all datasets, code, models, or other assets used in their research. This includes citing the original papers or sources that produced these assets.<br>2. Asset Version and Access: For each cited asset, the authors should specify the version used in their research. If possible, they should also provide direct URLs or stable DOIs (Digital Object Identifiers) to where these assets can be accessed.<br>3. License Information: It's important to explicitly state the license under which each asset is released. The authors should include the names of these licenses (e.g., MIT License, CC-BY 4.0) in their paper or supplementary materials. This step helps clarify the permissions and restrictions associated with the use of the assets.<br>4. Terms of Use Compliance: The authors should confirm they have complied with the terms of use for all assets. If any assets have specific requirements or restrictions, these should be acknowledged and detailed in the paper.<br>5. Clarification of Re-packaged or Modified Assets: If the research involves modifying or re-packaging existing assets, the authors need to clarify how these derived assets are licensed differently from the original assets. Both the original and derived licenses should be mentioned.<br>6. Documentation for Scraped Data: If the authors used data scraped from websites or other online sources, they should provide the copyright and terms of service for those sources, as well as discuss compliance with those terms.<br><br>By attending to these aspects, the authors can enhance the reproducibility of their research and demonstrate due diligence in crediting and using third-party assets responsibly.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: This is given in the github repo, whose link is given in the paper.</span><br><span style='color:#0000FF;'>Review: The response given by the authors is consistent with the typical requirements and practices within the domain of computer science research, particularly concerning the provision of assets like datasets and code. The justification aligns with the guidelines provided for documenting new assets introduced in a paper. Providing a GitHub repository link in the paper facilitates access to the datasets, code, models, and potentially other resources relevant to reproducing the research findings or extending the research. This is a common and acceptable practice in the field, promoting transparency, reproducibility, and open science.<br><br>However, to ensure the most critical and thorough review, it's important to highlight a few areas that could be further clarified or addressed by the authors, even though the score of 1 remains justifiable based on the information provided:<br><br>1. Documentation Quality: The specific contents of the GitHub repository are not detailed in the review excerpt provided. For a more thorough evaluation, it would be helpful to know if the documentation adequately covers how to use the datasets and code, explains any dependencies, outlines the expected outcomes, and provides troubleshooting tips. Actionable suggestions could include improving the README file to cover these aspects if not already done.<br><br>2. Licenses and Permissions: While the repository link is mentioned as the source of assets, the review excerpt does not confirm if the paper or repository includes information on the licensing for the use of these assets. It is essential for the documentation to clarify the license under which the code and datasets are released, ensuring potential users are aware of how they can legally use these assets. Suggest that the authors include clear licensing information in the GitHub repository if this is missing.<br><br>3. Limitations and Biases: The documentation should ideally also discuss the limitations of the assets, potential biases in the datasets, and the contexts in which the models or code are expected to perform well or poorly. Recommendations could be made to the authors to include such discussions either in the paper or within the documentation in the repository.<br><br>4. Anonymization and Consent: According to the guidelines, assets should be anonymized if applicable, and consent should be obtained for used assets. The provided excerpt does not clarify whether these aspects are addressed. Encouraging the authors to include statements regarding the anonymization of data and consent for use, especially if personal or sensitive data is involved, would be an appropriate suggestion.<br><br>Despite these points for potential improvement, the answer provided by the authors is correct based on the information given, justifying the score of 1.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer and justification provided by the authors are consistent with the content of the paper and the guidelines given for this question. The paper, titled "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," focuses on a computational method for algorithm recommendation and does not involve crowdsourcing experiments or research with human subjects. Therefore, it is not necessary for the paper to include instructions given to participants, screenshots, or details about compensation, as these elements are not applicable to the research conducted. The authors correctly identified their submission as NA (Not Applicable) for this question, in accordance with the guidelines provided.<br><br>There are no inconsistencies found between the answer, the justification, and the content of the paper related to research involving human subjects or crowdsourcing experiments. Consequently, no constructive or actionable suggestions are needed in this context, as the authors have adhered to the relevant guidelines for reporting on the type of study performed.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper and the guidelines provided. The paper describes an algorithmic approach to model selection or algorithm recommendation through active meta learning and does not involve human subjects or crowdsourcing. Therefore, the potential risks, disclosure to subjects, and the requirement for Institutional Review Board (IRB) approvals are not applicable to this study. The emphasis of the research is on algorithmic development and evaluation using datasets, making the NA (not applicable) response appropriate.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify Involvement of Human Data: While the paper is clearly algorithmic, if any of the benchmark datasets originated from human data (even if now publicly available and anonymized), a short clarification regarding the ethical considerations of using such datasets might be beneficial.<br><br>2. Future Work: Should future iterations of your work involve the collection of new datasets, especially ones that might include human participants (even indirectly), outlining potential steps towards ensuring ethical compliance would strengthen the research's integrity.<br><br>3. Transparency: Including a statement that explicitly mentions the non-involvement of human subjects directly in the research rather than simply stating "NA" could enhance clarity for readers unfamiliar with these terms of assessment.<br><br>No substantial changes are needed regarding the question of ethics involvement based on the current paper's content, as it remains strictly computational and data-driven.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The abstract and introduction clearly state the paper's main contributions and scope.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors that their abstract and introduction accurately reflect the paper's contributions and scope is correct based on the provided content. The abstract introduces the concept of algorithm recommendation with active meta-learning using a version of the CofiRank algorithm, the scope of work including real-world and artificial datasets, and ends with a claim about CofiRank's efficacy and computational efficiency. The introduction then expands on the field, setting the stage for their contribution and summarizing the state-of-the-art in algorithm recommendation, which aligns with the claims made in the abstract.<br><br>However, there are critical observations to be made regarding the depth and specificity of these claims:<br><br>1. Clarity and Specificity of Claims: While the abstract mentions that their approach rapidly identifies well-performing algorithms on new datasets at a reasonable computational cost, it might be beneficial to quantify what constitutes "rapidly" and "reasonable computational cost" based on their experiments. Providing specific examples from the results in the abstract could enhance clarity.<br><br>2. Scope of Contribution: The introduction lays a broad foundation but could highlight more distinctly how the proposed active meta-learning approach improves upon or differs from existing methods like AUTOSKLEARN and ALORS. Emphasizing unique contributions could enhance the reader's understanding of the paper's importance.<br><br>3. Limitations: According to the guidelines, it's critical for the abstract and/or introduction to also touch upon the limitations and assumptions of the proposed method. The document mentions doing so "in the future," which might leave readers wondering about the current limitations or assumptions involved.<br><br>Constructive and Actionable Suggestions:<br>- Quantify Contributions: Amend the abstract and introduction to include specific results or metrics that exemplify the paper's claims, making the contributions more tangible to the reader.<br>- Emphasize Unique Aspects: Clearly state how the approach is different from or improves upon existing methods in the introduction. This will help to underline the paper's originality and significance in the field.<br>- Address Limitations Clearly: Ensure that at least the introduction briefly outlines any assumptions or limitations of the proposed method. If these are not thoroughly discussed in the main text, including a section on this topic could be beneficial for setting realistic expectations for the reader.<br>- Enhanced Contextualization: Further contextualize the paper's contributions within the broader field of algorithm recommendation and meta-learning. This may include a brief discussion on how this work pushes the field forward or opens up new research directions.<br><br>The feedback aims at refining how the paper's contributions and scope are communicated to better align with reviewer expectations and enhance the overall impact of the paper.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The limitations are discussed in the section “Discussion and conclusion".</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. The "Conclusion and Limitations" section does indeed discuss the limitations of the work performed. The authors reflect on the scope of their claims, noting that the approach is based on a collaborative-filtering algorithm, CofiRank, and emphasizing that it does not always outperform a naive baseline method but consistently outperforms the median of Random ranking. Moreover, they mention a future direction to tackle "the harder and more interesting problem of meta-learning performance as a function of 'total' computational time rather than the number of algorithms tried," indicating an awareness of limitations related to computational efficiency and scalability.<br><br>However, for a more critical review, it would be beneficial to see more in-depth discussion about:<br>1. Robustness to violations of assumptions: The paper could improve by providing a more detailed discussion on how robust their results are against violations of any underlying assumptions made by their meta-learning algorithm, such as data quality issues in the meta-features used for the collaborative filtering.<br>2. Generalizability across different domains: There is a need for further elaboration on the generalizability of their approach across different domains and task types, considering that the empirical validation is limited to three real-world datasets (from StatLog, OpenML, and AutoML) and artificial data.<br>3. Impact of Hyperparameters: There is sparse discussion on how the choice of hyperparameters in their approach, such as the number of landmarks in Algorithm 5, influences the performance and computational cost. A detailed analysis or sensitivity test could enhance the understanding of the model's behavior across different configurations.<br>4. Scalability considerations: Even though the authors briefly mention computational considerations in their conclusion, a detailed analysis of how the proposed method scales with the number of algorithms or the size of the datasets (both in terms of features and samples) would provide valuable insights into its practical applicability.<br><br>Constructive suggestions for addressing these limitations could include:<br>- Conducting additional experiments to test the robustness of the proposed method under varying conditions, such as datasets with high levels of noise or missing values.<br>- Exploring the application of the method across a wider array of domains and including more diverse datasets in the experiments to prove its generalizability.<br>- Including a sensitivity analysis or ablation study to understand the impact of different hyperparameters on the algorithm's performance and computational cost.<br>- Providing theoretical analysis or empirical evidence on how the proposed method scales with increased dataset size and complexity, which could include exploring optimization techniques for scalability.<br><br>These suggestions aim to guide the authors in strengthening their work by addressing potential limitations and providing a clearer picture of the scope, robustness, and scalability of their proposed method.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: [TODO]</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors as "NA" is consistent with the content of the paper, indicating that the paper does not include theoretical results that require formal proofs or detailed assumptions. The focus of the paper is on an empirical approach to algorithm recommendation through active meta-learning and collaborative filtering, specifically using the ACTIVMETAL framework and the CofiRank algorithm.<br><br>Justification for agreement with the authors' answer:<br>- The paper primarily presents an empirical study rather than theoretical analysis. It proposes a novel method leveraging active meta-learning for algorithm recommendation and evaluates this approach on computational experiments using real-world and artificial datasets.<br>- The methodology section and the problem setting focus on the algorithmic and practical aspects of the proposed method, such as algorithm initialization schemes and selection strategies, without introducing new theoretical results that necessitate proofs.<br>- The results section deals with empirical validation, comparing the proposed method's performance against baseline approaches through the use of meta-learning curves rather than theoretical analysis.<br><br>However, for the sake of thoroughness, here are some constructive and actionable suggestions for the authors to enhance the clarity and completeness of their submission, if applicable in future work that involves theoretical contributions:<br>1. If the paper evolves to include theoretical analysis or results in the future, clearly state and enumerate all assumptions made in deriving these results at the beginning of the relevant section.<br>2. Include a section or appendix dedicated to formal proofs of any theoretical results introduced, ensuring that each step in the proof is well-documented and understandable.<br>3. For complex proofs that are too lengthy for the main paper, consider providing a proof sketch or overview in the main text, with detailed proofs relegated to supplementary material. Ensure the supplementary material is easily accessible and clearly references the associated results in the main paper.<br>4. Carefully cross-reference all theorems, lemmas, and corollaries within the text to improve the reader's ability to navigate and understand the theoretical framework.<br><br>These suggestions aim at hypothetical scenarios where the paper would include theoretical contributions and are provided for completeness in response to the question prompt.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors claims that all information needed for reproducing the experimental results is available in the form of pseudo-code and publicly accessible code. However, this does not fully address several vital components necessary for complete reproducibility, considering the guidelines for NeurIPS submissions.<br><br>Firstly, the paper provides algorithms' pseudo-code, which is a positive step towards reproducibility. Still, pseudo-code alone may not suffice to replicate the results without specific implementation details, versioning of libraries, or frameworks used. Moreover, the complexity of machine learning experiments often involves subtleties in data preprocessing, model initialization, or hyperparameter settings that are not fully captured by pseudo-code.<br><br>Secondly, while the paper mentions that code is accessible to the public, it doesn't explicitly specify the location (e.g., a URL to a GitHub repository) within the paper or check whether the accessibility is not restricted. For complete reproducibility, it is critical to ensure that both the data used in experiments and the exact code (including any scripts for data preprocessing, model training, evaluation, and generating results) are available and accessible.<br><br>Lastly, the paper refers to supplemental material and external sources (OpenML.org, AutoML datasets) but does not detail the exact versions of datasets used or how others could exactly reproduce these datasets from potentially dynamic sources. Similarly, relying on external resources requires guidance on dealing with potential discrepancies or updates to these resources over time.<br><br>Constructive Suggestions:<br>1. Explicitly Specify Code and Data Access: Provide clear, direct links to where the code and used datasets can be found, ensuring they are accessible without restrictions. Additionally, include instructions or scripts for generating or downloading the datasets.<br><br>2. Implementation Details: Accompany the pseudo-code with details on the implementation, including the programming language used, version numbers of major libraries or frameworks, and any crucial environment settings.<br><br>3. Parameter Settings and Initialization: Include a detailed description or a supplementary file specifying all hyperparameters, seeds for random number generators, and model initialization procedures that could affect the reproducibility of the results.<br><br>4. Versioning of Datasets: Clearly mention the version or snapshot of external datasets used. If datasets are created or modified, provide the exact procedures or scripts to enable others to recreate the exact dataset.<br><br>5. Supplemental Material: Make sure that all supplemental material mentioned for additional insights or validation (e.g., Jupyter notebooks, additional graphs) is also clearly linked and accessible.<br><br>6. Docker/Environment Setup: If a Docker image is recommended for setup, ensure the Dockerfile is available and includes instructions for building and running the environment necessary to replicate the experiments.<br><br>7. Guidelines for using External Data Sources: Provide guidance on handling any discrepancies due to updates or changes in external datasets or platforms like OpenML.org to ensure consistency in reproducing your results.<br><br>By addressing these points, the paper could substantially improve its adherence to the NeurIPS requirements for reproducibility and verifiability of results.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The github repo for reproducibility is given, in which you can find the code and dataset.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors correctly states that the paper offers open access to the data and code, which are essential for reproducing the main experimental results. The paper clearly states in the supplemental material section that datasets and code are available on Github, and it even goes further to recommend using a Docker image built for the purpose of ensuring a consistent environment for replication, thus addressing the guideline requirement for providing the exact command and environment needed. Moreover, additional materials, including a Jupyter notebook with further graphs referred to in the paper, are made accessible for a comprehensive understanding and replication of the experimental results, aligning with the guideline's suggestion to provide detailed instructions on data access, preparation, and scripts for reproducibility.<br><br>However, being critical and to ensure the reproducibility meets the highest standards, the following actionable suggestions are offered:<br>1. Detailed Reproducibility Instructions: Ensure that the Github repository contains a README file with step-by-step instructions on how to reproduce the main results, including any necessary data preprocessing steps.<br>2. Environment Details: Provide a requirements.txt file or an equivalent to detail all the dependencies and their versions. Although a Docker image is recommended, it's useful to have a list of dependencies for researchers not using Docker.<br>3. Clarify Dataset Licensing and Access: While the paper states that datasets are made available, it should clearly state any licensing conditions or restrictions on the datasets used, ensuring that all researchers know whether they can use the data freely.<br>4. Code Documentation: Enhance the documentation within the code to explain the purpose of different scripts and functions. This is particularly helpful for larger codebases or complex algorithms.<br>5. Anonymity Concerns: The paper addresses the need to preserve anonymity. Ensure that the anonymized versions of data and code do not inadvertently contain identifying information, and consider updating the repository with non-anonymized versions upon acceptance and publication.<br><br>These suggestions aim to bolster the reproducibility of the study and make it easier for other researchers to build upon this work. The authors have met the guidelines provided for data and code submission by making the necessary resources available and recommending an environment for reproduction.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results".</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors, stating that the details are given in the "Results" section, is not consistent with the content of the paper. Upon review, the "Results" section primarily focuses on the performance of the algorithms and the outcomes of the experiments, rather than detailing the experimental setup such as data splits, hyperparameters, type of optimizer, and how these were chosen. The experimental setup is crucial for understanding the context in which the results were obtained and for replicating the study.<br><br>To improve the paper, the authors should consider implementing the following suggestions:<br><br>1. Detail the Experimental Setup: Clearly outline the process of splitting the data into training and testing sets, including the ratio or method used (e.g., random split, stratified split). If cross-validation was used, describe how it was implemented.<br><br>2. Hyperparameter Configuration: Provide a detailed explanation of the hyperparameters used for each algorithm. Explain how these hyperparameters were chosen, whether through manual tuning, grid search, random search, or any other method. Include specific values or ranges considered.<br><br>3. Optimizer Information: If applicable, specify the type of optimizer used in the learning process (e.g., SGD, Adam) and any relevant configurations (e.g., learning rate, momentum).<br><br>4. Reproducibility: To ensure that the study can be replicated by other researchers, consider including a supplementary section or appendix detailing any setup aspects that couldn't be fully covered in the main text.<br><br>5. Code Availability: If not already included, provide a link to a repository containing the code used for the experiments, ensuring that it is well-documented. This greatly aids in understanding the experimental setup and reproducing the results.<br><br>Adding these details will not only improve the transparency of the research but also its impact, as other researchers will be able to build upon the work more effectively.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results" and the figures in this section.</span><br><span style='color:#0000FF;'>Review: The paper indeed reports error bars in the "Results" section, where performances of meta-learning algorithms are discussed in terms of the area under the meta-learning curve (AUMLC) normalized by the area of the best achievable curve. These error bars are presented alongside the mean performance scores for each method across the various benchmark datasets, with a notation such as ".91 (±0.03)" representing the mean and associated 1-sigma error bar. The authors ensure that the readers are informed about the statistical variability of the experiments' results. This adheres to the guidelines provided for reporting error bars, as it mentions how the variability factors are captured and clearly states the method used for calculating them (within the limitations of a summary given in the review context).<br><br>Nonetheless, while the paper correctly reports the presence and calculations of error bars, there is room for improvement to fully meet the guidelines provided:<br><br>1. Explicit Methodology Clarification: Although error bars are present, the paper could enhance its explanation of the exact methodology used to compute these error bars. Mentioning whether they are obtained through bootstrap methods, closed-form formulas, or a library function would provide clarity.<br><br>2. Assumptions Made: The paper does not specify the assumptions made when calculating the error bars, such as the normality of distribution, which is critical for interpreting the error bars correctly.<br><br>3. Variability Factors: While the presence of error bars suggests consideration of variability, the paper could further clarify whether these bars account for variability due to train/test splits, algorithm initialization, or other sources of randomness in the experimentation process.<br><br>4. Error Bar Type and Confidence Intervals: The paper should specify whether the reported error bars represent standard deviations, standard errors, or confidence intervals, and if confidence intervals are used, whether they represent a 95% confidence level or another value.<br><br>Constructive Suggestions:<br><br>- Clarify Calculation Method: Add a subsection or paragraph explicitly stating the methodology used for computing the error bars. If applicable, referencing specific statistical tests or software packages would enhance reproducibility.<br><br>- Explicitly State Assumptions: Include a discussion on the assumptions made in the statistical analysis, such as the distribution of errors. If the normality assumption is not verified, consider alternative ways to represent the data variability.<br><br>- Detail Variability Sources: Offer a clearer description of the sources of variability considered in the experiments and how these are reflected in the error bars. This information helps in understanding the robustness and reliability of the results.<br><br>- Error Bar Type: Specify the nature of the error bars (e.g., standard deviation, standard error, confidence interval) directly in the text where results are presented. For confidence intervals, clarify the confidence level used.<br><br>Implementing these suggestions would enhance the statistical clarity and robustness of the results presented, aligning the paper more closely with the provided guidelines for reporting statistical significance and error bars.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We only discussed the related computational expenses.</span><br><span style='color:#0000FF;'>Review: The provided answer is correct. Despite the engaging and innovative content concerning the active meta-learning approach for algorithm recommendation titled "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," the paper indeed lacks detailed information about the computational resources needed to reproduce the experiments. This omission is significant given the NeurIPS guidelines which emphasize the importance of describing computational resources to promote reproducibility and transparency. The answer "No" aligns with the content of the paper, as it explicitly acknowledges only discussing "related computational expenses" without specifying the type and extent of computational resources employed.<br><br>Constructive suggestions for the authors to consider in future revisions or related works:<br>1. Detail the Computational Resources Used: Include specific information about the computational environment in which the experiments were conducted. This should cover the type of processors (CPU/GPU), the amount/brand of RAM used, the type of storage, and any usage of cloud computing resources or internal clusters.<br><br>2. Execution Time Information: Provide details on the time taken for each experiment or model run. Indicate time in terms of the total execution time and, if applicable, break it down into training, validation, and testing phases.<br><br>3. Comparison to Other Methods: If relevant, compare the computational cost of your method with that of the baselines or state-of-the-art methods you have benchmarked against. This not only adds context to the computational cost discussions but also illustrates the efficiency or resource demands of your approach.<br><br>4. Disclose Complete Compute Requirements: Mention the total compute used, including for preliminary experiments, failed experiments, or any other computational effort that contributed to the final results or findings presented in the paper.<br><br>5. Guidance for Practitioners: Offer recommendations for practitioners on the minimum or optimal computational resources required for effective use of your proposed method or system. This can aid potential users in assessing the feasibility of adopting your approach in their own work.<br><br>6. Link to Code and Resources: If not already provided, ensure to link to a repository containing the source code, along with a detailed README file guiding users on how to deploy and run your experiments. Including a Dockerfile or environment setup scripts can significantly ease the process of replicating your work.<br><br>These suggestions aim to enhance the completeness, reproducibility, and accessibility of your research, aligning with the high standards of publication venues like NeurIPS.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We reviewed the code of ethics and our paper conforms with it.</span><br><span style='color:#0000FF;'>Review: Based on the content provided in the manuscript of "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," the answer provided by the authors, stating compliance with the NeurIPS Code of Ethics, appears consistent with what is discernible from the paper. The research appears to follow ethical standards in terms of its purpose, methodology, and presentation. However, the evaluation of ethical compliance based solely on the authors' affirmation and without explicit evidence or discussion in the paper itself inherently limits the depth of this assessment.<br><br>To ensure a rigorous evaluation and enhance the paper's adherence to ethical standards, consider the following itemized, constructive suggestions:<br><br>1. Data Ethics and Privacy: The utilization of real-world datasets from StatLog, OpenML, and AutoML suggests a commitment to utilizing existing, possibly public, benchmarks. However, it could enhance the ethical transparency of the paper if the authors explicitly addressed any privacy considerations related to these datasets, even if they are publicly available.<br><br>2. Bias and Fairness: Given the paper's focus on algorithm recommendation, an exploration of potential biases in the recommended algorithms or the datasets themselves would be pertinent. The authors could discuss whether their methodology accounts for or could perpetuate existing biases in machine learning applications and suggest ways to mitigate such issues.<br><br>3. Impact on Society: While the paper focuses on technical aspects, a brief discussion on the potential societal impact of making algorithm recommendations more efficient and accurate could enhance its ethical considerations. This includes both positive impacts (efficiency gains, broader applicability of machine learning) and potential negative consequences (reinforcement of existing biases, over-reliance on automated recommendations).<br><br>4. Reproducibility: The authors have provided supplemental material, datasets, and code on Github, which is commendable for reproducibility. A clear statement affirming this practice directly in the paper would reinforce their commitment to the ethical guideline of ensuring that research can be independently verified and builds a foundation for future research integrity.<br><br>5. Environmental Considerations: Given the computational nature of the work, a discussion on the environmental impact of running extensive machine learning models, as well as any steps taken to minimize this impact, could be an insightful addition, aligning with the broader ethical considerations of computational research.<br><br>6. Transparency and Conflict of Interest: Although the paper is listed as anonymous for review, ensuring that the final version, if accepted, clearly discloses any potential conflicts of interest would adhere to ethical guidelines regarding transparency.<br><br>Of course, without specific details or relevant sections of the paper devoted to ethical considerations, these suggestions are based on general best practices in ethical AI research and the information provided. Engaging with these topics within the paper would not only affirm the authors' claim of ethical compliance but also contribute to a more ethically aware and comprehensive body of research.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Our paper doesn't discuss potential positive societal impacts and negative societal impacts of the work performed.</span><br><span style='color:#0000FF;'>Review: The authors' answer and justification do not fully align with the expectations set out in the guidelines for addressing the societal impact of their work. Given the nature of the work on algorithm recommendation systems, particularly in the context of machine learning model selection, there are clear pathways to both positive and negative societal impacts. The active meta-learning approach proposed in their work could significantly streamline and optimize the process of model selection, leading to efficiencies in computational resources and potentially enabling more rapid advancements in fields that benefit society. On the other hand, the same technology could be applied in contexts where the optimization of algorithms could lead to unintended negative consequences, such as enhancing the capabilities of surveillance systems or contributing to the development of more sophisticated autonomous weapons systems.<br><br>Constructive feedback and actionable suggestions:<br>1. The authors should revisit the societal impact section of their paper and consider the broader implications of their work. This includes reflecting on how their active meta-learning approach could influence fields beyond purely technical advancements, either directly or indirectly.<br>2. The authors are encouraged to include a discussion on both the positive impacts, like advancements in disease research or more efficient energy usage through better algorithm selections, and potential negative impacts, including but not limited to privacy, security, and ethical considerations.<br>3. It would be beneficial for the authors to speculate on future applications of their work and the sectors it may most directly affect. This can help frame the societal impact discussion in a more tangible context.<br>4. In discussing negative impacts, the authors should consider providing suggestions for mitigating these risks. For example, they might propose guidelines or ethical considerations for deploying their technology, or suggest mechanisms for oversight and governance.<br>5. Finally, acknowledging the limitations or potential misuse of their technology can demonstrate a responsible and forward-thinking approach to research. This includes outlining steps for future research to address these concerns.<br><br>By addressing these points, the authors can provide a more balanced and comprehensive view of their work's societal implications, aligning with the conference's expectation for a thoughtful consideration of both positive and negative aspects.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: Based on the content and context of the paper "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," the authors' response of "NA" to the question regarding safeguards for responsible release seems appropriate and in accordance with the paper's content and focus. The primary aim of the paper is to introduce an advanced approach towards algorithm recommendation through active meta-learning, specifically in the context of model selection and algorithm recommendation using collaborative filtering. The paper's core contributions revolve around the development, evaluation, and comparison of meta-learning methods without any direct mention or indication of developing or releasing models or datasets that could inherently carry a high risk for misuse, such as pretrained language models, image generators, or scraped datasets.<br><br>Justification for the score:<br><br>- The paper's scope is confined to algorithm recommendation using an advanced meta-learning technique and does not involve creating or modifying datasets or models that could be categorized as high-risk for misuse.<br>- The methodological focus is on enhancing efficiency and performance in algorithm selection, which does not directly interact with sensitive, personally identifiable information or generate potentially harmful content.<br><br>Constructive suggestions:<br><br>While the "NA" response is justified based on the paper's content, for further clarity and thoroughness, the authors could briefly expand their "NA" response to explicitly state that their research, being primarily methodological and focused on algorithm recommendation, does not involve the creation, use, or release of models or datasets that could be misused. Additionally, given the evolving nature of ethical concerns and misuse potential in machine learning research, the authors may consider discussing any general ethical considerations they took into account during their research design and execution, even if these considerations do not directly relate to the specific use-case scenario outlined in the question. This would not only reinforce the paper's alignment with responsible AI principles but also preemptively address any related questions or concerns from readers or reviewers.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: creators or original owners of assets (e.g., code, data, models), used in the paper are cited.</span><br><span style='color:#0000FF;'>Review: The provided justification mentions that the creators or original owners of assets used in the paper are cited. However, this response is insufficient and lacks detail regarding the explicit mentioning of licenses and terms of use of these assets, which is a specific requirement according to the guidelines. Additionally, the response does not indicate whether URLs were provided for the different assets or if the version of the assets used was stated, both of which are necessary for complete compliance with the guidelines provided.<br><br>Constructive and actionable suggestions for improvement include:<br><br>1. Explicitly Mention Licenses and Terms of Use: For each asset used (datasets, code packages, or models), explicitly mention the name of the license (e.g., MIT, CC-BY 4.0) and include a brief statement on how the terms of use were respected in the context of your research.<br><br>2. Provide URLs and Version Information: If not already done, ensure that the paper includes URLs for accessing the assets used, if they are publicly available. Additionally, specify the versions of the assets to enable reproducibility and to ensure that readers can access the same resources as were used in the study.<br><br>3. Detail on Data Source Compliance: For datasets that were possibly scraped or collected from online sources, include a statement confirming compliance with the copyright and terms of service of those sources. This is especially important if there's any ambiguity about the data’s usage rights.<br><br>4. Clarifications in Supplemental Materials: If space constraints in the paper limit the amount of detail you can provide about asset licensing and terms of use, consider including detailed information in the supplemental materials. This could encompass a section dedicated to asset accreditation, licenses, and any permissions obtained for asset use.<br><br>5. Engage with Asset Creators: In cases where the licensing information is unclear or not publicly available, reach out to the creators of the assets for clarification. This engagement can also be documented in the paper to further validate the ethically responsible use of the assets.<br><br>By addressing these areas, the paper would not only meet the requirements laid out in the guidelines but also enhance its transparency and reproducibility, which are critical aspects of ethical research practices.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: This is given in the github repo, whose link is given in the paper.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors suggests that they have documented new assets and placed these documents in a GitHub repository linked within the paper. However, simply stating that documentation "is given in the GitHub repo" without further elaboration on the contents or quality of this documentation is insufficient to ascertain compliance with the guideline's comprehensive requirements.<br><br>To fully comply with the guidelines, the paper should explicitly detail:<br>1. The nature and scope of the documentation available in the GitHub repository.<br>2. If and how the documentation addresses training details, usage limitations, and any licensing issues.<br>3. Methods for ensuring the anonymity of assets, particularly if they include data derived from individuals who may not be aware their information is being used for research.<br><br>Constructive and actionable suggestions for the authors include:<br>1. Enhance the section of the paper that discusses the assets (e.g., datasets, software, models) introduced by providing a summary of what types of documentation are available for each asset.<br>2. Include specific references or links to files or sections within the GitHub repository that contain the relevant documentation.<br>3. Ensure the documentation within the GitHub repository includes a clear and detailed explanation of how the assets can be used, any restrictions on their use (including licensing information), and any prerequisites or dependencies required to use the assets.<br>4. Discuss the process of collecting data or developing the assets, with a particular focus on ethical considerations such as consent and data privacy. If applicable, describe how the anonymity of data sources has been preserved.<br>5. For better transparency and reproducibility, consider adding a section in the supplementary material that demonstrates a use case of the documented assets to show their applicability and potential impact on future research.<br><br>Addressing these points in the paper and the associated documentation can significantly enhance the utility and transparency of the assets introduced, making them more valuable to the broader research community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer is consistent with the content of the paper. Upon thorough review of the provided paper segments, there is no mention of crowdsourcing experiments or research involving human subjects. The focus is on an algorithmic approach for model selection and algorithm recommendation using active meta-learning, particularly through a novel method that incorporates aspects of collaborative filtering. There is a clear emphasis on computational processes, algorithmic evaluation, and data modeling, thus making the involvement of human subjects or crowdsourcing efforts irrelevant to the nature of this research.<br><br>However, it is crucial to maintain attentiveness to such considerations in reviewing papers, particularly those that may implicitly involve human data or subjectivity in dataset creation, selection, or analysis. In this case, the paper's structure and content do not warrant the inclusion of instructions, screenshots, compensation details, or ethical considerations related to human subjects. The authors' designation of "NA" is accurate and aligns well with the guidelines provided.<br><br>Constructive and Actionable Suggestions:<br><br>Although not applicable in this specific case, for future reference or similar studies by the authors, should any aspect of human subject research or crowdsourcing be incorporated, it is recommended to:<br><br>1. Clearly delineate the scope and nature of human participation within the research, detailing the roles and contributions of participants.<br>2. Provide comprehensive instructions, information sheets, and consent forms used in the research process, ideally within the supplemental materials if too voluminous for the main body.<br>3. Discuss the compensation framework, ensuring adherence to ethical guidelines and local laws regarding minimum wage and fair compensation.<br>4. Implement and describe ethical considerations and approvals obtained, specifically outlining measures taken to ensure participant privacy, confidentiality, and well-being throughout the research process.<br><br>These steps will not only enrich the paper's methodology section but also reinforce the ethical integrity and societal value of the research outcomes.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The authors have correctly answered "NA" to the question regarding the potential risks incurred by study participants, disclosure of risks, and Institutional Review Board (IRB) approvals, justifying their response with the reasoning that the paper does not involve crowdsourcing nor research with human subjects.<br><br>Explanation: Upon review of the paper content, it is clear that the research focuses on a technical framework for algorithm recommendation with active meta-learning. The work is computational in nature, dealing with model selection, recommendation systems, and the application of active learning and collaborative filtering techniques to the problem of algorithm selection. The methodology involves experiments with both artificial and real-world datasets but does not incorporate human participants in the study design nor does it utilize crowdsourced data in a manner that would involve interaction with study participants, thus eliminating concerns related to human subjects research.<br><br>Given the technical and computational focus of the study, which does not interact with or involve human subjects, the authors' response that IRB approval is not applicable (NA) aligns with the content and nature of the research. This means there was no necessity for ethical review concerning risks to human subjects or a need for disclosures typically associated with human-centered research.<br><br>Constructive Suggestions:<br>1. Although not applicable in this specific case, it might be beneficial for future works that could potentially touch upon data privacy or ethical concerns, even tangentially, to include a brief statement acknowledging these aspects and ensuring compliance with ethical standards.<br>2. For papers within the scope that might later involve human-related data or interaction, authors should be encouraged to proactively consider ethical implications and IRB requirements as part of their initial research design planning.<br>3. The authors may also consider including a statement on data privacy and ethical use of the datasets, especially when utilizing real-world datasets, to further reinforce their commitment to ethical research practices, even in purely computational studies.<br><br>Overall, the authors' response is consistent with the presented work's focus and methodology, making the justification for "NA" accurate and appropriate.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>truth_adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The abstract accurately reflects the paper's contributions and scope. But the first paragraph of the introduction was replaced by a fake version.</span><br><span style='color:#0000FF;'>Review: Justification: The reviewer's comment that "the first paragraph of the introduction was replaced by a fake version" does not align with the expectation set by the guidelines for accurately reflecting the paper's contributions and scope. The abstract did indeed provide an accurate overview of the paper's contributions, focusing on an active meta-learning approach to algorithm recommendation and comparing it with baseline methods through real-world and artificial datasets, which aligns with the content subsequently detailed in the paper. The introduction, despite containing a whimsical narrative about "Mystic Computation" and "Synthetic Cognition," eventually converges back to the topic at hand - active meta-learning for algorithm recommendation, providing a context that blends imaginative exposition with the paper's actual focus. While the whimsical narrative might initially seem disconnected from the paper's technical content, it serves a thematic introduction to the field of Automated Machine Learning (AutoML) in a creative manner, before transitioning to a detailed discussion on related works and the novelty of the proposed approach. Therefore, the claim in the answer that the abstract accurately reflects the paper's contributions seems consistent with the paper's content, but the justification dismissing the entirety of the introduction based on its creative start is not completely fair, given that the introduction does eventually align well with the contributions detailed later in the paper.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify the Transition: Ensure a clear transition in the introduction from the creative narrative to the paper's technical content. This could include a sentence that directly connects the whimsical elements to the actual focus of the paper, ensuring readers understand the creative exposition's purpose.<br>2. Emphasize Contributions Early: To avoid any ambiguity about the paper's contributions and scope, consider briefly summarizing the main contributions earlier in the introduction before delving into thematic narratives.<br>3. Align Justifications with Content: When justifying the alignment of the abstract and introduction with the paper's contributions, explicitly acknowledge any creative or thematic elements and explain how they serve to contextualize the technical content or engage the reader with the topic.<br>4. Ensure Consistency with Guidelines: Review the guidelines provided for assessing whether the abstract and introduction accurately reflect the paper’s contributions and scope. The justification for the assessment should explicitly mention how the paper meets these guidelines, even when employing creative narratives or thematic elements.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: There is a section named “Conclusion and Limitations", but it doesn't discuss the limitations of the work at all.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors stating "No" in reference to discussing limitations is inconsistent with the content presented in the "Conclusion and Limitations" section of the paper. The paper does outline limitations, particularly focusing on the current scope of their active learning approach and its reliance on previous algorithm runs without considering the computational run time of algorithms. They briefly mention the intention to explore algorithm performance as a function of total computational time in future studies.<br><br>To align correctly with the guidelines and the content, the answer should have been "Yes", as the paper does discuss limitations albeit perhaps not in extensive depth. It acknowledges the need for future work to tackle computational efficiency and broader application scenarios, which is in line with the guidelines' emphasis on discussing strong assumptions, the robustness of results, and the computational efficiency of proposed algorithms.<br><br>Constructive and actionable suggestions for the authors to improve the discussion of limitations include:<br><br>1. Expand on Existing Limitations: Provide more detailed discussions on the limitations mentioned, particularly regarding computational efficiency and the methodology's scalability. This could involve exploring hypothetical scenarios or providing preliminary theoretical analysis.<br><br>2. Address Generalizability: Considering the approach was tested on specific benchmark datasets, discuss how the approach might perform in potentially less favorable conditions or on datasets with characteristics significantly different from those tested.<br><br>3. Assumptions and Robustness: Elaborate on any underlying assumptions made during the development of the proposed approach and actively discuss how robust the approach is against violations of these assumptions. Offering some insights into the sensitivity analysis could be beneficial.<br><br>4. Ethical and Practical Considerations: Where applicable, briefly discuss any ethical considerations, especially if the algorithms recommended might be used in sensitive applications. Additionally, consider discussing the fairness and privacy implications if relevant.<br><br>5. Encourage Transparency and Reproducibility: Make sure that all datasets, code, and additional material necessary to reproduce the results are readily available. Highlight this in the paper to promote transparency and ease of validation by other researchers.<br><br>By addressing these points, the paper will provide a more complete and honest assessment of its contributions and limitations, aligning with the best practices for scientific reporting and potentially fostering greater trust and engagement from the research community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: [TODO]</span><br><span style='color:#0000FF;'>Review: The provided answer by the authors, "NA," accurately reflects the content and focus of the paper under review. Based on the content provided in the summary, the paper, "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," focuses on an active meta-learning approach for algorithm recommendation and does not explicitly present or rely on theoretical results, theorems, or mathematical proofs. Instead, it is centered around algorithm development, empirical validation, and comparison with baseline methods. The research is oriented towards a practical application of active meta-learning in algorithm recommendation, leveraging collaborative filtering techniques and does not delve into theoretical analyses that would require detailed proofs or theorem formulations.<br><br>Justification for the score:<br><br>1. Consistency with the paper's content: The abstract and sections provided in the summary give a clear indication that the paper's primary contribution is in proposing a novel active meta-learning method and demonstrating its effectiveness through empirical results on real-world and artificial datasets. There's no mention of theoretical results or derivations that necessitate formal proofs or deep theoretical analysis.<br><br>2. Adherence to guidelines: Since the paper does not include theoretical results, the "NA" answer aligns with the guidelines for answering this specific question. The guidelines clearly state that "NA" should be selected if the paper does not include theoretical results, which is the case here.<br><br>Constructive Feedback and Actionable Suggestions:<br><br>Though the theoretical aspect might not be the focus of this paper, incorporating discussions or sections that touch on the theoretical foundations of the proposed methods or principles (even if peripheral) could add depth and foster a deeper understanding for readers. For future work or iterations of this research, consider:<br><br>1. Discussion on theoretical underpinnings: Briefly discuss the theoretical motivations or the underlying principles that guide the proposed active meta-learning approach, even if the paper primarily focuses on empirical validation.<br><br>2. Proof sketches for key propositions: If there are any key propositions or assumptions that the approach relies on, including short proof sketches or rationale in the supplementary materials could enhance the paper's rigour.<br><br>3. Impact of assumptions on empirical results: While detailed proofs might not be necessary, discussing how certain theoretical assumptions or conditions could impact the algorithm's performance in practical scenarios could provide valuable insights and strengthen the paper's contributions.<br><br>4. References to foundational works: Where applicable, reference foundational works or theoretical studies that support the design and implementation of the proposed methods.<br><br>These suggestions are aimed at enhancing the paper's comprehensiveness and bridging the gap between empirical findings and their theoretical implications, without detracting from the paper's primary focus on active meta-learning for algorithm recommendation.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public.</span><br><span style='color:#0000FF;'>Review: The provided justification for the full disclosure of information needed to reproduce the experimental results leans heavily on the assumption that the pseudo-code and accessibility of code to the public are sufficient for reproduction. However, this assumption overlooks several critical aspects necessary for true reproducibility, as mentioned in the paper and further emphasized in the NeurIPS guidelines on reproducibility.<br><br>1. Lack of Specifics about Computational Resources: Reproducing experimental results isn't just about the algorithm's logic (contained in pseudo-code) or having access to code. The computational resources (e.g., CPU, GPU specifications) and the exact environment (versions of software/libraries used) can significantly affect the outcome of experiments, especially in fields where results are sensitive to numerical precision and computational performance.<br><br>2. Details on Data Preprocessing: While the paper mentions the datasets used, there is insufficient detail on any preprocessing steps. Data preprocessing can dramatically affect machine learning outcomes, and without precise replication of these steps, the results may not be reproducible.<br><br>3. Hyperparameters Setup: The paper does not delve into the specifics of how hyperparameters were chosen for the algorithms compared. Given that the performance of machine learning algorithms can be highly sensitive to hyperparameter settings, this oversight can hamper reproducibility.<br><br>4. Evaluation Metrics and Partitioning of Data: There should be a detailed explanation of how the data was split into training and testing sets, along with a thorough description of the evaluation metrics used. This ensures that others can exactly replicate the study's experimental setup.<br><br>Constructive and Actionable Suggestions:<br><br>a. Detailed Computational Environment: Provide detailed information about the computational environment used, including hardware specifications and versions of all relevant software and libraries.<br><br>b. Data Preprocessing Steps: Elaborate on any data preprocessing steps taken before the actual experiments, including data cleaning, normalization, feature extraction, etc.<br><br>c. Hyperparameter Choices and Tuning: Explain how hyperparameters were chosen or tuned for each algorithm examined. If any form of automated hyperparameter optimization was used, detail the process and include it in the supplementary material.<br><br>d. Code and Data Availability: Ensuring the code is publicly accessible is commendable; however, confirm that all data used in the experiments is also available. If there are any proprietary or privacy concerns with the data, consider providing a simulated dataset that can produce similar experimental outcomes.<br><br>e. Supplemental Material: Utilize supplemental material to provide exhaustive implementation details that could not be included due to space constraints in the main paper. This can include complete code listings, detailed environmental setup instructions, and additional experimental results.<br><br>By addressing these details, the authors will significantly enhance the reproducibility of their research, aligning with the spirit of the NeurIPS guidelines and fostering greater trust and validation of their findings within the community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The link to the github repo given in the paper for reproducibility is fake.</span><br><span style='color:#0000FF;'>Review: Justification for Disagreement:<br>The provided review information in the answer is not consistent with the content of the paper. The paper explicitly mentions in the supplemental material section that datasets and code are available on GitHub for full reproducibility of results. Further, it suggests using a Docker image built for this purpose, indicating an effort to ensure that the environment for reproducing the experimental results is well-defined. The statement about the GitHub link being fake seems to be incorrect based on the paper content presented, which suggests an earnest effort to enable reproducibility.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify GitHub Link Validity: Ensure that the GitHub link mentioned in the paper is active and accessible. If the review period is still anonymized, consider using a temporary anonymous repository link, and ensure it is operational.<br>2. Detail Reproduction Steps: Make sure the GitHub repository includes detailed instructions on how to reproduce the main experimental results. This should include environment setup (e.g., required libraries and versions), data preprocessing steps, and the exact commands to run the experiments.<br>3. Include Environment Specifications: Since the Docker image is recommended for running the experiments, provide clear instructions on how to use this Docker image, including how to install Docker, pull the image, and run the container with the necessary parameters.<br>4. Data Accessibility: Include detailed instructions on accessing, downloading, and preparing the data used in the experiments. If there are any permission or access issues related to the data, provide guidance on how these can be addressed.<br>5. Script Organization: Organize the scripts and code in the GitHub repository in a manner that is easy to navigate. Clearly label scripts associated with specific experiments or figures in the paper.<br>6. Address Omitted Experiments: If any experiments or results in the paper cannot be reproduced with the provided code and scripts, clearly state this and explain why. Offer alternative ways to verify these results, if possible.<br>7. Post-Review Update: After the review process and acceptance of the paper, update the paper with the final, non-anonymous GitHub repository link to ensure long-term access to the data and code.<br><br>These suggestions aim to enhance the reproducibility of the experimental results and align with best practices for sharing code and data in scientific research.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results".</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors does not accurately reflect the content of the paper regarding the specificity of training and test details necessary to understand the results. While the "Results" section discusses outcomes and performance comparisons of the proposed methods to baselines, there is a significant lack of detailed information on data splits, hyperparameters, how these were chosen, and the optimizer type used in the experiments.<br><br>Inconsistencies with the Paper Content:<br><br>1. Data Splits: The paper mentions using benchmarks from several sources and mentions cross-validation in the benchmark data section, but it lacks specifics on how the data was split (e.g., proportions for training, validation, and testing) for each dataset. Additionally, the leave-one-dataset-out procedure is mentioned without detailed methodology on execution.<br><br>2. Hyperparameters: There is no explicit section or mention of how hyperparameters for each algorithm in the active meta-learning process were chosen or optimized.<br><br>3. Type of Optimizer: The paper does not specify the type of optimizer used for training the algorithms, which is crucial for the reproducibility of results and understanding the optimization landscape of the model recommendations.<br><br>Constructive and Actionable Suggestions:<br><br>1. Detail Data Split Methodology: The authors should provide a clear description of how the datasets were split for training, testing, and if applicable, validation. If different datasets required different splitting strategies, these should be articulated clearly per dataset.<br><br>2. Hyperparameter Selection Process: The paper should include a dedicated section that describes the hyperparameter tuning process. This could include detailing any search strategies employed (e.g., grid search, random search), the range or space of hyperparameters explored, and any cross-validation schemes used in the hyperparameter optimization process.<br><br>3. Optimizer Information: The authors should mention the specific optimizer(s) used in the experiments, including any relevant configurations such as learning rate, momentum, etc. If different optimizers were used for different parts of the process or different datasets, this information should be included.<br><br>4. Supplemental Material/Appendix: If including this information within the main body of the paper is problematic due to space constraints, the authors could consider adding it as an appendix or supplemental material. However, a summary of these details should still be present in the main text to give readers a clear understanding of the experimental setup.<br><br>5. Code and Reproducibility: Strengthen the paper's contribution to the field by ensuring that the provided code repository is well-documented, with clear instructions on replicating the experiments, including data preprocessing, model training, and evaluation procedures.<br><br>Addressing these points would significantly improve the paper's clarity regarding experimental setup, making it easier for readers to understand the results and accurately reproduce the study.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The figure is fake, with NO ERROR BARS.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors does not align with the paper content. The paper does contain error bars and relevant information regarding the statistical significance of the experiments. Specifically, in Table 2, performance metrics are presented with a 1-sigma error range (e.g., Active_Meta_Learning w. CofiRank .91 (±0.03)). This directly contradicts the claim made in the justification that "the figure is fake, with NO ERROR BARS." The error bars capture variability factors, likely from leave-one-dataset-out cross-validation as implied by the paper's methodology, and provide a measure of statistical confidence in the reported results.<br><br>Constructive suggestions:<br>1. The response might have been a misunderstanding or an oversight. It is essential to accurately review and interpret the results and methodologies described in the paper.<br>2. Ensure that all figures and tables mentioned in the justification correctly reflect the information presented in the paper. Specifically, double-check that error bars or confidence intervals are accurately reported and described in both the text and any visual representation.<br>3. Clarify in the paper how the error bars were calculated. Although the paper mentions 1-sigma error bars, a more detailed explanation of their computation would strengthen the manuscript. This includes specifying whether these error bars account for variations due to train/test splits, initialization randomness, or other factors.<br>4. The paper could benefit from a discussion on the assumptions made for statistical analysis. For example, if assuming normality for error calculations, this should be justified or acknowledged as a limitation.<br>5. Consider expanding on the statistical analysis by possibly including additional measures of statistical significance, such as p-values or confidence intervals, to complement the error bars.<br>6. Reference figures, tables, and specific results in the main text when discussing the statistical significance and error bars, ensuring that readers can easily locate and understand the basis for the paper's claims.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We only discussed the related computational expenses.</span><br><span style='color:#0000FF;'>Review: The authors acknowledge that the paper does not provide specific details on the computer resources needed for reproducing the experiments, aligning with the question asked. Given this admission and upon reviewing the paper content, it is clear that specific details such as the type of compute workers (CPU or GPU), the memory required, the execution time, or the computing platform (internal cluster, cloud provider, etc.) are indeed not disclosed. This information is necessary for replicating the study's results accurately and efficiently, as computational requirements can significantly affect the outcomes and reproducibility of machine learning experiments.<br><br>Constructive and actionable suggestions:<br>1. Detailed Computational Resources: Future versions of the paper should include a section or a table that details the computational resources used for each experiment. This should cover the type of compute workers (CPU/GPU), the specific models or generations (e.g., NVIDIA Tesla V100 GPU), memory specifications, storage requirements, and the runtime for each experimental setup.<br><br>2. Compute Platform Details: If experiments were conducted on an internal cluster or via a cloud provider, please specify this and include any relevant configurations that could impact performance or cost, such as the use of preemptible instances or special hardware accelerators.<br><br>3. Computational Cost Analysis: Given the active learning nature of the proposed approach and its potential for reducing the computational burden by minimizing the number of algorithm evaluations, a discussion on the computational cost benefits in comparison to traditional methods could significantly enhance the paper's impact. Quantitative analyses highlighting the computational savings or efficiency improvements would be particularly valuable.<br><br>4. Failed Experiments and Preliminary Runs: If applicable, briefly discuss any preliminary or failed experiments that required substantial compute resources but did not make it into the final paper. This transparency can provide valuable insights into the development process of your method and help readers gauge the overall computational demands of your research project.<br><br>5. Reproducibility Package: Consider providing a reproducibility package or supplementary material that includes scripts for setting up computational environments (e.g., Docker containers, as already mentioned for CofiRank), along with detailed requirements and instructions. This package should ideally allow researchers to replicate your experiments with minimal setup overhead.<br><br>By addressing these points, the paper would significantly improve in terms of reproducibility and transparency, allowing others in the community to build upon this work more effectively.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We reviewed the code of ethics and our paper conforms with it.</span><br><span style='color:#0000FF;'>Review: The authors' response that their work conforms with the NeurIPS Code of Ethics, without providing specific details, is consistent with standard practice when there are no ethical concerns directly implicated by the research. Based on the content of the paper titled "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," there are no explicit ethical issues or concerns evident in the methodologies, experiments, or discussions presented. The research focuses on the development of an algorithmic approach for recommending machine learning models based on active meta-learning, which does not directly involve human subjects, sensitive data, or ethically questionable practices.<br><br>However, the response could have been more informative if the authors had briefly discussed specific aspects of the Code of Ethics relevant to their work, even if to confirm that those aspects were considered and found to be compliant. For instance, they could have mentioned considerations regarding transparency, fairness, and the potential impact of their work.<br><br>Constructive and Actionable Suggestions:<br><br>1. Enhance Ethical Justification: In future submissions or revisions, it might benefit the authors to provide a brief summary of the main ethical considerations relevant to their research area (e.g., algorithmic fairness, transparency, and potential societal impact) and explicitly state how their research addresses these considerations.<br><br>2. Data Handling and Privacy: Even though the research primarily involves algorithmic development and comparisons, ensuring the ethical use of datasets is crucial. The authors should confirm that the datasets used (especially those sourced from OpenML and AutoML competitions) do not contain sensitive or proprietary information and that they comply with data privacy regulations.<br><br>3. Algorithmic Bias and Fairness: Given the increasing scrutiny of algorithmic decision-making systems, the authors should consider discussing any potential biases their recommendation system might introduce and how such biases could be mitigated. This is especially relevant if their system were to be applied in heterogeneous or high-stakes environments.<br><br>4. Impact Assessment: The authors could enhance the paper by including a brief discussion on the potential positive and negative impacts of their work on the broader scientific community and society. Highlighting applications where their algorithm recommendation system could be particularly beneficial (or harmful if misapplied) would help in evaluating the ethical considerations of their research’s impact.<br><br>Overall, while the response to the question on ethical conformity is acceptable based on the provided information, these suggestions aim at strengthening the narrative around ethical considerations in their research, in alignment with the NeurIPS Code of Ethics.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Our paper doesn't discuss potential positive societal impacts and negative societal impacts of the work performed.</span><br><span style='color:#0000FF;'>Review: Justification:<br>The reviewers' justification and the guidelines provided indicate a mismatch in understanding the potential societal impacts of the work presented in the paper. The paper focuses on an active meta-learning approach for algorithm recommendation, which, while perhaps foundational in nature, can have broader societal implications worth considering. Although the paper is primary research and may not directly apply to immediate applications or deployments, the technology's indirect pathways to societal impact should be acknowledged.<br><br>Constructive and actionable suggestions:<br><br>1. Societal Impact Discussion: The authors should include a brief section discussing potential positive societal impacts, such as how the proposed active meta-learning approach could accelerate the process of finding optimal machine learning models, thereby advancing fields that rely on machine learning technologies. This could include speeding up research in healthcare, environmental science, and other areas critical to societal welfare.<br><br>2. Consideration of Negative Impacts: Even if the research is foundational, a discussion on the possible negative societal impacts should be included. This could involve theoretical explorations of how improvements in algorithm recommendation systems might be misused or lead to unintended consequences, such as exacerbating biases in model selection, increasing automation and potential job losses in certain sectors, or even misuse in surveillance applications.<br><br>3. Mitigation Strategies: If negative societal impacts are identified, the authors should discuss possible mitigation strategies. For instance, they could propose guidelines for the ethical use of algorithm recommendation tools, mechanisms for bias detection and correction in model selection, or recommend transparency in the use of such technologies in critical applications.<br><br>4. Broader Ethical Considerations: The paper should briefly touch upon any ethical considerations relevant to the development and use of active meta-learning technologies. This includes but is not limited to, privacy concerns, fairness, transparency, and accountability in automated decision-making.<br><br>5. Community Engagement: Lastly, the authors could suggest how engaging with broader communities, including policymakers, social scientists, and ethicists, might help identify and mitigate potential negative impacts early in the technology development process.<br><br>Including these considerations will not only comply with the conference guidelines but also enrich the paper's contribution by highlighting the broader implications of the research within society.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors as "NA" with the justification that their paper poses no such risks is consistent with the content and nature of the paper as described. The paper focuses on an algorithm recommendation system using active meta-learning methods, specifically in the context of machine learning model selection. From the information provided, the work does not involve the development or release of potentially risky pre-trained models, image generators, or datasets that could be classified as high-risk for misuse or raise ethical concerns related to their release or application. It primarily deals with algorithm performance prediction and recommendation which, in itself, does not directly pose a risk for misuse in the way that releasing a pre-trained model or a scraped dataset might.<br><br>However, to provide constructive feedback and assure comprehensive coverage of potential future adaptations that might involve sensitive data or models:<br><br>1. Clarify potential future risks: Although the current scope of the work does not involve direct misuse risks, it would be beneficial for the authors to briefly discuss any potential risks that could arise if the method was adapted or extended to sensitive domains in future work. This would show foresight and responsibility in research conduct.<br><br>2. Discuss data handling and privacy: While the paper does not present immediate concerns, discussing how data was handled, even if it's publicly available datasets from Statlog, OpenML, and AutoML, reassures the reader of the authors' commitment to data privacy and ethical considerations, especially considering the growing sensitivity around data privacy.<br><br>3. Future guidelines: Should future work based on this research involve more sensitive models or datasets, the authors could outline hypothetical safeguards or guidelines they believe should be in place to mitigate potential risks. This could include controlled access to models, ethical guidelines for use, or mechanisms to prevent misuse.<br><br>4. Engage with the broader ethical implications: Lastly, engaging briefly with the broader ethical implications of automated model selection and its potential impact on various domains could provide valuable insights. While not directly related to misuse, considering the ethical dimension of automated decision-making processes adds depth to the discussion and acknowledges the complexity of technology's role in society.<br><br>These suggestions aim to enhance the paper's exploration of the ethical considerations surrounding the research, ensuring that readers and future researchers are aware of the responsibilities involved in developing and applying machine learning technologies.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: the citation to the used algorithm ‘CofiRank' has been removed.</span><br><span style='color:#0000FF;'>Review: Justification: The authors' answer contradicts the paper content and the provided justification. The paper explicitly mentions CofiRank, indicating its usage within their methodology. Moreover, references section [10], associated with the CofiRank algorithm, suggests that the original work related to CofiRank has been acknowledged. However, there is no clear mention of the licensing terms or explicit credit to the creators or original owners beyond the reference. For an exhaustive review concerning the respect of licensing and terms of use, more details are needed than provided in the justification.<br><br>Constructive and Actionable Suggestions:<br>1. Explicit Credit and Licensing Information: Although CofiRank is mentioned and referenced, the paper should explicitly state the licensing terms under which CofiRank is used if available. If CofiRank is under a specific license that requires attribution or imposes usage restrictions, these details should be clearly mentioned to comply with legal and ethical standards.<br><br>2. Version and URL: To enhance reproducibility and credibility, it's recommended to specify the version of CofiRank utilized in the study. If CofiRank is available online, providing a URL to where the algorithm or its implementation can be accessed would be beneficial for readers interested in following up on this work.<br><br>3. Comprehensive Acknowledgment: For any other code, dataset, or model utilized in the paper, ensure that the original creators are properly credited. If these assets are accompanied by a license, this should be mentioned explicitly. This practice respects the intellectual property rights of others and promotes transparency and reproducibility.<br><br>4. Guidance for Derived Works: If the authors have modified or expanded upon existing assets (like adapting CofiRank for their active meta-learning approach), they should clarify the nature of these modifications and discuss the licensing implications of the derived work.<br><br>5. Contacting Original Creators: In cases where licensing information is not clear or publicly available, the authors should attempt to contact the original creators to ensure that their terms of use are being respected. Documentation of such efforts could further strengthen the paper's adherence to ethical guidelines.<br><br>By addressing these aspects, the authors can better align their paper with the best practices regarding the use of existing assets, which is crucial for both ethical and legal considerations in academic publishing.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: This is given in the github repo, whose link is given in the paper.</span><br><span style='color:#0000FF;'>Review: Justification for Score:<br>The provided answer and justification are somewhat sparse and lack detail regarding the documentation and quality of the assets introduced in the paper. While the mention of a GitHub repository suggests that assets such as code, datasets, or models are indeed shared, it does not specifically address the evaluation criteria outlined in the guidelines. Specifically, confirmation is needed on whether the documentation in the GitHub repository includes structured templates detailing training, license, limitations, etc., and whether consent issues are discussed. Furthermore, the statement does not provide information on the anonymization of the assets, which is an essential aspect as per the guidelines.<br><br>Constructive and Actionable Suggestions:<br>1. Detail Documentation: The authors should provide a more detailed account of the documentation accompanying the new assets introduced in the paper. Specifically, it would be beneficial if the review explicitly stated whether the GitHub repository includes structured templates with essential details such as the methodology behind the assets, training procedures, the license under which the assets are released, any limitations or caveats users should be aware of, and how these assets should be cited.<br><br>2. Address Consent and Anonymization: The authors should clarify whether and how consent was obtained for any data used in their work, especially if personal or sensitive information is involved. Additionally, they should ensure that the assets provided for review are properly anonymized in line with the guidelines, and describe the steps taken to achieve this.<br><br>3. Expand on the Utility and Limitations: While sharing assets is commendable, a more thorough discussion in the paper regarding the potential utility of these assets to other researchers, as well as any limitations or challenges in using them, would be valuable. This could help future researchers understand how they might use or build upon the provided assets in their own work.<br><br>4. Provide Clear Links and Instructions: Ensure that the GitHub repository link is easy to find in the paper and that the repository itself is well-organized with clear instructions on how to use the assets. A README file detailing setup, requirements, and a quickstart guide would enhance the utility of the repository.<br><br>5. License Information: It's crucial to include clear license information in the GitHub repository to inform users about how they can legally use the assets. If not already done, adding a LICENSE file to the repository that specifies the terms under which the assets are made available would address this point.<br><br>By addressing these suggestions, the authors would significantly improve the transparency and reproducibility of their work, aligning with the best practices for sharing new assets in the research community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper under review. The paper titled "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning" focuses on an active meta-learning approach for algorithm recommendation, adopting techniques from collaborative filtering recommender systems and does not involve human subjects or crowdsourcing experiments. The methodology primarily involves computational models, benchmark datasets, and algorithm performance evaluation without any mention of human participation in the experiment setup.<br><br>Justification:<br>- The content of the paper revolves around automated model selection, meta-learning algorithms, and their comparison on benchmark datasets. There is no section or mention of crowdsourcing experiments or involvement of human subjects where instructions, screenshots, or compensation details would be required.<br>- Since the paper's contributions and experimental evaluation strictly pertain to algorithmic development and performance analysis on datasets, the requirements for detailing human subject research protocols do not apply.<br><br>Constructive and actionable suggestions are not applicable in this context as the answer and justification provided align with the paper content and the nature of the research conducted.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The provided answer "NA" is consistent with the content of the paper, as there is no mention of crowdsourcing, human subject research, or any activities that would involve human participants directly. The main focus of the study is on developing and testing a model for algorithm recommendation through active meta-learning, which involves computational experiments rather than interaction with human subjects. Therefore, the considerations for potential risks to study participants, disclosures to subjects, and Institutional Review Board (IRB) approvals do not apply to the design and scope of this research.<br><br>Given the paper's focus on algorithmic development and evaluation using datasets from online platforms and earlier research without involving human subjects directly, the absence of IRB-related discussions is justified. The research falls outside the purview of IRB oversight, which is primarily concerned with studies that directly involve human subjects in a manner that could impact their rights, well-being, or privacy.<br><br>The response to the checklist question aligns with the provided guidelines, recognizing when IRB approval is not applicable due to the nature of the research. This approach adheres to the NeurIPS Code of Ethics, which expects authors to accurately represent the ethical considerations relevant to their research.<br><br>Constructive and actionable suggestions for the authors (despite the answer being appropriate):<br>1. To avoid any potential confusion or concerns during the review process or after publication, consider adding a brief statement in the methodology or ethics considerations section of the paper explicitly stating that the study did not involve human subjects or animal research, thus IRB approval was not applicable.<br>2. Ensure that any datasets used, especially those from online platforms or previous studies, were collected and shared in compliance with applicable privacy and ethical guidelines, and mention this compliance in the paper.<br><br>These suggestions are aimed at reinforcing the ethical considerations of the study and providing clarity to readers and reviewers, but they do not indicate any inconsistencies in the answer given by the authors.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h3> Post Submission Survey</h3><p><a href='https://docs.google.com/forms/d/e/1FAIpQLSfRIDkcXFbsOrR09j4qA1MlG4Rfir2lPD_u9YC4eqKBJ8tHkw/viewform?usp=pp_url&entry.463237339=QUNUSVZNRVRBTDpBbGdvcml0aG0gUmVjb21tZW5kYXRpb253aXRoIEFjdGl2ZSBNZXRhIExlYXJuaW5n'>Click here to submit a post submission survey</a></p><br><br><br><br><br><h2>ACTIVMETAL:Algorithm Recommendationwith Active Meta Learning</h2><br>--------------------------------------<br>[+] Correctness Score (genuine): 0.67<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Score (adversarial): 0.67<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Score (truth_adversarial): 0.4<br>--------------------------------------<br>--------------------------------------<br>[+] Resiliance Score: 0.47<br>--------------------------------------<br>--------------------------------------<br>[+] Combined Score: 0.1<br>--------------------------------------<br><h1><strong>genuine</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The abstract and introduction clearly state the paper's main contributions and scope.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors, stating that the abstract and introduction accurately reflect the paper's contributions and scope, is consistent with the content of the paper. The abstract clearly outlines the novel approach of algorithm recommendation through active meta-learning, highlighting its application to model selection. It also states the paper’s methodology of using a collaborative filtering recommender system and compares the proposed CofiRank algorithm with baseline methods across real-world and artificial datasets. Similarly, the introduction sets the stage by discussing the significance and current progress in the field of AutoML, positioning the study's contribution within this context.<br><br>Justification for the agreement:<br>- The paper indeed presents an active meta-learning approach called ACTIVMETAL for algorithm recommendation, which is precisely what is claimed in the abstract and introduction.<br>- The scope of the study, including the comparison of the proposed method with baseline approaches across specific datasets, is outlined early in the document, accurately reflecting the subsequent sections of the paper.<br>- The introduction provides a thorough background, motivating the need for such research, which aligns with the claims made about the paper’s contributions to the field.<br><br>Constructive suggestions for further clarity:<br>1. Clarify Limitations and Assumptions: While the paper does mention planning future work and briefly discusses computational considerations, explicitly stating any assumptions made (such as datasets having no missing values except for the new dataset being tested) and limitations in the introduction or abstract could enhance clarity and set realistic expectations for the reader.<br>2. Highlight Novelty More Explicitly: The introduction does a good job of background setting, but emphasizing the novel contributions of this research compared to existing work (such as AUTOSKLEARN and ALORS) could be made more prominent. Making a distinct section for contributions could help in highlighting how this work advances the state-of-the-art.<br>3. Generalization Claims: The paper claims the methodology could be applicable to a broader range of problems beyond classical machine learning settings. Providing a brief discussion or speculation on how and in what specific areas this approach could be extended would enrich the introduction section and better frame the paper’s aspirational goals.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The limitations are discussed in the section “Discussion and conclusion".</span><br><span style='color:#0000FF;'>Review: The provided answer aligns correctly with the paper's content regarding discussing the limitations of the work performed by the authors. As stated in the "Discussion and conclusion" section, the authors clearly identify and discuss several limitations, including:<br><br>1. The simplicity of their meta-learning algorithm in comparison to complex real-world requirements such as dealing with different types of tasks and algorithms beyond their current scope.<br>2. The computational efficiency and scalability concerns when dealing with larger meta-learning datasets and more extensive algorithm evaluations.<br>3. Assumptions about the computational cost of different algorithms being similar, which might not hold true for all cases and could affect the applicability of their approach in diverse scenarios.<br>4. The active learning method's leaning toward exploitation rather than exploration, leaving room for improvement in balancing these aspects for better performance across varying datasets.<br>5. The need for future research to consider the computational expense more thoroughly, potentially including the duration of each algorithm in the assessment for a better trade-off between accuracy and computation time.<br><br><ul><li>Constructive Suggestions*:</li><br></ul>- To further strengthen the paper, the authors could explore methodologies for incorporating computational time directly into their algorithm selection process, potentially through a more nuanced analysis or machine learning techniques designed to predict algorithm performance based on dataset characteristics and algorithm complexity.<br>- Consider expanding the scope of tested datasets and algorithms, including more diverse and challenging cases, to evaluate the robustness and applicability of their approach across a broader range of scenarios.<br>- Investigate and develop strategies for better managing the exploration-exploitation trade-off in active learning settings, potentially including mechanisms for adaptive exploration based on the learning context.<br>- Detail potential strategies or future research directions aimed at addressing the identified limitations, providing a clear road map for how the work could evolve to overcome these challenges.<br>- Engage in a deeper discussion on the ethical considerations, such as privacy and fairness, that could arise from the application of their approach in different domains, and propose preliminary guidelines or considerations to mitigate potential negative impacts.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not include theoretical results.</span><br><span style='color:#0000FF;'>Review: The response of "NA" from the authors is consistent with the content of the paper. After reviewing the paper, it's clear that the focus is on presenting an empirical study of an algorithm recommendation system utilizing active meta-learning and collaborative filtering. The study does not introduce new theoretical concepts that would necessitate formal proofs or the elaboration of theoretical assumptions in the traditional mathematical sense. Rather, it contributes to the field through the development and empirical validation of a novel approach to algorithm recommendation, comparing its performance with existing methods using benchmark datasets.<br><br>The setup of the problem and the design of the algorithm (ACTIVMETAL) are explained conceptually and empirically rather than through theoretical formulations. Thus, there are no theorems, hypotheses, formulas, or mathematical models presented which would require proofs or a detailed discussion of assumptions in a mathematical/theoretical framework.<br><br>Constructive suggestions for the authors, should theoretical aspects be introduced in future iterations of the work, include:<br><br>1. Clearly State Assumptions: If theoretical results are introduced, ensure that all assumptions are clearly stated or referenced. This will aid readers in understanding the conditions under which your results hold and how they might extend to other contexts.<br><br>2. Provide Detailed Proofs: For any theoretical claims, provide complete and clear proofs, either within the main body of the paper or as part of the supplementary material. If opting for the latter, consider including a proof sketch or intuitive explanation in the main text.<br><br>3. Theorem Numbering and Cross-Referencing: Adopt a systematic approach to numbering theorems, lemmas, propositions, etc., and ensure they are properly cross-referenced throughout the paper. This improves readability and helps readers navigate the document.<br><br>4. Reference Relied Upon Theorems and Lemmas: Make sure to properly cite any external theorems, lemmas, or significant proofs that your work directly relies upon. This not only provides due credit but also helps situate your contribution within the broader academic conversation.<br><br>These suggestions are moot for the current version of the paper, as it does not include theoretical content necessitating such considerations. Nevertheless, they are offered as guidance for potential future expansions of the work or other projects that may incorporate theoretical elements.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public.</span><br><span style='color:#0000FF;'>Review: Justification: The answer provided by the authors overstates the availability and sufficiency of information for reproducing the experimental results. While the paper does indicate that algorithms and their code are accessible to the public, and provides pseudo-code for key algorithms, several pieces of critical information seem to be either missing or not fully detailed, according to the excerpts provided. Specifically, the paper does not discuss the process of collecting data, nor does it elaborate on the specifics of how the algorithms were implemented beyond pseudo-code, which might not cover all implementation nuances. The reported supplemental material appears crucial for full reproducibility but is merely referenced without clear in-text description or guidance on how it substantiates the main results.<br><br>Suggestions for the Authors:<br><br>1. Explicit Detailing of Computational Environment: Provide detailed information about the computational environment (e.g., hardware specifications, software versions) in which the experiments were conducted. This ensures that other researchers can align their setup to match yours as closely as possible.<br><br>2. Data Preparation Steps: Elaborate on the steps involved in preparing the datasets for the experiments. Given the mention of preprocessing steps (e.g., global normalization, taking square roots for StatLog), detailed descriptions of these steps are essential for reproducibility.<br><br>3. Detailed Implementation Notes: Complement pseudo-code with implementation notes that cover aspects not easily conveyed through pseudo-code. This includes choices of library functions, parameter settings, and any non-trivial coding decisions that could affect the experimental outcome.<br><br>4. Access to Supplemental Material: Ensure that references to supplemental material are complemented with explicit guidance on how such material supports reproducibility. Offer direct links to any repositories or supplementary files and provide an overview or index of what researchers can find in these resources.<br><br>5. Describe Evaluation Metrics and Procedures: Clearly describe the evaluation metrics and statistical methods used to analyze the results. For experiments involving average scores or error bars, detail how these averages and error ranges are computed.<br><br>6. Dependence on External Libraries or Tools: If your experiments rely on external libraries or software (e.g., CofiRank, Docker), provide detailed versions and configurations used. This includes instructions for setting up these tools in a manner that mirrors your experimental setup.<br><br>7. Clarify Access to Data and Code: While the paper states that datasets and code are available on GitHub, a direct link to the specific GitHub repository should be provided within the main manuscript. If possible, include a DOI for the repository to ensure permanent availability.<br><br>By addressing these points, the paper can significantly improve its reproducibility and allow the community to both verify and build upon the presented work.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The github repo for reproducibility is given, in which you can find the code and dataset.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. The paper mentions that for full reproducibility of their results, datasets and code are available on GitHub, and it is recommended to use the Docker image they built for this purpose. Additionally, the authors note that their repository includes a Jupyter notebook with additional graphs referred to in the text, contributing to easier reproduction of the results.<br><br>Justification Review and Suggestions:<br>- The justification aligns with the guidelines provided for ensuring open access to data and code for reproducing the main experimental results. Mentioning the availability of a GitHub repository and the recommendation to use a Docker image suggests that the authors have made efforts to comply with the reproducibility criteria.<br>- While the answer and justification seem appropriate, it is critical to ensure that the GitHub repository contains not only the code and datasets but also detailed instructions on how to use them. This includes the exact commands needed, information on the computing environment, and instructions for data preparation and script execution.<br>- To enhance this submission further, the authors could:<br>1. Ensure that the GitHub repository is well-documented, with a README file that clearly outlines step-by-step instructions for reproducing the experiments.<br>2. Include information about any dependencies or specific software versions required to run the code.<br>3. If not all experimental results are reproducible due to constraints, clearly state which results can be reproduced and the reasons for any limitations.<br>4. Since the paper is under review and anonymity is required, verify that the GitHub repository and Docker image do not contain identifiable metadata.<br><br>Overall, the commitment to open science and reproducibility is commendable. Ensuring the completeness and clarity of the instructions and resources provided will further strengthen the submission.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results".</span><br><span style='color:#0000FF;'>Review: The provided justification and the answer seem inconsistent with the detailed content of the paper, particularly concerning the level of detail provided about training and test setups necessary for fully understanding and replicating the results. Upon review of the content summaries provided, the paper does discuss the benchmark datasets used for validation and introduces the algorithms (including initialization schemes and selection of algorithms for evaluation on new datasets). However, specifics about data splits, hyperparameters (how they were chosen or optimized), or the type of optimizer used in experiments (if applicable) are not explicitly detailed in the provided summary. These elements are crucial for replicating the results and understanding the methodology fully.<br><br>Constructive and actionable suggestions include:<br><br>1. Detailed Methodology Description: Amend the manuscript to include explicit details about data splits (how the datasets were divided into training and testing), including the ratio or methodology used.<br><br>2. Hyperparameters Justification: Provide a section or amend existing sections to articulate how hyperparameters were chosen or optimized. This could include discussion on any hyperparameter tuning process, fixed values based on literature, or rationale for their selection.<br><br>3. Optimizer Details: If any optimizer was used as part of the model training or algorithm recommendation process, detail the type of optimizer, its settings, and the rationale for its choice.<br><br>4. Enhance Reproducibility: To strengthen the paper's contribution and aid in reproducibility, consider including an appendix or supplemental material section that encapsulates all the technical details related to training and testing. This could also be addressed by ensuring the provided code (if available) is well-documented with comments explaining critical decisions related to experiment setups.<br><br>5. Benchmark Comparison: Where applicable, discuss how the experimental setup compares with the setups used in benchmarking studies or the setups of the baseline models for a clearer understanding of the results' significance and fairness in comparison.<br><br>Including these details will not only strengthen the paper's transparency and reproducibility but will also enhance its utility for readers and practitioners aiming to apply or extend the proposed methods.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results" and the figures in this section.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the contents of the paper, as the "Results" section and accompanying figures include details about statistical significance, specifically the presentation of 1-sigma error bars in Table 2 and the discussion about the significance of the performance improvement of their proposed method over baselines.<br><br>However, to improve clarity and completeness, the authors could consider augmenting their statistical analysis with additional information:<br><br>1. Clarify the variability factors captured by the error bars, such as whether they account for variability across different datasets, algorithm initializations, or other experimental conditions.<br>2. Expand on the method used to calculate the error bars. For example, if bootstrapping or a specific library function was used, providing this detail would enhance reproducibility.<br>3. State explicitly whether the error bars represent standard deviations, standard errors of the mean, or confidence intervals. This clarification would assist readers in correctly interpreting the results.<br>4. In the case of asymmetrical distributions, consider providing confidence intervals or clearly denoting the asymmetric nature of the error bars to avoid misleading interpretations.<br>5. Enhance the text to explicitly reference figures or tables where error bars are shown, explaining in the main text how these were calculated or the statistical significance tests used.<br><br>These suggestions aim to further strengthen the paper by ensuring its statistical analyses are not only correctly defined but also transparently reported, aiding in the reproducibility and rigorous evaluation of the experimental results.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We only discussed the related computational expenses.</span><br><span style='color:#0000FF;'>Review: The given answer correctly identifies a shortfall in the provided paper regarding the detailed reporting of computational resources used for the experiments. The authors mention some discussion of computational expenses but do not provide specific information such as the type of hardware (CPU/GPU), the amount of memory, storage requirements, or execution times necessary for reproducing the experiments. This lack of detail indeed falls short of the guidelines provided, which ask for comprehensive reporting on computational resources.<br><br>However, while I agree with the answer's correctness in pointing out the paper's deficiency in this aspect, it's imperative to offer constructive, actionable suggestions for improvement. The authors could enhance their paper by incorporating the following:<br><br>1. Detailed Specifications of Computational Environment: The authors should specify the types of compute workers used (e.g., CPU, GPU), whether these were accessed through an internal cluster or cloud providers, and the models or specifications of these resources.<br><br>2. Memory and Storage Needs: Include information about the memory requirements and storage needs for executing the experiments. This is especially crucial for computational experiments where resource limitations could affect reproducibility.<br><br>3. Execution Time: For each experiment, the authors should provide the execution time, helping readers gauge the computational cost and feasibility of reproducing the results.<br><br>4. Estimate of Total Compute: Offering an estimate of the total compute required for reproducing the experiments, including any preliminary or failed experiments not detailed in the paper, would provide a clearer picture of the project's computational demands.<br><br>5. Explicit Disclosure: If applicable, the paper should explicitly disclose any substantial computational resources required beyond what was directly reported for the experiments, such as for preliminary testing or development of the methodology.<br><br>Adding this information would not only meet the NeurIPS guidelines but also improve the paper's transparency and reproducibility, essential criteria for the research community.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We reviewed the code of ethics and our paper conforms with it.</span><br><span style='color:#0000FF;'>Review: While the authors affirm that their work conforms to the NeurIPS Code of Ethics, the justification provided lacks depth and specificity. Simply stating that the paper conforms with the code of ethics without elaboration does not provide assurance that all aspects of the code were thoroughly considered and adhered to. This is especially pertinent given the complexity and breadth of ethical considerations in AI research, including data privacy, fairness, transparency, and potential societal impacts.<br><br>Constructive and Actionable Suggestions:<br><br>1. Elaborate on Ethical Considerations: It would be beneficial for the authors to include a section or a paragraph detailing their considerations of ethical aspects relevant to their research. For instance, discussing how they ensure data privacy in the datasets used, addressing any potential biases in algorithm recommendations, and considering the implications of automated algorithm selection on fairness and transparency.<br><br>2. Data Privacy and Usage Permissions: Given that the paper uses real-world datasets from various sources, the authors should ensure and state that all datasets were used in accordance with respective data privacy laws and that necessary permissions were obtained.<br><br>3. Algorithm Bias and Fairness: The authors should discuss whether they evaluate the potential biases their method might introduce or exacerbate in algorithm recommendation. If biases are found, it is essential to mention steps taken to mitigate them.<br><br>4. Transparency and Reproducibility: While the authors provide supplemental material and code for reproducibility, they should also ensure that the method’s decision-making process is transparent. This includes discussing the interpretability of the active learning method and the collaborative-filtering algorithm (CofiRank) used.<br><br>5. Impact on Research and Practice: Briefly discuss the potential societal and ethical impacts of their work on the broader field of AutoML. Consider both positive impacts (e.g., making machine learning more accessible) and potential negative consequences (e.g., misuse of algorithm recommendation systems, over-reliance on automated systems without understanding underlying principles).<br><br>6. Compliance and Anonymity Preservation: Reiterate compliance with the NeurIPS anonymity policy within the ethical considerations section, ensuring that no direct or indirect identification of the authors or their affiliations is present.<br><br>By addressing these suggestions, the authors can strengthen the ethical considerations of their work, making a more compelling and ethically sound contribution to the field of machine learning and AutoML.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Our paper doesn't discuss potential positive societal impacts and negative societal impacts of the work performed.</span><br><span style='color:#0000FF;'>Review: Justification for<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: The response "NA" provided by the authors is consistent with the paper content. The paper introduces a meta-learning approach for algorithm recommendation, which does not involve the development or release of technologies inherently associated with high misuse potential, such as pretrained language models, image generators, or scraped datasets. The research focuses on enhancing model selection processes using existing data without specifying any new data collection or model creation that could raise ethical or misuse concerns. Thus, the justification that the paper poses no such risks aligns with the content and nature of the study.<br><br>No additional itemized constructive suggestions are necessary in this context since the question of responsible release and misuse does not apply to the research presented.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: creators or original owners of assets (e.g., code, data, models), used in the paper are cited.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors does not fully comply with the guidelines regarding the crediting and licensing of assets used in the research. While the answer indicates that the creators or original owners of assets are cited, it lacks specific information about the version of the assets used, URLs where those assets can be found, the names of the licenses under which these assets are released, and the explicit respect of the terms of use. This information is crucial for ensuring transparency, replicability, and legal compliance in academic research.<br><br>Constructive and actionable suggestions:<br>1. Citation Details: The authors should include specific references for all datasets, code, models, or other assets used in their research. This includes citing the original papers or sources that produced these assets.<br>2. Asset Version and Access: For each cited asset, the authors should specify the version used in their research. If possible, they should also provide direct URLs or stable DOIs (Digital Object Identifiers) to where these assets can be accessed.<br>3. License Information: It's important to explicitly state the license under which each asset is released. The authors should include the names of these licenses (e.g., MIT License, CC-BY 4.0) in their paper or supplementary materials. This step helps clarify the permissions and restrictions associated with the use of the assets.<br>4. Terms of Use Compliance: The authors should confirm they have complied with the terms of use for all assets. If any assets have specific requirements or restrictions, these should be acknowledged and detailed in the paper.<br>5. Clarification of Re-packaged or Modified Assets: If the research involves modifying or re-packaging existing assets, the authors need to clarify how these derived assets are licensed differently from the original assets. Both the original and derived licenses should be mentioned.<br>6. Documentation for Scraped Data: If the authors used data scraped from websites or other online sources, they should provide the copyright and terms of service for those sources, as well as discuss compliance with those terms.<br><br>By attending to these aspects, the authors can enhance the reproducibility of their research and demonstrate due diligence in crediting and using third-party assets responsibly.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: This is given in the github repo, whose link is given in the paper.</span><br><span style='color:#0000FF;'>Review: The response given by the authors is consistent with the typical requirements and practices within the domain of computer science research, particularly concerning the provision of assets like datasets and code. The justification aligns with the guidelines provided for documenting new assets introduced in a paper. Providing a GitHub repository link in the paper facilitates access to the datasets, code, models, and potentially other resources relevant to reproducing the research findings or extending the research. This is a common and acceptable practice in the field, promoting transparency, reproducibility, and open science.<br><br>However, to ensure the most critical and thorough review, it's important to highlight a few areas that could be further clarified or addressed by the authors, even though the score of 1 remains justifiable based on the information provided:<br><br>1. Documentation Quality: The specific contents of the GitHub repository are not detailed in the review excerpt provided. For a more thorough evaluation, it would be helpful to know if the documentation adequately covers how to use the datasets and code, explains any dependencies, outlines the expected outcomes, and provides troubleshooting tips. Actionable suggestions could include improving the README file to cover these aspects if not already done.<br><br>2. Licenses and Permissions: While the repository link is mentioned as the source of assets, the review excerpt does not confirm if the paper or repository includes information on the licensing for the use of these assets. It is essential for the documentation to clarify the license under which the code and datasets are released, ensuring potential users are aware of how they can legally use these assets. Suggest that the authors include clear licensing information in the GitHub repository if this is missing.<br><br>3. Limitations and Biases: The documentation should ideally also discuss the limitations of the assets, potential biases in the datasets, and the contexts in which the models or code are expected to perform well or poorly. Recommendations could be made to the authors to include such discussions either in the paper or within the documentation in the repository.<br><br>4. Anonymization and Consent: According to the guidelines, assets should be anonymized if applicable, and consent should be obtained for used assets. The provided excerpt does not clarify whether these aspects are addressed. Encouraging the authors to include statements regarding the anonymization of data and consent for use, especially if personal or sensitive data is involved, would be an appropriate suggestion.<br><br>Despite these points for potential improvement, the answer provided by the authors is correct based on the information given, justifying the score of 1.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer and justification provided by the authors are consistent with the content of the paper and the guidelines given for this question. The paper, titled "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," focuses on a computational method for algorithm recommendation and does not involve crowdsourcing experiments or research with human subjects. Therefore, it is not necessary for the paper to include instructions given to participants, screenshots, or details about compensation, as these elements are not applicable to the research conducted. The authors correctly identified their submission as NA (Not Applicable) for this question, in accordance with the guidelines provided.<br><br>There are no inconsistencies found between the answer, the justification, and the content of the paper related to research involving human subjects or crowdsourcing experiments. Consequently, no constructive or actionable suggestions are needed in this context, as the authors have adhered to the relevant guidelines for reporting on the type of study performed.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper and the guidelines provided. The paper describes an algorithmic approach to model selection or algorithm recommendation through active meta learning and does not involve human subjects or crowdsourcing. Therefore, the potential risks, disclosure to subjects, and the requirement for Institutional Review Board (IRB) approvals are not applicable to this study. The emphasis of the research is on algorithmic development and evaluation using datasets, making the NA (not applicable) response appropriate.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify Involvement of Human Data: While the paper is clearly algorithmic, if any of the benchmark datasets originated from human data (even if now publicly available and anonymized), a short clarification regarding the ethical considerations of using such datasets might be beneficial.<br><br>2. Future Work: Should future iterations of your work involve the collection of new datasets, especially ones that might include human participants (even indirectly), outlining potential steps towards ensuring ethical compliance would strengthen the research's integrity.<br><br>3. Transparency: Including a statement that explicitly mentions the non-involvement of human subjects directly in the research rather than simply stating "NA" could enhance clarity for readers unfamiliar with these terms of assessment.<br><br>No substantial changes are needed regarding the question of ethics involvement based on the current paper's content, as it remains strictly computational and data-driven.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The abstract and introduction clearly state the paper's main contributions and scope.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors that their abstract and introduction accurately reflect the paper's contributions and scope is correct based on the provided content. The abstract introduces the concept of algorithm recommendation with active meta-learning using a version of the CofiRank algorithm, the scope of work including real-world and artificial datasets, and ends with a claim about CofiRank's efficacy and computational efficiency. The introduction then expands on the field, setting the stage for their contribution and summarizing the state-of-the-art in algorithm recommendation, which aligns with the claims made in the abstract.<br><br>However, there are critical observations to be made regarding the depth and specificity of these claims:<br><br>1. Clarity and Specificity of Claims: While the abstract mentions that their approach rapidly identifies well-performing algorithms on new datasets at a reasonable computational cost, it might be beneficial to quantify what constitutes "rapidly" and "reasonable computational cost" based on their experiments. Providing specific examples from the results in the abstract could enhance clarity.<br><br>2. Scope of Contribution: The introduction lays a broad foundation but could highlight more distinctly how the proposed active meta-learning approach improves upon or differs from existing methods like AUTOSKLEARN and ALORS. Emphasizing unique contributions could enhance the reader's understanding of the paper's importance.<br><br>3. Limitations: According to the guidelines, it's critical for the abstract and/or introduction to also touch upon the limitations and assumptions of the proposed method. The document mentions doing so "in the future," which might leave readers wondering about the current limitations or assumptions involved.<br><br>Constructive and Actionable Suggestions:<br>- Quantify Contributions: Amend the abstract and introduction to include specific results or metrics that exemplify the paper's claims, making the contributions more tangible to the reader.<br>- Emphasize Unique Aspects: Clearly state how the approach is different from or improves upon existing methods in the introduction. This will help to underline the paper's originality and significance in the field.<br>- Address Limitations Clearly: Ensure that at least the introduction briefly outlines any assumptions or limitations of the proposed method. If these are not thoroughly discussed in the main text, including a section on this topic could be beneficial for setting realistic expectations for the reader.<br>- Enhanced Contextualization: Further contextualize the paper's contributions within the broader field of algorithm recommendation and meta-learning. This may include a brief discussion on how this work pushes the field forward or opens up new research directions.<br><br>The feedback aims at refining how the paper's contributions and scope are communicated to better align with reviewer expectations and enhance the overall impact of the paper.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The limitations are discussed in the section “Discussion and conclusion".</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. The "Conclusion and Limitations" section does indeed discuss the limitations of the work performed. The authors reflect on the scope of their claims, noting that the approach is based on a collaborative-filtering algorithm, CofiRank, and emphasizing that it does not always outperform a naive baseline method but consistently outperforms the median of Random ranking. Moreover, they mention a future direction to tackle "the harder and more interesting problem of meta-learning performance as a function of 'total' computational time rather than the number of algorithms tried," indicating an awareness of limitations related to computational efficiency and scalability.<br><br>However, for a more critical review, it would be beneficial to see more in-depth discussion about:<br>1. Robustness to violations of assumptions: The paper could improve by providing a more detailed discussion on how robust their results are against violations of any underlying assumptions made by their meta-learning algorithm, such as data quality issues in the meta-features used for the collaborative filtering.<br>2. Generalizability across different domains: There is a need for further elaboration on the generalizability of their approach across different domains and task types, considering that the empirical validation is limited to three real-world datasets (from StatLog, OpenML, and AutoML) and artificial data.<br>3. Impact of Hyperparameters: There is sparse discussion on how the choice of hyperparameters in their approach, such as the number of landmarks in Algorithm 5, influences the performance and computational cost. A detailed analysis or sensitivity test could enhance the understanding of the model's behavior across different configurations.<br>4. Scalability considerations: Even though the authors briefly mention computational considerations in their conclusion, a detailed analysis of how the proposed method scales with the number of algorithms or the size of the datasets (both in terms of features and samples) would provide valuable insights into its practical applicability.<br><br>Constructive suggestions for addressing these limitations could include:<br>- Conducting additional experiments to test the robustness of the proposed method under varying conditions, such as datasets with high levels of noise or missing values.<br>- Exploring the application of the method across a wider array of domains and including more diverse datasets in the experiments to prove its generalizability.<br>- Including a sensitivity analysis or ablation study to understand the impact of different hyperparameters on the algorithm's performance and computational cost.<br>- Providing theoretical analysis or empirical evidence on how the proposed method scales with increased dataset size and complexity, which could include exploring optimization techniques for scalability.<br><br>These suggestions aim to guide the authors in strengthening their work by addressing potential limitations and providing a clearer picture of the scope, robustness, and scalability of their proposed method.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: [TODO]</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors as "NA" is consistent with the content of the paper, indicating that the paper does not include theoretical results that require formal proofs or detailed assumptions. The focus of the paper is on an empirical approach to algorithm recommendation through active meta-learning and collaborative filtering, specifically using the ACTIVMETAL framework and the CofiRank algorithm.<br><br>Justification for agreement with the authors' answer:<br>- The paper primarily presents an empirical study rather than theoretical analysis. It proposes a novel method leveraging active meta-learning for algorithm recommendation and evaluates this approach on computational experiments using real-world and artificial datasets.<br>- The methodology section and the problem setting focus on the algorithmic and practical aspects of the proposed method, such as algorithm initialization schemes and selection strategies, without introducing new theoretical results that necessitate proofs.<br>- The results section deals with empirical validation, comparing the proposed method's performance against baseline approaches through the use of meta-learning curves rather than theoretical analysis.<br><br>However, for the sake of thoroughness, here are some constructive and actionable suggestions for the authors to enhance the clarity and completeness of their submission, if applicable in future work that involves theoretical contributions:<br>1. If the paper evolves to include theoretical analysis or results in the future, clearly state and enumerate all assumptions made in deriving these results at the beginning of the relevant section.<br>2. Include a section or appendix dedicated to formal proofs of any theoretical results introduced, ensuring that each step in the proof is well-documented and understandable.<br>3. For complex proofs that are too lengthy for the main paper, consider providing a proof sketch or overview in the main text, with detailed proofs relegated to supplementary material. Ensure the supplementary material is easily accessible and clearly references the associated results in the main paper.<br>4. Carefully cross-reference all theorems, lemmas, and corollaries within the text to improve the reader's ability to navigate and understand the theoretical framework.<br><br>These suggestions aim at hypothetical scenarios where the paper would include theoretical contributions and are provided for completeness in response to the question prompt.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors claims that all information needed for reproducing the experimental results is available in the form of pseudo-code and publicly accessible code. However, this does not fully address several vital components necessary for complete reproducibility, considering the guidelines for NeurIPS submissions.<br><br>Firstly, the paper provides algorithms' pseudo-code, which is a positive step towards reproducibility. Still, pseudo-code alone may not suffice to replicate the results without specific implementation details, versioning of libraries, or frameworks used. Moreover, the complexity of machine learning experiments often involves subtleties in data preprocessing, model initialization, or hyperparameter settings that are not fully captured by pseudo-code.<br><br>Secondly, while the paper mentions that code is accessible to the public, it doesn't explicitly specify the location (e.g., a URL to a GitHub repository) within the paper or check whether the accessibility is not restricted. For complete reproducibility, it is critical to ensure that both the data used in experiments and the exact code (including any scripts for data preprocessing, model training, evaluation, and generating results) are available and accessible.<br><br>Lastly, the paper refers to supplemental material and external sources (OpenML.org, AutoML datasets) but does not detail the exact versions of datasets used or how others could exactly reproduce these datasets from potentially dynamic sources. Similarly, relying on external resources requires guidance on dealing with potential discrepancies or updates to these resources over time.<br><br>Constructive Suggestions:<br>1. Explicitly Specify Code and Data Access: Provide clear, direct links to where the code and used datasets can be found, ensuring they are accessible without restrictions. Additionally, include instructions or scripts for generating or downloading the datasets.<br><br>2. Implementation Details: Accompany the pseudo-code with details on the implementation, including the programming language used, version numbers of major libraries or frameworks, and any crucial environment settings.<br><br>3. Parameter Settings and Initialization: Include a detailed description or a supplementary file specifying all hyperparameters, seeds for random number generators, and model initialization procedures that could affect the reproducibility of the results.<br><br>4. Versioning of Datasets: Clearly mention the version or snapshot of external datasets used. If datasets are created or modified, provide the exact procedures or scripts to enable others to recreate the exact dataset.<br><br>5. Supplemental Material: Make sure that all supplemental material mentioned for additional insights or validation (e.g., Jupyter notebooks, additional graphs) is also clearly linked and accessible.<br><br>6. Docker/Environment Setup: If a Docker image is recommended for setup, ensure the Dockerfile is available and includes instructions for building and running the environment necessary to replicate the experiments.<br><br>7. Guidelines for using External Data Sources: Provide guidance on handling any discrepancies due to updates or changes in external datasets or platforms like OpenML.org to ensure consistency in reproducing your results.<br><br>By addressing these points, the paper could substantially improve its adherence to the NeurIPS requirements for reproducibility and verifiability of results.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The github repo for reproducibility is given, in which you can find the code and dataset.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors correctly states that the paper offers open access to the data and code, which are essential for reproducing the main experimental results. The paper clearly states in the supplemental material section that datasets and code are available on Github, and it even goes further to recommend using a Docker image built for the purpose of ensuring a consistent environment for replication, thus addressing the guideline requirement for providing the exact command and environment needed. Moreover, additional materials, including a Jupyter notebook with further graphs referred to in the paper, are made accessible for a comprehensive understanding and replication of the experimental results, aligning with the guideline's suggestion to provide detailed instructions on data access, preparation, and scripts for reproducibility.<br><br>However, being critical and to ensure the reproducibility meets the highest standards, the following actionable suggestions are offered:<br>1. Detailed Reproducibility Instructions: Ensure that the Github repository contains a README file with step-by-step instructions on how to reproduce the main results, including any necessary data preprocessing steps.<br>2. Environment Details: Provide a requirements.txt file or an equivalent to detail all the dependencies and their versions. Although a Docker image is recommended, it's useful to have a list of dependencies for researchers not using Docker.<br>3. Clarify Dataset Licensing and Access: While the paper states that datasets are made available, it should clearly state any licensing conditions or restrictions on the datasets used, ensuring that all researchers know whether they can use the data freely.<br>4. Code Documentation: Enhance the documentation within the code to explain the purpose of different scripts and functions. This is particularly helpful for larger codebases or complex algorithms.<br>5. Anonymity Concerns: The paper addresses the need to preserve anonymity. Ensure that the anonymized versions of data and code do not inadvertently contain identifying information, and consider updating the repository with non-anonymized versions upon acceptance and publication.<br><br>These suggestions aim to bolster the reproducibility of the study and make it easier for other researchers to build upon this work. The authors have met the guidelines provided for data and code submission by making the necessary resources available and recommending an environment for reproduction.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results".</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors, stating that the details are given in the "Results" section, is not consistent with the content of the paper. Upon review, the "Results" section primarily focuses on the performance of the algorithms and the outcomes of the experiments, rather than detailing the experimental setup such as data splits, hyperparameters, type of optimizer, and how these were chosen. The experimental setup is crucial for understanding the context in which the results were obtained and for replicating the study.<br><br>To improve the paper, the authors should consider implementing the following suggestions:<br><br>1. Detail the Experimental Setup: Clearly outline the process of splitting the data into training and testing sets, including the ratio or method used (e.g., random split, stratified split). If cross-validation was used, describe how it was implemented.<br><br>2. Hyperparameter Configuration: Provide a detailed explanation of the hyperparameters used for each algorithm. Explain how these hyperparameters were chosen, whether through manual tuning, grid search, random search, or any other method. Include specific values or ranges considered.<br><br>3. Optimizer Information: If applicable, specify the type of optimizer used in the learning process (e.g., SGD, Adam) and any relevant configurations (e.g., learning rate, momentum).<br><br>4. Reproducibility: To ensure that the study can be replicated by other researchers, consider including a supplementary section or appendix detailing any setup aspects that couldn't be fully covered in the main text.<br><br>5. Code Availability: If not already included, provide a link to a repository containing the code used for the experiments, ensuring that it is well-documented. This greatly aids in understanding the experimental setup and reproducing the results.<br><br>Adding these details will not only improve the transparency of the research but also its impact, as other researchers will be able to build upon the work more effectively.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results" and the figures in this section.</span><br><span style='color:#0000FF;'>Review: The paper indeed reports error bars in the "Results" section, where performances of meta-learning algorithms are discussed in terms of the area under the meta-learning curve (AUMLC) normalized by the area of the best achievable curve. These error bars are presented alongside the mean performance scores for each method across the various benchmark datasets, with a notation such as ".91 (±0.03)" representing the mean and associated 1-sigma error bar. The authors ensure that the readers are informed about the statistical variability of the experiments' results. This adheres to the guidelines provided for reporting error bars, as it mentions how the variability factors are captured and clearly states the method used for calculating them (within the limitations of a summary given in the review context).<br><br>Nonetheless, while the paper correctly reports the presence and calculations of error bars, there is room for improvement to fully meet the guidelines provided:<br><br>1. Explicit Methodology Clarification: Although error bars are present, the paper could enhance its explanation of the exact methodology used to compute these error bars. Mentioning whether they are obtained through bootstrap methods, closed-form formulas, or a library function would provide clarity.<br><br>2. Assumptions Made: The paper does not specify the assumptions made when calculating the error bars, such as the normality of distribution, which is critical for interpreting the error bars correctly.<br><br>3. Variability Factors: While the presence of error bars suggests consideration of variability, the paper could further clarify whether these bars account for variability due to train/test splits, algorithm initialization, or other sources of randomness in the experimentation process.<br><br>4. Error Bar Type and Confidence Intervals: The paper should specify whether the reported error bars represent standard deviations, standard errors, or confidence intervals, and if confidence intervals are used, whether they represent a 95% confidence level or another value.<br><br>Constructive Suggestions:<br><br>- Clarify Calculation Method: Add a subsection or paragraph explicitly stating the methodology used for computing the error bars. If applicable, referencing specific statistical tests or software packages would enhance reproducibility.<br><br>- Explicitly State Assumptions: Include a discussion on the assumptions made in the statistical analysis, such as the distribution of errors. If the normality assumption is not verified, consider alternative ways to represent the data variability.<br><br>- Detail Variability Sources: Offer a clearer description of the sources of variability considered in the experiments and how these are reflected in the error bars. This information helps in understanding the robustness and reliability of the results.<br><br>- Error Bar Type: Specify the nature of the error bars (e.g., standard deviation, standard error, confidence interval) directly in the text where results are presented. For confidence intervals, clarify the confidence level used.<br><br>Implementing these suggestions would enhance the statistical clarity and robustness of the results presented, aligning the paper more closely with the provided guidelines for reporting statistical significance and error bars.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We only discussed the related computational expenses.</span><br><span style='color:#0000FF;'>Review: The provided answer is correct. Despite the engaging and innovative content concerning the active meta-learning approach for algorithm recommendation titled "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," the paper indeed lacks detailed information about the computational resources needed to reproduce the experiments. This omission is significant given the NeurIPS guidelines which emphasize the importance of describing computational resources to promote reproducibility and transparency. The answer "No" aligns with the content of the paper, as it explicitly acknowledges only discussing "related computational expenses" without specifying the type and extent of computational resources employed.<br><br>Constructive suggestions for the authors to consider in future revisions or related works:<br>1. Detail the Computational Resources Used: Include specific information about the computational environment in which the experiments were conducted. This should cover the type of processors (CPU/GPU), the amount/brand of RAM used, the type of storage, and any usage of cloud computing resources or internal clusters.<br><br>2. Execution Time Information: Provide details on the time taken for each experiment or model run. Indicate time in terms of the total execution time and, if applicable, break it down into training, validation, and testing phases.<br><br>3. Comparison to Other Methods: If relevant, compare the computational cost of your method with that of the baselines or state-of-the-art methods you have benchmarked against. This not only adds context to the computational cost discussions but also illustrates the efficiency or resource demands of your approach.<br><br>4. Disclose Complete Compute Requirements: Mention the total compute used, including for preliminary experiments, failed experiments, or any other computational effort that contributed to the final results or findings presented in the paper.<br><br>5. Guidance for Practitioners: Offer recommendations for practitioners on the minimum or optimal computational resources required for effective use of your proposed method or system. This can aid potential users in assessing the feasibility of adopting your approach in their own work.<br><br>6. Link to Code and Resources: If not already provided, ensure to link to a repository containing the source code, along with a detailed README file guiding users on how to deploy and run your experiments. Including a Dockerfile or environment setup scripts can significantly ease the process of replicating your work.<br><br>These suggestions aim to enhance the completeness, reproducibility, and accessibility of your research, aligning with the high standards of publication venues like NeurIPS.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We reviewed the code of ethics and our paper conforms with it.</span><br><span style='color:#0000FF;'>Review: Based on the content provided in the manuscript of "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," the answer provided by the authors, stating compliance with the NeurIPS Code of Ethics, appears consistent with what is discernible from the paper. The research appears to follow ethical standards in terms of its purpose, methodology, and presentation. However, the evaluation of ethical compliance based solely on the authors' affirmation and without explicit evidence or discussion in the paper itself inherently limits the depth of this assessment.<br><br>To ensure a rigorous evaluation and enhance the paper's adherence to ethical standards, consider the following itemized, constructive suggestions:<br><br>1. Data Ethics and Privacy: The utilization of real-world datasets from StatLog, OpenML, and AutoML suggests a commitment to utilizing existing, possibly public, benchmarks. However, it could enhance the ethical transparency of the paper if the authors explicitly addressed any privacy considerations related to these datasets, even if they are publicly available.<br><br>2. Bias and Fairness: Given the paper's focus on algorithm recommendation, an exploration of potential biases in the recommended algorithms or the datasets themselves would be pertinent. The authors could discuss whether their methodology accounts for or could perpetuate existing biases in machine learning applications and suggest ways to mitigate such issues.<br><br>3. Impact on Society: While the paper focuses on technical aspects, a brief discussion on the potential societal impact of making algorithm recommendations more efficient and accurate could enhance its ethical considerations. This includes both positive impacts (efficiency gains, broader applicability of machine learning) and potential negative consequences (reinforcement of existing biases, over-reliance on automated recommendations).<br><br>4. Reproducibility: The authors have provided supplemental material, datasets, and code on Github, which is commendable for reproducibility. A clear statement affirming this practice directly in the paper would reinforce their commitment to the ethical guideline of ensuring that research can be independently verified and builds a foundation for future research integrity.<br><br>5. Environmental Considerations: Given the computational nature of the work, a discussion on the environmental impact of running extensive machine learning models, as well as any steps taken to minimize this impact, could be an insightful addition, aligning with the broader ethical considerations of computational research.<br><br>6. Transparency and Conflict of Interest: Although the paper is listed as anonymous for review, ensuring that the final version, if accepted, clearly discloses any potential conflicts of interest would adhere to ethical guidelines regarding transparency.<br><br>Of course, without specific details or relevant sections of the paper devoted to ethical considerations, these suggestions are based on general best practices in ethical AI research and the information provided. Engaging with these topics within the paper would not only affirm the authors' claim of ethical compliance but also contribute to a more ethically aware and comprehensive body of research.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Our paper doesn't discuss potential positive societal impacts and negative societal impacts of the work performed.</span><br><span style='color:#0000FF;'>Review: The authors' answer and justification do not fully align with the expectations set out in the guidelines for addressing the societal impact of their work. Given the nature of the work on algorithm recommendation systems, particularly in the context of machine learning model selection, there are clear pathways to both positive and negative societal impacts. The active meta-learning approach proposed in their work could significantly streamline and optimize the process of model selection, leading to efficiencies in computational resources and potentially enabling more rapid advancements in fields that benefit society. On the other hand, the same technology could be applied in contexts where the optimization of algorithms could lead to unintended negative consequences, such as enhancing the capabilities of surveillance systems or contributing to the development of more sophisticated autonomous weapons systems.<br><br>Constructive feedback and actionable suggestions:<br>1. The authors should revisit the societal impact section of their paper and consider the broader implications of their work. This includes reflecting on how their active meta-learning approach could influence fields beyond purely technical advancements, either directly or indirectly.<br>2. The authors are encouraged to include a discussion on both the positive impacts, like advancements in disease research or more efficient energy usage through better algorithm selections, and potential negative impacts, including but not limited to privacy, security, and ethical considerations.<br>3. It would be beneficial for the authors to speculate on future applications of their work and the sectors it may most directly affect. This can help frame the societal impact discussion in a more tangible context.<br>4. In discussing negative impacts, the authors should consider providing suggestions for mitigating these risks. For example, they might propose guidelines or ethical considerations for deploying their technology, or suggest mechanisms for oversight and governance.<br>5. Finally, acknowledging the limitations or potential misuse of their technology can demonstrate a responsible and forward-thinking approach to research. This includes outlining steps for future research to address these concerns.<br><br>By addressing these points, the authors can provide a more balanced and comprehensive view of their work's societal implications, aligning with the conference's expectation for a thoughtful consideration of both positive and negative aspects.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: Based on the content and context of the paper "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," the authors' response of "NA" to the question regarding safeguards for responsible release seems appropriate and in accordance with the paper's content and focus. The primary aim of the paper is to introduce an advanced approach towards algorithm recommendation through active meta-learning, specifically in the context of model selection and algorithm recommendation using collaborative filtering. The paper's core contributions revolve around the development, evaluation, and comparison of meta-learning methods without any direct mention or indication of developing or releasing models or datasets that could inherently carry a high risk for misuse, such as pretrained language models, image generators, or scraped datasets.<br><br>Justification for the score:<br><br>- The paper's scope is confined to algorithm recommendation using an advanced meta-learning technique and does not involve creating or modifying datasets or models that could be categorized as high-risk for misuse.<br>- The methodological focus is on enhancing efficiency and performance in algorithm selection, which does not directly interact with sensitive, personally identifiable information or generate potentially harmful content.<br><br>Constructive suggestions:<br><br>While the "NA" response is justified based on the paper's content, for further clarity and thoroughness, the authors could briefly expand their "NA" response to explicitly state that their research, being primarily methodological and focused on algorithm recommendation, does not involve the creation, use, or release of models or datasets that could be misused. Additionally, given the evolving nature of ethical concerns and misuse potential in machine learning research, the authors may consider discussing any general ethical considerations they took into account during their research design and execution, even if these considerations do not directly relate to the specific use-case scenario outlined in the question. This would not only reinforce the paper's alignment with responsible AI principles but also preemptively address any related questions or concerns from readers or reviewers.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: creators or original owners of assets (e.g., code, data, models), used in the paper are cited.</span><br><span style='color:#0000FF;'>Review: The provided justification mentions that the creators or original owners of assets used in the paper are cited. However, this response is insufficient and lacks detail regarding the explicit mentioning of licenses and terms of use of these assets, which is a specific requirement according to the guidelines. Additionally, the response does not indicate whether URLs were provided for the different assets or if the version of the assets used was stated, both of which are necessary for complete compliance with the guidelines provided.<br><br>Constructive and actionable suggestions for improvement include:<br><br>1. Explicitly Mention Licenses and Terms of Use: For each asset used (datasets, code packages, or models), explicitly mention the name of the license (e.g., MIT, CC-BY 4.0) and include a brief statement on how the terms of use were respected in the context of your research.<br><br>2. Provide URLs and Version Information: If not already done, ensure that the paper includes URLs for accessing the assets used, if they are publicly available. Additionally, specify the versions of the assets to enable reproducibility and to ensure that readers can access the same resources as were used in the study.<br><br>3. Detail on Data Source Compliance: For datasets that were possibly scraped or collected from online sources, include a statement confirming compliance with the copyright and terms of service of those sources. This is especially important if there's any ambiguity about the data’s usage rights.<br><br>4. Clarifications in Supplemental Materials: If space constraints in the paper limit the amount of detail you can provide about asset licensing and terms of use, consider including detailed information in the supplemental materials. This could encompass a section dedicated to asset accreditation, licenses, and any permissions obtained for asset use.<br><br>5. Engage with Asset Creators: In cases where the licensing information is unclear or not publicly available, reach out to the creators of the assets for clarification. This engagement can also be documented in the paper to further validate the ethically responsible use of the assets.<br><br>By addressing these areas, the paper would not only meet the requirements laid out in the guidelines but also enhance its transparency and reproducibility, which are critical aspects of ethical research practices.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: This is given in the github repo, whose link is given in the paper.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors suggests that they have documented new assets and placed these documents in a GitHub repository linked within the paper. However, simply stating that documentation "is given in the GitHub repo" without further elaboration on the contents or quality of this documentation is insufficient to ascertain compliance with the guideline's comprehensive requirements.<br><br>To fully comply with the guidelines, the paper should explicitly detail:<br>1. The nature and scope of the documentation available in the GitHub repository.<br>2. If and how the documentation addresses training details, usage limitations, and any licensing issues.<br>3. Methods for ensuring the anonymity of assets, particularly if they include data derived from individuals who may not be aware their information is being used for research.<br><br>Constructive and actionable suggestions for the authors include:<br>1. Enhance the section of the paper that discusses the assets (e.g., datasets, software, models) introduced by providing a summary of what types of documentation are available for each asset.<br>2. Include specific references or links to files or sections within the GitHub repository that contain the relevant documentation.<br>3. Ensure the documentation within the GitHub repository includes a clear and detailed explanation of how the assets can be used, any restrictions on their use (including licensing information), and any prerequisites or dependencies required to use the assets.<br>4. Discuss the process of collecting data or developing the assets, with a particular focus on ethical considerations such as consent and data privacy. If applicable, describe how the anonymity of data sources has been preserved.<br>5. For better transparency and reproducibility, consider adding a section in the supplementary material that demonstrates a use case of the documented assets to show their applicability and potential impact on future research.<br><br>Addressing these points in the paper and the associated documentation can significantly enhance the utility and transparency of the assets introduced, making them more valuable to the broader research community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer is consistent with the content of the paper. Upon thorough review of the provided paper segments, there is no mention of crowdsourcing experiments or research involving human subjects. The focus is on an algorithmic approach for model selection and algorithm recommendation using active meta-learning, particularly through a novel method that incorporates aspects of collaborative filtering. There is a clear emphasis on computational processes, algorithmic evaluation, and data modeling, thus making the involvement of human subjects or crowdsourcing efforts irrelevant to the nature of this research.<br><br>However, it is crucial to maintain attentiveness to such considerations in reviewing papers, particularly those that may implicitly involve human data or subjectivity in dataset creation, selection, or analysis. In this case, the paper's structure and content do not warrant the inclusion of instructions, screenshots, compensation details, or ethical considerations related to human subjects. The authors' designation of "NA" is accurate and aligns well with the guidelines provided.<br><br>Constructive and Actionable Suggestions:<br><br>Although not applicable in this specific case, for future reference or similar studies by the authors, should any aspect of human subject research or crowdsourcing be incorporated, it is recommended to:<br><br>1. Clearly delineate the scope and nature of human participation within the research, detailing the roles and contributions of participants.<br>2. Provide comprehensive instructions, information sheets, and consent forms used in the research process, ideally within the supplemental materials if too voluminous for the main body.<br>3. Discuss the compensation framework, ensuring adherence to ethical guidelines and local laws regarding minimum wage and fair compensation.<br>4. Implement and describe ethical considerations and approvals obtained, specifically outlining measures taken to ensure participant privacy, confidentiality, and well-being throughout the research process.<br><br>These steps will not only enrich the paper's methodology section but also reinforce the ethical integrity and societal value of the research outcomes.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The authors have correctly answered "NA" to the question regarding the potential risks incurred by study participants, disclosure of risks, and Institutional Review Board (IRB) approvals, justifying their response with the reasoning that the paper does not involve crowdsourcing nor research with human subjects.<br><br>Explanation: Upon review of the paper content, it is clear that the research focuses on a technical framework for algorithm recommendation with active meta-learning. The work is computational in nature, dealing with model selection, recommendation systems, and the application of active learning and collaborative filtering techniques to the problem of algorithm selection. The methodology involves experiments with both artificial and real-world datasets but does not incorporate human participants in the study design nor does it utilize crowdsourced data in a manner that would involve interaction with study participants, thus eliminating concerns related to human subjects research.<br><br>Given the technical and computational focus of the study, which does not interact with or involve human subjects, the authors' response that IRB approval is not applicable (NA) aligns with the content and nature of the research. This means there was no necessity for ethical review concerning risks to human subjects or a need for disclosures typically associated with human-centered research.<br><br>Constructive Suggestions:<br>1. Although not applicable in this specific case, it might be beneficial for future works that could potentially touch upon data privacy or ethical concerns, even tangentially, to include a brief statement acknowledging these aspects and ensuring compliance with ethical standards.<br>2. For papers within the scope that might later involve human-related data or interaction, authors should be encouraged to proactively consider ethical implications and IRB requirements as part of their initial research design planning.<br>3. The authors may also consider including a statement on data privacy and ethical use of the datasets, especially when utilizing real-world datasets, to further reinforce their commitment to ethical research practices, even in purely computational studies.<br><br>Overall, the authors' response is consistent with the presented work's focus and methodology, making the justification for "NA" accurate and appropriate.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>truth_adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The abstract accurately reflects the paper's contributions and scope. But the first paragraph of the introduction was replaced by a fake version.</span><br><span style='color:#0000FF;'>Review: Justification: The reviewer's comment that "the first paragraph of the introduction was replaced by a fake version" does not align with the expectation set by the guidelines for accurately reflecting the paper's contributions and scope. The abstract did indeed provide an accurate overview of the paper's contributions, focusing on an active meta-learning approach to algorithm recommendation and comparing it with baseline methods through real-world and artificial datasets, which aligns with the content subsequently detailed in the paper. The introduction, despite containing a whimsical narrative about "Mystic Computation" and "Synthetic Cognition," eventually converges back to the topic at hand - active meta-learning for algorithm recommendation, providing a context that blends imaginative exposition with the paper's actual focus. While the whimsical narrative might initially seem disconnected from the paper's technical content, it serves a thematic introduction to the field of Automated Machine Learning (AutoML) in a creative manner, before transitioning to a detailed discussion on related works and the novelty of the proposed approach. Therefore, the claim in the answer that the abstract accurately reflects the paper's contributions seems consistent with the paper's content, but the justification dismissing the entirety of the introduction based on its creative start is not completely fair, given that the introduction does eventually align well with the contributions detailed later in the paper.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify the Transition: Ensure a clear transition in the introduction from the creative narrative to the paper's technical content. This could include a sentence that directly connects the whimsical elements to the actual focus of the paper, ensuring readers understand the creative exposition's purpose.<br>2. Emphasize Contributions Early: To avoid any ambiguity about the paper's contributions and scope, consider briefly summarizing the main contributions earlier in the introduction before delving into thematic narratives.<br>3. Align Justifications with Content: When justifying the alignment of the abstract and introduction with the paper's contributions, explicitly acknowledge any creative or thematic elements and explain how they serve to contextualize the technical content or engage the reader with the topic.<br>4. Ensure Consistency with Guidelines: Review the guidelines provided for assessing whether the abstract and introduction accurately reflect the paper’s contributions and scope. The justification for the assessment should explicitly mention how the paper meets these guidelines, even when employing creative narratives or thematic elements.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: There is a section named “Conclusion and Limitations", but it doesn't discuss the limitations of the work at all.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors stating "No" in reference to discussing limitations is inconsistent with the content presented in the "Conclusion and Limitations" section of the paper. The paper does outline limitations, particularly focusing on the current scope of their active learning approach and its reliance on previous algorithm runs without considering the computational run time of algorithms. They briefly mention the intention to explore algorithm performance as a function of total computational time in future studies.<br><br>To align correctly with the guidelines and the content, the answer should have been "Yes", as the paper does discuss limitations albeit perhaps not in extensive depth. It acknowledges the need for future work to tackle computational efficiency and broader application scenarios, which is in line with the guidelines' emphasis on discussing strong assumptions, the robustness of results, and the computational efficiency of proposed algorithms.<br><br>Constructive and actionable suggestions for the authors to improve the discussion of limitations include:<br><br>1. Expand on Existing Limitations: Provide more detailed discussions on the limitations mentioned, particularly regarding computational efficiency and the methodology's scalability. This could involve exploring hypothetical scenarios or providing preliminary theoretical analysis.<br><br>2. Address Generalizability: Considering the approach was tested on specific benchmark datasets, discuss how the approach might perform in potentially less favorable conditions or on datasets with characteristics significantly different from those tested.<br><br>3. Assumptions and Robustness: Elaborate on any underlying assumptions made during the development of the proposed approach and actively discuss how robust the approach is against violations of these assumptions. Offering some insights into the sensitivity analysis could be beneficial.<br><br>4. Ethical and Practical Considerations: Where applicable, briefly discuss any ethical considerations, especially if the algorithms recommended might be used in sensitive applications. Additionally, consider discussing the fairness and privacy implications if relevant.<br><br>5. Encourage Transparency and Reproducibility: Make sure that all datasets, code, and additional material necessary to reproduce the results are readily available. Highlight this in the paper to promote transparency and ease of validation by other researchers.<br><br>By addressing these points, the paper will provide a more complete and honest assessment of its contributions and limitations, aligning with the best practices for scientific reporting and potentially fostering greater trust and engagement from the research community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: [TODO]</span><br><span style='color:#0000FF;'>Review: The provided answer by the authors, "NA," accurately reflects the content and focus of the paper under review. Based on the content provided in the summary, the paper, "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," focuses on an active meta-learning approach for algorithm recommendation and does not explicitly present or rely on theoretical results, theorems, or mathematical proofs. Instead, it is centered around algorithm development, empirical validation, and comparison with baseline methods. The research is oriented towards a practical application of active meta-learning in algorithm recommendation, leveraging collaborative filtering techniques and does not delve into theoretical analyses that would require detailed proofs or theorem formulations.<br><br>Justification for the score:<br><br>1. Consistency with the paper's content: The abstract and sections provided in the summary give a clear indication that the paper's primary contribution is in proposing a novel active meta-learning method and demonstrating its effectiveness through empirical results on real-world and artificial datasets. There's no mention of theoretical results or derivations that necessitate formal proofs or deep theoretical analysis.<br><br>2. Adherence to guidelines: Since the paper does not include theoretical results, the "NA" answer aligns with the guidelines for answering this specific question. The guidelines clearly state that "NA" should be selected if the paper does not include theoretical results, which is the case here.<br><br>Constructive Feedback and Actionable Suggestions:<br><br>Though the theoretical aspect might not be the focus of this paper, incorporating discussions or sections that touch on the theoretical foundations of the proposed methods or principles (even if peripheral) could add depth and foster a deeper understanding for readers. For future work or iterations of this research, consider:<br><br>1. Discussion on theoretical underpinnings: Briefly discuss the theoretical motivations or the underlying principles that guide the proposed active meta-learning approach, even if the paper primarily focuses on empirical validation.<br><br>2. Proof sketches for key propositions: If there are any key propositions or assumptions that the approach relies on, including short proof sketches or rationale in the supplementary materials could enhance the paper's rigour.<br><br>3. Impact of assumptions on empirical results: While detailed proofs might not be necessary, discussing how certain theoretical assumptions or conditions could impact the algorithm's performance in practical scenarios could provide valuable insights and strengthen the paper's contributions.<br><br>4. References to foundational works: Where applicable, reference foundational works or theoretical studies that support the design and implementation of the proposed methods.<br><br>These suggestions are aimed at enhancing the paper's comprehensiveness and bridging the gap between empirical findings and their theoretical implications, without detracting from the paper's primary focus on active meta-learning for algorithm recommendation.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public.</span><br><span style='color:#0000FF;'>Review: The provided justification for the full disclosure of information needed to reproduce the experimental results leans heavily on the assumption that the pseudo-code and accessibility of code to the public are sufficient for reproduction. However, this assumption overlooks several critical aspects necessary for true reproducibility, as mentioned in the paper and further emphasized in the NeurIPS guidelines on reproducibility.<br><br>1. Lack of Specifics about Computational Resources: Reproducing experimental results isn't just about the algorithm's logic (contained in pseudo-code) or having access to code. The computational resources (e.g., CPU, GPU specifications) and the exact environment (versions of software/libraries used) can significantly affect the outcome of experiments, especially in fields where results are sensitive to numerical precision and computational performance.<br><br>2. Details on Data Preprocessing: While the paper mentions the datasets used, there is insufficient detail on any preprocessing steps. Data preprocessing can dramatically affect machine learning outcomes, and without precise replication of these steps, the results may not be reproducible.<br><br>3. Hyperparameters Setup: The paper does not delve into the specifics of how hyperparameters were chosen for the algorithms compared. Given that the performance of machine learning algorithms can be highly sensitive to hyperparameter settings, this oversight can hamper reproducibility.<br><br>4. Evaluation Metrics and Partitioning of Data: There should be a detailed explanation of how the data was split into training and testing sets, along with a thorough description of the evaluation metrics used. This ensures that others can exactly replicate the study's experimental setup.<br><br>Constructive and Actionable Suggestions:<br><br>a. Detailed Computational Environment: Provide detailed information about the computational environment used, including hardware specifications and versions of all relevant software and libraries.<br><br>b. Data Preprocessing Steps: Elaborate on any data preprocessing steps taken before the actual experiments, including data cleaning, normalization, feature extraction, etc.<br><br>c. Hyperparameter Choices and Tuning: Explain how hyperparameters were chosen or tuned for each algorithm examined. If any form of automated hyperparameter optimization was used, detail the process and include it in the supplementary material.<br><br>d. Code and Data Availability: Ensuring the code is publicly accessible is commendable; however, confirm that all data used in the experiments is also available. If there are any proprietary or privacy concerns with the data, consider providing a simulated dataset that can produce similar experimental outcomes.<br><br>e. Supplemental Material: Utilize supplemental material to provide exhaustive implementation details that could not be included due to space constraints in the main paper. This can include complete code listings, detailed environmental setup instructions, and additional experimental results.<br><br>By addressing these details, the authors will significantly enhance the reproducibility of their research, aligning with the spirit of the NeurIPS guidelines and fostering greater trust and validation of their findings within the community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The link to the github repo given in the paper for reproducibility is fake.</span><br><span style='color:#0000FF;'>Review: Justification for Disagreement:<br>The provided review information in the answer is not consistent with the content of the paper. The paper explicitly mentions in the supplemental material section that datasets and code are available on GitHub for full reproducibility of results. Further, it suggests using a Docker image built for this purpose, indicating an effort to ensure that the environment for reproducing the experimental results is well-defined. The statement about the GitHub link being fake seems to be incorrect based on the paper content presented, which suggests an earnest effort to enable reproducibility.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify GitHub Link Validity: Ensure that the GitHub link mentioned in the paper is active and accessible. If the review period is still anonymized, consider using a temporary anonymous repository link, and ensure it is operational.<br>2. Detail Reproduction Steps: Make sure the GitHub repository includes detailed instructions on how to reproduce the main experimental results. This should include environment setup (e.g., required libraries and versions), data preprocessing steps, and the exact commands to run the experiments.<br>3. Include Environment Specifications: Since the Docker image is recommended for running the experiments, provide clear instructions on how to use this Docker image, including how to install Docker, pull the image, and run the container with the necessary parameters.<br>4. Data Accessibility: Include detailed instructions on accessing, downloading, and preparing the data used in the experiments. If there are any permission or access issues related to the data, provide guidance on how these can be addressed.<br>5. Script Organization: Organize the scripts and code in the GitHub repository in a manner that is easy to navigate. Clearly label scripts associated with specific experiments or figures in the paper.<br>6. Address Omitted Experiments: If any experiments or results in the paper cannot be reproduced with the provided code and scripts, clearly state this and explain why. Offer alternative ways to verify these results, if possible.<br>7. Post-Review Update: After the review process and acceptance of the paper, update the paper with the final, non-anonymous GitHub repository link to ensure long-term access to the data and code.<br><br>These suggestions aim to enhance the reproducibility of the experimental results and align with best practices for sharing code and data in scientific research.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: These details are given in the section of “Results".</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors does not accurately reflect the content of the paper regarding the specificity of training and test details necessary to understand the results. While the "Results" section discusses outcomes and performance comparisons of the proposed methods to baselines, there is a significant lack of detailed information on data splits, hyperparameters, how these were chosen, and the optimizer type used in the experiments.<br><br>Inconsistencies with the Paper Content:<br><br>1. Data Splits: The paper mentions using benchmarks from several sources and mentions cross-validation in the benchmark data section, but it lacks specifics on how the data was split (e.g., proportions for training, validation, and testing) for each dataset. Additionally, the leave-one-dataset-out procedure is mentioned without detailed methodology on execution.<br><br>2. Hyperparameters: There is no explicit section or mention of how hyperparameters for each algorithm in the active meta-learning process were chosen or optimized.<br><br>3. Type of Optimizer: The paper does not specify the type of optimizer used for training the algorithms, which is crucial for the reproducibility of results and understanding the optimization landscape of the model recommendations.<br><br>Constructive and Actionable Suggestions:<br><br>1. Detail Data Split Methodology: The authors should provide a clear description of how the datasets were split for training, testing, and if applicable, validation. If different datasets required different splitting strategies, these should be articulated clearly per dataset.<br><br>2. Hyperparameter Selection Process: The paper should include a dedicated section that describes the hyperparameter tuning process. This could include detailing any search strategies employed (e.g., grid search, random search), the range or space of hyperparameters explored, and any cross-validation schemes used in the hyperparameter optimization process.<br><br>3. Optimizer Information: The authors should mention the specific optimizer(s) used in the experiments, including any relevant configurations such as learning rate, momentum, etc. If different optimizers were used for different parts of the process or different datasets, this information should be included.<br><br>4. Supplemental Material/Appendix: If including this information within the main body of the paper is problematic due to space constraints, the authors could consider adding it as an appendix or supplemental material. However, a summary of these details should still be present in the main text to give readers a clear understanding of the experimental setup.<br><br>5. Code and Reproducibility: Strengthen the paper's contribution to the field by ensuring that the provided code repository is well-documented, with clear instructions on replicating the experiments, including data preprocessing, model training, and evaluation procedures.<br><br>Addressing these points would significantly improve the paper's clarity regarding experimental setup, making it easier for readers to understand the results and accurately reproduce the study.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: The figure is fake, with NO ERROR BARS.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors does not align with the paper content. The paper does contain error bars and relevant information regarding the statistical significance of the experiments. Specifically, in Table 2, performance metrics are presented with a 1-sigma error range (e.g., Active_Meta_Learning w. CofiRank .91 (±0.03)). This directly contradicts the claim made in the justification that "the figure is fake, with NO ERROR BARS." The error bars capture variability factors, likely from leave-one-dataset-out cross-validation as implied by the paper's methodology, and provide a measure of statistical confidence in the reported results.<br><br>Constructive suggestions:<br>1. The response might have been a misunderstanding or an oversight. It is essential to accurately review and interpret the results and methodologies described in the paper.<br>2. Ensure that all figures and tables mentioned in the justification correctly reflect the information presented in the paper. Specifically, double-check that error bars or confidence intervals are accurately reported and described in both the text and any visual representation.<br>3. Clarify in the paper how the error bars were calculated. Although the paper mentions 1-sigma error bars, a more detailed explanation of their computation would strengthen the manuscript. This includes specifying whether these error bars account for variations due to train/test splits, initialization randomness, or other factors.<br>4. The paper could benefit from a discussion on the assumptions made for statistical analysis. For example, if assuming normality for error calculations, this should be justified or acknowledged as a limitation.<br>5. Consider expanding on the statistical analysis by possibly including additional measures of statistical significance, such as p-values or confidence intervals, to complement the error bars.<br>6. Reference figures, tables, and specific results in the main text when discussing the statistical significance and error bars, ensuring that readers can easily locate and understand the basis for the paper's claims.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We only discussed the related computational expenses.</span><br><span style='color:#0000FF;'>Review: The authors acknowledge that the paper does not provide specific details on the computer resources needed for reproducing the experiments, aligning with the question asked. Given this admission and upon reviewing the paper content, it is clear that specific details such as the type of compute workers (CPU or GPU), the memory required, the execution time, or the computing platform (internal cluster, cloud provider, etc.) are indeed not disclosed. This information is necessary for replicating the study's results accurately and efficiently, as computational requirements can significantly affect the outcomes and reproducibility of machine learning experiments.<br><br>Constructive and actionable suggestions:<br>1. Detailed Computational Resources: Future versions of the paper should include a section or a table that details the computational resources used for each experiment. This should cover the type of compute workers (CPU/GPU), the specific models or generations (e.g., NVIDIA Tesla V100 GPU), memory specifications, storage requirements, and the runtime for each experimental setup.<br><br>2. Compute Platform Details: If experiments were conducted on an internal cluster or via a cloud provider, please specify this and include any relevant configurations that could impact performance or cost, such as the use of preemptible instances or special hardware accelerators.<br><br>3. Computational Cost Analysis: Given the active learning nature of the proposed approach and its potential for reducing the computational burden by minimizing the number of algorithm evaluations, a discussion on the computational cost benefits in comparison to traditional methods could significantly enhance the paper's impact. Quantitative analyses highlighting the computational savings or efficiency improvements would be particularly valuable.<br><br>4. Failed Experiments and Preliminary Runs: If applicable, briefly discuss any preliminary or failed experiments that required substantial compute resources but did not make it into the final paper. This transparency can provide valuable insights into the development process of your method and help readers gauge the overall computational demands of your research project.<br><br>5. Reproducibility Package: Consider providing a reproducibility package or supplementary material that includes scripts for setting up computational environments (e.g., Docker containers, as already mentioned for CofiRank), along with detailed requirements and instructions. This package should ideally allow researchers to replicate your experiments with minimal setup overhead.<br><br>By addressing these points, the paper would significantly improve in terms of reproducibility and transparency, allowing others in the community to build upon this work more effectively.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We reviewed the code of ethics and our paper conforms with it.</span><br><span style='color:#0000FF;'>Review: The authors' response that their work conforms with the NeurIPS Code of Ethics, without providing specific details, is consistent with standard practice when there are no ethical concerns directly implicated by the research. Based on the content of the paper titled "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning," there are no explicit ethical issues or concerns evident in the methodologies, experiments, or discussions presented. The research focuses on the development of an algorithmic approach for recommending machine learning models based on active meta-learning, which does not directly involve human subjects, sensitive data, or ethically questionable practices.<br><br>However, the response could have been more informative if the authors had briefly discussed specific aspects of the Code of Ethics relevant to their work, even if to confirm that those aspects were considered and found to be compliant. For instance, they could have mentioned considerations regarding transparency, fairness, and the potential impact of their work.<br><br>Constructive and Actionable Suggestions:<br><br>1. Enhance Ethical Justification: In future submissions or revisions, it might benefit the authors to provide a brief summary of the main ethical considerations relevant to their research area (e.g., algorithmic fairness, transparency, and potential societal impact) and explicitly state how their research addresses these considerations.<br><br>2. Data Handling and Privacy: Even though the research primarily involves algorithmic development and comparisons, ensuring the ethical use of datasets is crucial. The authors should confirm that the datasets used (especially those sourced from OpenML and AutoML competitions) do not contain sensitive or proprietary information and that they comply with data privacy regulations.<br><br>3. Algorithmic Bias and Fairness: Given the increasing scrutiny of algorithmic decision-making systems, the authors should consider discussing any potential biases their recommendation system might introduce and how such biases could be mitigated. This is especially relevant if their system were to be applied in heterogeneous or high-stakes environments.<br><br>4. Impact Assessment: The authors could enhance the paper by including a brief discussion on the potential positive and negative impacts of their work on the broader scientific community and society. Highlighting applications where their algorithm recommendation system could be particularly beneficial (or harmful if misapplied) would help in evaluating the ethical considerations of their research’s impact.<br><br>Overall, while the response to the question on ethical conformity is acceptable based on the provided information, these suggestions aim at strengthening the narrative around ethical considerations in their research, in alignment with the NeurIPS Code of Ethics.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Our paper doesn't discuss potential positive societal impacts and negative societal impacts of the work performed.</span><br><span style='color:#0000FF;'>Review: Justification:<br>The reviewers' justification and the guidelines provided indicate a mismatch in understanding the potential societal impacts of the work presented in the paper. The paper focuses on an active meta-learning approach for algorithm recommendation, which, while perhaps foundational in nature, can have broader societal implications worth considering. Although the paper is primary research and may not directly apply to immediate applications or deployments, the technology's indirect pathways to societal impact should be acknowledged.<br><br>Constructive and actionable suggestions:<br><br>1. Societal Impact Discussion: The authors should include a brief section discussing potential positive societal impacts, such as how the proposed active meta-learning approach could accelerate the process of finding optimal machine learning models, thereby advancing fields that rely on machine learning technologies. This could include speeding up research in healthcare, environmental science, and other areas critical to societal welfare.<br><br>2. Consideration of Negative Impacts: Even if the research is foundational, a discussion on the possible negative societal impacts should be included. This could involve theoretical explorations of how improvements in algorithm recommendation systems might be misused or lead to unintended consequences, such as exacerbating biases in model selection, increasing automation and potential job losses in certain sectors, or even misuse in surveillance applications.<br><br>3. Mitigation Strategies: If negative societal impacts are identified, the authors should discuss possible mitigation strategies. For instance, they could propose guidelines for the ethical use of algorithm recommendation tools, mechanisms for bias detection and correction in model selection, or recommend transparency in the use of such technologies in critical applications.<br><br>4. Broader Ethical Considerations: The paper should briefly touch upon any ethical considerations relevant to the development and use of active meta-learning technologies. This includes but is not limited to, privacy concerns, fairness, transparency, and accountability in automated decision-making.<br><br>5. Community Engagement: Lastly, the authors could suggest how engaging with broader communities, including policymakers, social scientists, and ethicists, might help identify and mitigate potential negative impacts early in the technology development process.<br><br>Including these considerations will not only comply with the conference guidelines but also enrich the paper's contribution by highlighting the broader implications of the research within society.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our paper poses no such risks.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors as "NA" with the justification that their paper poses no such risks is consistent with the content and nature of the paper as described. The paper focuses on an algorithm recommendation system using active meta-learning methods, specifically in the context of machine learning model selection. From the information provided, the work does not involve the development or release of potentially risky pre-trained models, image generators, or datasets that could be classified as high-risk for misuse or raise ethical concerns related to their release or application. It primarily deals with algorithm performance prediction and recommendation which, in itself, does not directly pose a risk for misuse in the way that releasing a pre-trained model or a scraped dataset might.<br><br>However, to provide constructive feedback and assure comprehensive coverage of potential future adaptations that might involve sensitive data or models:<br><br>1. Clarify potential future risks: Although the current scope of the work does not involve direct misuse risks, it would be beneficial for the authors to briefly discuss any potential risks that could arise if the method was adapted or extended to sensitive domains in future work. This would show foresight and responsibility in research conduct.<br><br>2. Discuss data handling and privacy: While the paper does not present immediate concerns, discussing how data was handled, even if it's publicly available datasets from Statlog, OpenML, and AutoML, reassures the reader of the authors' commitment to data privacy and ethical considerations, especially considering the growing sensitivity around data privacy.<br><br>3. Future guidelines: Should future work based on this research involve more sensitive models or datasets, the authors could outline hypothetical safeguards or guidelines they believe should be in place to mitigate potential risks. This could include controlled access to models, ethical guidelines for use, or mechanisms to prevent misuse.<br><br>4. Engage with the broader ethical implications: Lastly, engaging briefly with the broader ethical implications of automated model selection and its potential impact on various domains could provide valuable insights. While not directly related to misuse, considering the ethical dimension of automated decision-making processes adds depth to the discussion and acknowledges the complexity of technology's role in society.<br><br>These suggestions aim to enhance the paper's exploration of the ethical considerations surrounding the research, ensuring that readers and future researchers are aware of the responsibilities involved in developing and applying machine learning technologies.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: the citation to the used algorithm ‘CofiRank' has been removed.</span><br><span style='color:#0000FF;'>Review: Justification: The authors' answer contradicts the paper content and the provided justification. The paper explicitly mentions CofiRank, indicating its usage within their methodology. Moreover, references section [10], associated with the CofiRank algorithm, suggests that the original work related to CofiRank has been acknowledged. However, there is no clear mention of the licensing terms or explicit credit to the creators or original owners beyond the reference. For an exhaustive review concerning the respect of licensing and terms of use, more details are needed than provided in the justification.<br><br>Constructive and Actionable Suggestions:<br>1. Explicit Credit and Licensing Information: Although CofiRank is mentioned and referenced, the paper should explicitly state the licensing terms under which CofiRank is used if available. If CofiRank is under a specific license that requires attribution or imposes usage restrictions, these details should be clearly mentioned to comply with legal and ethical standards.<br><br>2. Version and URL: To enhance reproducibility and credibility, it's recommended to specify the version of CofiRank utilized in the study. If CofiRank is available online, providing a URL to where the algorithm or its implementation can be accessed would be beneficial for readers interested in following up on this work.<br><br>3. Comprehensive Acknowledgment: For any other code, dataset, or model utilized in the paper, ensure that the original creators are properly credited. If these assets are accompanied by a license, this should be mentioned explicitly. This practice respects the intellectual property rights of others and promotes transparency and reproducibility.<br><br>4. Guidance for Derived Works: If the authors have modified or expanded upon existing assets (like adapting CofiRank for their active meta-learning approach), they should clarify the nature of these modifications and discuss the licensing implications of the derived work.<br><br>5. Contacting Original Creators: In cases where licensing information is not clear or publicly available, the authors should attempt to contact the original creators to ensure that their terms of use are being respected. Documentation of such efforts could further strengthen the paper's adherence to ethical guidelines.<br><br>By addressing these aspects, the authors can better align their paper with the best practices regarding the use of existing assets, which is crucial for both ethical and legal considerations in academic publishing.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: This is given in the github repo, whose link is given in the paper.</span><br><span style='color:#0000FF;'>Review: Justification for Score:<br>The provided answer and justification are somewhat sparse and lack detail regarding the documentation and quality of the assets introduced in the paper. While the mention of a GitHub repository suggests that assets such as code, datasets, or models are indeed shared, it does not specifically address the evaluation criteria outlined in the guidelines. Specifically, confirmation is needed on whether the documentation in the GitHub repository includes structured templates detailing training, license, limitations, etc., and whether consent issues are discussed. Furthermore, the statement does not provide information on the anonymization of the assets, which is an essential aspect as per the guidelines.<br><br>Constructive and Actionable Suggestions:<br>1. Detail Documentation: The authors should provide a more detailed account of the documentation accompanying the new assets introduced in the paper. Specifically, it would be beneficial if the review explicitly stated whether the GitHub repository includes structured templates with essential details such as the methodology behind the assets, training procedures, the license under which the assets are released, any limitations or caveats users should be aware of, and how these assets should be cited.<br><br>2. Address Consent and Anonymization: The authors should clarify whether and how consent was obtained for any data used in their work, especially if personal or sensitive information is involved. Additionally, they should ensure that the assets provided for review are properly anonymized in line with the guidelines, and describe the steps taken to achieve this.<br><br>3. Expand on the Utility and Limitations: While sharing assets is commendable, a more thorough discussion in the paper regarding the potential utility of these assets to other researchers, as well as any limitations or challenges in using them, would be valuable. This could help future researchers understand how they might use or build upon the provided assets in their own work.<br><br>4. Provide Clear Links and Instructions: Ensure that the GitHub repository link is easy to find in the paper and that the repository itself is well-organized with clear instructions on how to use the assets. A README file detailing setup, requirements, and a quickstart guide would enhance the utility of the repository.<br><br>5. License Information: It's crucial to include clear license information in the GitHub repository to inform users about how they can legally use the assets. If not already done, adding a LICENSE file to the repository that specifies the terms under which the assets are made available would address this point.<br><br>By addressing these suggestions, the authors would significantly improve the transparency and reproducibility of their work, aligning with the best practices for sharing new assets in the research community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper under review. The paper titled "ACTIVMETAL: Algorithm Recommendation with Active Meta Learning" focuses on an active meta-learning approach for algorithm recommendation, adopting techniques from collaborative filtering recommender systems and does not involve human subjects or crowdsourcing experiments. The methodology primarily involves computational models, benchmark datasets, and algorithm performance evaluation without any mention of human participation in the experiment setup.<br><br>Justification:<br>- The content of the paper revolves around automated model selection, meta-learning algorithms, and their comparison on benchmark datasets. There is no section or mention of crowdsourcing experiments or involvement of human subjects where instructions, screenshots, or compensation details would be required.<br>- Since the paper's contributions and experimental evaluation strictly pertain to algorithmic development and performance analysis on datasets, the requirements for detailing human subject research protocols do not apply.<br><br>Constructive and actionable suggestions are not applicable in this context as the answer and justification provided align with the paper content and the nature of the research conducted.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This paper does not involve crowdsourcing nor research with human subjects.</span><br><span style='color:#0000FF;'>Review: The provided answer "NA" is consistent with the content of the paper, as there is no mention of crowdsourcing, human subject research, or any activities that would involve human participants directly. The main focus of the study is on developing and testing a model for algorithm recommendation through active meta-learning, which involves computational experiments rather than interaction with human subjects. Therefore, the considerations for potential risks to study participants, disclosures to subjects, and Institutional Review Board (IRB) approvals do not apply to the design and scope of this research.<br><br>Given the paper's focus on algorithmic development and evaluation using datasets from online platforms and earlier research without involving human subjects directly, the absence of IRB-related discussions is justified. The research falls outside the purview of IRB oversight, which is primarily concerned with studies that directly involve human subjects in a manner that could impact their rights, well-being, or privacy.<br><br>The response to the checklist question aligns with the provided guidelines, recognizing when IRB approval is not applicable due to the nature of the research. This approach adheres to the NeurIPS Code of Ethics, which expects authors to accurately represent the ethical considerations relevant to their research.<br><br>Constructive and actionable suggestions for the authors (despite the answer being appropriate):<br>1. To avoid any potential confusion or concerns during the review process or after publication, consider adding a brief statement in the methodology or ethics considerations section of the paper explicitly stating that the study did not involve human subjects or animal research, thus IRB approval was not applicable.<br>2. Ensure that any datasets used, especially those from online platforms or previous studies, were collected and shared in compliance with applicable privacy and ethical guidelines, and mention this compliance in the paper.<br><br>These suggestions are aimed at reinforcing the ethical considerations of the study and providing clarity to readers and reviewers, but they do not indicate any inconsistencies in the answer given by the authors.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h3> Post Submission Survey</h3><p><a href='https://docs.google.com/forms/d/e/1FAIpQLSfRIDkcXFbsOrR09j4qA1MlG4Rfir2lPD_u9YC4eqKBJ8tHkw/viewform?usp=pp_url&entry.463237339=QUNUSVZNRVRBTDpBbGdvcml0aG0gUmVjb21tZW5kYXRpb253aXRoIEFjdGl2ZSBNZXRhIExlYXJuaW5n'>Click here to submit a post submission survey</a></p><br><br><br><br><br>