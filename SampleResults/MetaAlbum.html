<h2>Meta-Album: Multi-domain Meta-Dataset forFew-Shot Image Classification</h2><br>--------------------------------------<br>[+] Correctness Rate (genuine): 0.73<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Rate (adversarial): 0.27<br>--------------------------------------<br>--------------------------------------<br>[+] Correctness Rate (truth_adversarial): 0.33<br>--------------------------------------<br>--------------------------------------<br>[+] Resiliance Score: 0.27<br>--------------------------------------<br>--------------------------------------<br>[+] Combined Score: 0.05<br>--------------------------------------<br><h1><strong>genuine</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The abstract and introduction clearly state the main contributions and scope of the paper</span><br><span style='color:#0000FF;'>Review: The answer given by the authors reflects a high level of consistency with the content of the paper. The abstract and introduction of the paper "Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification" precisely outline its main contributions, which include the introduction of Meta-Album, an image classification meta-dataset designed for few-shot learning, transfer learning, and meta-learning. These sections also detail the structure and scope of Meta-Album, including its diversity, the number of datasets, and the domain coverage.<br><br>However, in line with the guidelines, it's important to ensure that all claims made in the abstract and introduction, especially regarding the paper's contributions, are supported by theoretical and experimental results within the paper. While the answer acknowledges these aspects correctly, a more thorough review of the paper would assess whether its experimental results robustly support these claims and to what extent these findings generalize to other settings outside the experimented conditions.<br><br>Constructive and actionable suggestions for the authors may include:<br>1. Providing a more detailed discussion on the limitations and assumptions of the study to ensure readers are fully aware of the contexts in which the findings are most applicable.<br>2. If not already present, include comparative analysis or benchmarks against existing meta-datasets or few-shot learning approaches to highlight the distinct contributions of Meta-Album.<br>3. Consider expanding on how Meta-Album could facilitate future research in the field, potentially including case studies or scenarios demonstrating its application beyond the settings tested in the paper.<br>4. Ensure that the discussion section critically evaluates the generalizability of the findings and explicitly states any domains or settings where the applicability of Meta-Album might be limited.<br><br>Overall, the answer provided by the authors effectively communicates the core contributions and scope of the paper, as per the requirements stated in the question's guidelines.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have clearly explained the limitations of our work in Section: Discussion and conclusion</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. In the "Discussion and conclusion" section, the paper indeed discusses several limitations of the work performed. These include biases introduced by the data and preprocessing, the variety and quality of datasets, and the potential for artifacts introduced during image preprocessing. The authors also mention the need for further work to address these limitations, such as ensuring tasks consist of related concepts and introducing more challenging "domain-independent" settings.<br><br>However, for a more comprehensive review and to ensure transparency and integrity within the research community, it would be beneficial for future iterations of the work to elaborate further on other potential limitations not explicitly discussed. For instance, addressing computational efficiency, scalability with dataset size, generalization across different domains, and the implications of the assumptions made during the study.<br><br>Constructive Suggestions:<br>1. Explicit Section on Limitations: Consider adding a dedicated subsection or section to discuss limitations more explicitly. This can help readers quickly understand the scope and potential constraints of the proposed method and dataset.<br>2. Computational Efficiency and Scalability: Provide details on the computational resources required for the experiments, the algorithm's scalability, and whether it can handle larger datasets or more complex setups.<br>3. Generalizability and Robustness: Address how well the proposed method and dataset generalize across different domains not included in the study. Discuss the robustness of the algorithm to variations in data quality, size, and distribution.<br>4. Assumptions and their Implications: Detail any assumptions made during the development of the dataset and the algorithm. Discuss how these assumptions might affect the application of the dataset and algorithm in real-world scenarios.<br>5. Enhanced Transparency: Encourage future work to continue being transparent about challenges and shortcomings. This fosters a culture of openness in research and can lead to more collaborative efforts to overcome these limitations.<br>6. Addressing Privacy and Fairness: If applicable, discuss how the dataset and algorithm uphold privacy and fairness principles, especially in sensitive applications. Suggest potential mitigation strategies for any identified risks.<br><br>These suggestions aim to strengthen the paper's impact and help guide future research in addressing these limitations systematically.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: nan</span><br><span style='color:#008000;'>Justification: This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset</span><br><span style='color:#0000FF;'>Review: The authors' response of N/A (Not Applicable) to the question about theoretical results, assumptions, and the completeness of proofs is consistent with the paper's content and context. As per the detailed content provided in the review of "Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification", the research primarily revolves around the development of a meta-dataset designed to facilitate various machine learning tasks such as few-shot learning, transfer learning, and meta-learning. The work focuses on data collection, preprocessing, dataset formatting, and experimental evaluation of the utility of the Meta-Album for few-shot learning problems. There isn't any theoretical derivation or model that would require the provision of assumptions or mathematical proofs.<br><br>The guidelines for answering the question do underscore that the answer "N/A" should be selected when the paper does not include theoretical results, which aligns with the authors' response. Therefore, the justification provided by the authors is correct given that their work does not delve into theoretical findings but rather into dataset creation and empirical testing.<br><br>However, for the sake of completeness and to facilitate a rigorous review, here are some constructive and actionable suggestions:<br>1. Clearly State the Nature of the Work: Although not directly related to theoretical analysis, stating early in the paper that the research focuses on empirical work and dataset creation can help set clear expectations for readers and reviewers.<br>2. Highlight Contributions: While the paper does not include theoretical results, clearly outlining its empirical contributions and the significance of the Meta-Album for the few-shot learning and meta-learning community can yield a profound impact. Consider detailing how this work fills current gaps in existing datasets and benchmarks.<br>3. Discuss Implications and Future Work: It would be useful to include a section discussing the broader implications of the dataset, potential theoretical frameworks that could leverage the dataset for developing new algorithms, and any limitations that could guide future research directions.<br><br>In summary, the authors' response is accurate and aligned with the nature of their research. Nonetheless, incorporating the above suggestions could strengthen the paper by offering a clearer depiction of its scope, contributions, and potential impact on both empirical and theoretical research frontiers in machine learning.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have publicly provided the code on our github repository and details about how to access the datasets on our website.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for the full disclosure of all information needed to reproduce the main experimental results is insufficient to meet the standards required for a thorough and complete experimental reproducibility assessment. Merely stating that the code is provided on a GitHub repository and that details about accessing the datasets are available on their website falls short of providing a complete overview of reproducibility. There are several key aspects that the authors have not addressed in their justification, which are critical for assessing the reproducibility of the paper's main experimental results:<br><br>1. Exact Commands and Environment: The authors have not mentioned whether the provided code includes exact command lines and the software environment (including versions of the software and libraries) needed to reproduce the results. This information is crucial for ensuring that others can run the experiments under the same conditions.<br><br>2. Data Access and Preparation Instructions: Although the authors mention that details about accessing the datasets are available on their website, there is no explicit mention of whether they have provided instructions for data preparation, including preprocessing steps, data splits used for training/validation/testing, and any data augmentation techniques applied.<br><br>3. Reproducibility of Baselines and New Methods: The authors have not specified whether the provided scripts allow for the reproduction of results not only for the proposed method but also for the baseline methods against which their method was compared. This information is essential to validate the claims made in the paper regarding the superiority or effectiveness of the proposed method.<br><br>4. Anonymization: Given that the review is for a conference submission, the authors should ensure that any provided code or data links are appropriately anonymized to conform to the double-blind review process.<br><br>To address these concerns and improve the reproducibility aspect of the paper, I would suggest the following actionable items:<br><br>1. Provide Detailed Instructions: Include a section in either the supplementary material or the GitHub repository README that details the software environment, exact command lines for running the experiments, and any specific settings or configurations needed.<br><br>2. Clarify Data Preparation: Elaborate on how to access, preprocess, and use the datasets for reproducing the experiments. If there are any scripts used for data preparation, make them available and well-documented.<br><br>3. Include Baseline Reproducibility: Ensure that the code repository includes scripts to reproduce the baseline results as well as the results obtained with the proposed method. Clearly mark these scripts and provide instructions on how to execute them.<br><br>4. Ensure Anonymization: If not already done, anonymize any links to code or data in the submission phase to adhere to the conference's double-blind review policy.<br><br>By incorporating these suggestions, the authors can significantly improve the reproducibility of their experimental results and thereby strengthen the paper's contribution to the field.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have publicly provided the code on our github repository and details about how to access the datasets on our website.</span><br><span style='color:#0000FF;'>Review: The answer and justification provided by the paper are consistent with the content and guidelines provided. The paper mentions the provision of code on their GitHub repository and details about accessing datasets on their website, which aligns with the guidelines requiring open access to data and code with sufficient instructions to faithfully reproduce the main experimental results. Additionally, the comprehensive information provided in the paper about Meta-Album, including links to the GitHub repo, competition details, and contact information, supports the claim of open access and reproducibility.<br><br>However, to further enhance reproducibility and adhere more closely to the guidelines, the authors could consider the following actionable suggestions:<br>1. Ensure that the GitHub repository contains clear and detailed instructions on setting up the environment needed for running the provided code, including dependencies versioning.<br>2. Include exact commands that were used to produce the results discussed in the paper within the repository's README file or as a separate documentation/script file.<br>3. Detail the process of accessing, downloading, and preparing the datasets for use with the provided code, to facilitate ease of replication for external researchers.<br>4. For completeness, explicitly state in the documentation any experiments or results mentioned in the paper that are not directly reproducible with the provided scripts, along with reasons.<br>5. Consider providing a Dockerfile or a similar container setup to encapsulate the computational environment, ensuring that the code runs consistently across different platforms.<br><br>By implementing these suggestions, the authors would strengthen the reproducibility aspect of their work, making it easier for other researchers to validate and build upon their findings.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have explained the experimental settings or details in Section 3.2: Experiments. Additional details can be found in the Meta-Album GitHub repository.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors seems to lack specific details about the experimental setup directly in the paper. While mentioning that experimental settings or details can be found in Section 3.2 and the Meta-Album GitHub repository, the given justification does not confirm that all necessary training and test details (such as data splits, hyperparameters, how they were chosen, type of optimizer, etc.) are sufficiently specified within the main content of the paper itself. Relying on external sources like GitHub for key experimental details may make it difficult for readers to fully understand the results and methodology without consulting additional resources.<br><br>Constructive and Actionable Suggestions:<br>1. Include a more detailed description of the experimental setup directly in the paper, such as:<br>- The specific data splits used for training, validation, and testing.<br>- Detailed information on hyperparameters used and the methodology for selecting them.<br>- The type of optimizer utilized and any relevant settings or configurations.<br><br>2. If space constraints limit the amount of detail that can be included in the main paper, consider adding an appendix or supplemental material section that contains these details. Ensure this material is readily accessible and referenced within the main paper.<br><br>3. Clarify whether results shared are averaged over multiple runs or a description of the statistical methods used to ensure the robustness of the experimental results.<br><br>4. If the GitHub repository is to be used as a supplementary source for details, ensure it is well-organized and contains clear documentation of the experimental procedures, hyperparameters, and code for reproducibility. Mention specific folders or files in the paper where readers can find relevant information.<br><br>By incorporating these suggestions, the paper would provide a more comprehensive and self-contained explanation of the experimental settings and methodology, thereby facilitating a better understanding and reproducibility of the results.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars</span><br><span style='color:#0000FF;'>Review: The authors appropriately report error bars in the figures and appendices mentioned, adhering to the guidelines provided. They include error bars at several points throughout the paper, demonstrating a correct understanding of the statistical significance of their experiments.<br><br>However, while the justification claims that error bars are reported appropriately, for a fully comprehensive review, it would be beneficial if the paper also explicitly stated the type of error bars used (e.g., standard deviation or standard error of the mean) and the methodology for their calculation. Including this information directly in the manuscript can enhance the clarity and reliability of the statistical analysis presented.<br><br>Constructive suggestions for improvement:<br>1. Provide explicit details in the text on the type of error bars used (e.g., standard deviation, standard error) and the method employed to calculate these error bars. If different parts of the paper use different types of error bars or methods for their calculation, these distinctions should be clearly stated.<br>2. If applicable, mention the assumptions made for the calculation of these error bars, such as the assumption of normality. If the distribution is known to be asymmetric, consider using confidence intervals that reflect this asymmetry.<br>3. Reference the figures and appendices that contain error bars directly in the sections where statistical significance is discussed. This will help readers quickly find the relevant data supporting the paper’s claims.<br>4. Consider expanding the discussion in the paper to include the implications of the observed error bars on the reliability and generality of the presented results. This can provide readers with a deeper understanding of the robustness of the findings.<br><br>Overall, the answer and justification align with the paper content, showing that statistical significance considerations have been appropriately addressed. Yet, there's room for enhancing the detail and clarity about statistical methods and assumptions, as outlined in the suggestions above.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning</span><br><span style='color:#0000FF;'>Review: Justification: While the paper does mention details about the compute resources used in specific sections, the provided information remains somewhat vague and does not fully adhere to the guidelines for disclosing computer resources. Specifically, the descriptions in the mentioned sections lack detail on the type of compute workers (CPU or GPU specifics), internal cluster or cloud provider specifics, relevant memory and storage, and the overall compute required for each experimental run. Additionally, there is no mention of the estimate of the total compute or disclosures about the full research project's compute requirements beyond the experiments reported.<br><br>Constructive and Actionable Suggestions:<br>1. Specify Compute Worker Details: Clearly state the type of compute workers used (e.g., GPU models such as NVIDIA V100 or PNY GeForce RTX 2080TI) and whether these were part of an internal cluster or accessed via a cloud provider. Include specifics about the number of GPUs used if applicable.<br><br>2. Memory and Storage Information: Provide details on the memory available for the compute workers used in the experiments, as well as any relevant storage details that would impact reproducing the results.<br><br>3. Detailed Compute Requirements: For each experimental set-up mentioned in Section 3.2, Appendix E, and Appendix F, delineate the amount of compute required. This should include execution times and whether experiments were performed on CPUs or GPUs. If both were used, specify what roles each played in the computational process.<br><br>4. Estimate Total Compute: Offer an estimation of the total compute involved in completing all experiments mentioned in the paper, including any preparatory or auxiliary computations not detailed in the main experimental sections.<br><br>5. Disclosure of Full Research Project Compute Requirements: If applicable, disclose whether the experiments presented in the paper constitute the entirety of the computational effort involved in the research or if there were additional computations (e.g., preliminary experiments, failed experiments) that are not included in the paper. This will provide a fuller picture of the computational resource demands of reproducing the research outcomes.<br><br>By addressing these suggestions, the paper would provide a more comprehensive and detailed account of the computer resources required to reproduce the experiments, aligning more closely with the guidelines provided.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We completely comply with the NeurIPS Code of Ethics</span><br><span style='color:#0000FF;'>Review: Based on the content provided in the review of the paper "Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification," the answer given by the authors, stating their full compliance with the NeurIPS Code of Ethics, appears to align with the overall presentation and content of the paper. The authors have not indicated any specific ethical concerns or deviations from standard ethical considerations in the creation, presentation, or intention of their work. The creation of a meta-dataset for few-shot learning, as described, does not inherently raise ethical issues that would contravene NeurIPS's Code of Ethics.<br><br>However, it is worth noting as a constructive and actionable suggestion that for comprehensive transparency and adherence to ethical guidelines:<br>- The authors could briefly discuss any ethical considerations regarding the datasets included in the Meta-Album, especially concerning data privacy, consent where applicable (especially for datasets including or derived from human subjects), and the potential for misuse of the meta-dataset.<br>- They should ensure transparency about the sources of their datasets, licenses, and any modifications made to original datasets to align with ethical data use.<br>- It might be beneficial to include a section or a statement regarding potential biases in the datasets and mitigation strategies to prevent harmful consequences in applications developed using the Meta-Album.<br><br>These considerations would strengthen the assurance that the research indeed complies fully with ethical guidelines, reflecting a proactive stance on ethical concerns in AI and machine learning research.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: There are no negative societal impacts. Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning. We have added “recommended use” in Section 1.3.</span><br><span style='color:#0000FF;'>Review: Justification:<br>The answer provided by the authors suggests a lack of critical engagement with the potential societal impacts of their work. While the authors assert that "there are no negative societal impacts" and highlight only the potential positive impacts of the Meta-Album meta-dataset, this assessment might be overly optimistic and not fully reflective of the complex reality of deploying new technologies and datasets in machine learning research. It is essential for researchers to consider both the intended and unintended consequences of their work, including potential misuse or ethical concerns that may not be immediately apparent.<br><br>Constructive Suggestions:<br>1. Broaden Impact Discussion: Encourage a more thorough evaluation of potential societal impacts, both positive and negative. This could include considering how the technology might be used in ways not intended by the creators, or how it might exacerbate or mitigate existing biases in machine learning models.<br><br>2. Consider Data Privacy and Consent: As the dataset includes images from diverse sources, assess whether there are privacy considerations or concerns around informed consent for the use of these images, especially if any could potentially identify individuals.<br><br>3. Evaluate Bias and Fairness: Explore the dataset's composition for potential biases that could affect model training and performance across different demographic groups. Discuss any measures taken to ensure diversity and representation within the datasets included in Meta-Album.<br><br>4. Address Security Concerns: Discuss any potential security implications of the meta-dataset, such as vulnerabilities to adversarial attacks due to the aggregated nature of data from multiple sources.<br><br>5. Mitigation Strategies: If any potential negative impacts are identified, propose mitigation strategies to address these concerns. This might include recommendations for dataset usage guidelines, ethical considerations for researchers using the dataset, or technical measures to prevent misuse.<br><br>6. Engage with Broader Ethical Principles: Frame the discussion of societal impacts within broader ethical principles guiding AI and machine learning research, such as transparency, accountability, and public benefit.<br><br>By addressing these suggestions, the paper would provide a more balanced and comprehensive view of its societal impacts, contributing to responsible research and innovation in the field of machine learning.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.</span><br><span style='color:#0000FF;'>Review: The author's response and justification align with the expectation to responsibly release data with high potential for misuse, showcasing awareness and measures taken for safeguarding. They highlight two key aspects: licensing and recommended usage guidelines. This indicates a thoughtful approach towards mitigating risks associated with data release, adhering to the ethical considerations outlined in the guidelines.<br><br>Constructive and actionable suggestions for further improvement:<br><br>1. Clarify Licensing Details: While stating that datasets are released with proper licenses is good, specifying the types of licenses (e.g., MIT, Apache 2.0, Creative Commons) for each dataset can provide clearer guidelines on how the data can be used or modified. This can help potential users understand their rights and obligations more precisely.<br><br>2. Expand on Terms of Use: Elaborate on the terms of use by including specific restrictions or conditions under which the datasets can be used. For instance, mention if the data is intended solely for academic research or if commercial use is permitted.<br><br>3. Enhance Recommended Usage Guidelines: Provide more detailed recommended usage guidelines, especially focusing on ethical considerations. This could involve guidelines on avoiding biased or unfair use of the data, respecting privacy rights, and ensuring that the data is used in a manner that does not harm individuals or groups represented in the datasets.<br><br>4. Implement Access Controls: Consider implementing access controls for sensitive or potentially harmful data. This could involve an application process where users must demonstrate their intent to use the data responsibly.<br><br>5. Detail Safeguarding Measures: While licenses and usage recommendations are important, detailing any technical or procedural safeguards put in place to prevent misuse would be beneficial. For example, mention if any data anonymization, filtering, or moderation processes were employed before releasing the datasets.<br><br>6. Encourage Feedback and Reporting Mechanism: Establish a clear mechanism for users to report concerns or misuse of the data. Encourage the community to provide feedback that can lead to further improvements in data usage policies and practices.<br><br>These suggestions aim to enhance the paper's alignment with the provided guidelines, ensuring that the datasets are used responsibly and ethically, minimizing risks of misuse.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets</span><br><span style='color:#0000FF;'>Review: The answer provided by the paper's authors aligns with the expectations outlined in the guidelines regarding proper credit to asset creators and respect for license and terms of use. However, tracing back to the detailed content in the section on "License information of Meta-Album datasets" in the paper, it is clear that the authors have taken considerable effort to detail original sources, specific license information for every dataset utilized (both in the main content and in Appendix B), and the procedures followed which respect the original terms and conditions.<br><br>Moreover, the authors emphasize their commitment to extending the Meta-Album with contributions from the community under strict quality control and review processes, again respecting and acknowledging ownership and licensing. The provision of a dedicated section on how new datasets can be prepared, formatted, and checked for quality—a practice that involves dealing with licensing and proper accreditation—further validates the answer.<br><br>Constructive Suggestions:<br>1. Ensure that all URLs provided for accessing the datasets or more information about them are active and lead directly to the relevant pages. Dead links or redirects can hinder the transparency and reproducibility of accessing licensed assets.<br>2. For future updates or versions of the dataset, maintain a log or version control documentation that clearly records any changes, including additions or retractions of datasets due to licensing issues. This ensures ongoing transparency and respect for licensing agreements.<br>3. When discussing the process for adding new datasets or domains by the community, provide a clear step-by-step guideline or checklist that emphasizes the importance of verifying and adhering to licensing requirements before submission.<br>4. Encourage the use of a standard format or template for citing and acknowledging assets within the project/documentation to streamline the process for contributors and maintain consistency.<br>5. Regularly review and update the licensing information and acknowledgments section to reflect any changes in the original assets’ terms of use that may occur over time.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices). We have also designed a website for users to easily get all the datasets and related information</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper, as they have indeed documented their datasets thoroughly using meta-data files and datasheets included in the appendices. Furthermore, they have developed a dedicated website that facilitates easy access to datasets and related information. This approach aligns with the guidelines for documenting new assets introduced in a paper, ensuring that details about the dataset/code/model, including training, license, limitations, etc., are communicated effectively as part of their submission.<br><br>However, while the documentation of the datasets seems comprehensive, the authors could further strengthen their justification by specifically addressing elements such as the license information for each dataset, how consent was obtained (if applicable), and any limitations of the datasets that users should be aware of. Including direct pointers to the sections of the supplementary materials or the website where these details can be found would also improve the clarity and usefulness of the response.<br><br>Constructive and actionable suggestions for the authors:<br><br>1. Clearly specify the type of license for each dataset in the main text or the website and ensure that license information is easily accessible to users.<br>2. If relevant, discuss how consent was obtained for any data that involves individuals or proprietary information.<br>3. Highlight any known limitations or biases within the datasets to guide potential users in their research.<br>4. Provide direct links or references to where in the appendices or on the website users can find detailed documentation, including training details, consent information, and license specifics.<br>5. Consider including a section on the website or in the documentation that addresses frequently asked questions (FAQs) about the datasets, including usage scenarios, compatibility with different machine learning frameworks, and tips for achieving optimal performance.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: nan</span><br><span style='color:#008000;'>Justification: This does not apply on our research as we are not doing any crowdsourcing experiments</span><br><span style='color:#0000FF;'>Review: The answer and justification provided by the authors are consistent with the content of the paper. From the content outlined, the paper focuses on introducing Meta-Album, a multi-domain meta-dataset designed for few-shot learning and related tasks in computer vision. It involves handling and processing existing datasets for image classification and does not mention conducting new experiments with human subjects or crowdsourcing. Thus, the N/A (Not Applicable) response to the question regarding the involvement of human subjects or crowdsourcing is accurate and aligned with the paper's scope.<br><br>Constructive and actionable suggestions for improvement or clarification are not necessary in this context, as the authors correctly identified the nature of their research and its relevance to the specific question about human subjects or crowdsourcing experiments.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: nan</span><br><span style='color:#008000;'>Justification: This does not concerns us as we are not dealing with human subjects in our research.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content and context of their paper. The paper introduces "Meta-Album," a meta-dataset for facilitating research in few-shot learning, meta-learning, and transfer learning in the domain of image classification. It does not involve human subjects, crowdsourcing, or any form of research that entails direct interaction with individuals or could incur risks to study participants. Therefore, the concerns about disclosing risks to study participants and obtaining Institutional Review Board (IRB) approvals do not apply to this study.<br><br>However, for the sake of thoroughness and to ensure the authors have considered all aspects related to ethics and potential indirect impacts on individuals, the following constructive suggestions are provided:<br><br>1. Clarification: Although not directly involving human subjects, it might be helpful to explicitly mention if any part of the dataset compilation, such as image sourcing or labeling, indirectly involved human effort and if any ethical considerations were taken into account in those processes.<br><br>2. Future Implications: Discuss any potential future uses of the proposed dataset or technology that could raise ethical concerns, especially if the work could be extended or applied in areas involving human subjects. This could include privacy concerns related to images or biases in AI systems trained on the dataset.<br><br>3. Dataset Content Review: Ensure that the datasets included in Meta-Album do not contain sensitive or potentially harmful content. Given the diverse sources of the datasets, a brief review process or a mention of any existing review processes from the original datasets would reinforce the commitment to ethical research practices.<br><br>4. Diversity and Bias: Address any measures taken to ensure diversity within the datasets and to mitigate any potential biases, which is critical for the fairness and generalizability of machine learning models trained on this meta-dataset.<br><br>These suggestions aim to enhance the submission by considering broader ethical aspects related to the research output and its potential applications, even though the study itself does not directly involve human participants.<br></span><br>Correctness Score: 1<br>--------------------------------------<br><h1><strong>adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We haven't mentioned the main claims in the abstract and introduction</span><br><span style='color:#0000FF;'>Review: The justification provided in the peer review does not seem to align with the comprehensive detail presented in the content shared about the Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification paper. The abstract and introduction sections clearly delineate the main claims and contributions of the paper, contrary to the reviewer's assertion that "We haven't mentioned the main claims in the abstract and introduction."<br><br>The abstract introduces Meta-Album as an extensible multidomain meta-dataset, highlighting its purpose to facilitate research across various domains including few-shot learning, transfer learning, and meta-learning. It mentions the inclusion of 40 image classification datasets from diverse domains, the preparation and preprocessing steps to ensure uniformity, and the intention for Meta-Album to serve as a rolling benchmark through ongoing competitions. Furthermore, the introduction meticulously backs up the necessity of such a dataset by discussing the limitations of current meta-datasets and how Meta-Album addresses these issues by providing a more diverse and realistically challenging set of data for benchmarking few-shot learning algorithms.<br><br>However, to further improve the submissions and address the reviewer's concerns, the following constructive suggestions are offered:<br><br>1. Clarify and highlight the main claims and contributions more prominently in both the abstract and the introduction. It might be helpful to use bullet points or a separate section at the end of the introduction to summarize these points explicitly.<br><br>2. Ensure that any mention of the claims or contributions is directly supported by the content presented in the paper, including experimental results and theoretical discussions.<br><br>3. If there are assumptions or limitations not explicitly mentioned in the current version of the abstract or introduction, consider adding a brief discussion on these topics to provide a more comprehensive overview of the paper's scope.<br><br>4. Encourage the reviewer to provide specific examples or sections where they felt the claims were not mentioned or discussed adequately. This feedback could be invaluable for revising the paper to meet the expectations more effectively.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have clearly explained the limitations of our work in Section: Discussion and conclusion</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the expected content of a thorough and transparent research paper. Discussing the limitations of the work performed is a critical aspect that adds to the credibility and academic integrity of the publication. By acknowledging the existence of a "Discussion and conclusion" section where limitations are discussed, the authors meet the criterion related to reflecting on the limitations of their work.<br><br>However, for further improvement and adherence to best practices in scientific reporting, the authors are encouraged to ensure that the "Discussion and conclusion" section explicitly addresses the following if not already covered:<br>- A detailed discussion on the assumptions made during the research and potential impacts of these assumptions on the generalizability and applicability of the findings.<br>- Reflection on the scope of the claims, especially considering the variety and number of datasets used, and the replication of experiments to validate findings across different settings.<br>- Analysis of factors influencing the performance of their approach and any condition-specific limitations.<br>- An honest evaluation of the computational efficiency and scalability of the proposed algorithms.<br>- Consideration of broader implications related to privacy, fairness, and ethical aspects of the proposed methods.<br>- Efforts made by the authors to transparently address these aspects can further enhance the value of the paper by informing future work and encouraging a norm of transparency and integrity within the research community.<br><br>These itemized and constructive suggestions aim to guide the authors in providing a comprehensive limitation discussion that not only acknowledges the existing constraints but also offers a roadmap for future research directions.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have detailed all the theoretical proofs in the main paper.</span><br><span style='color:#0000FF;'>Review: The provided justification for this question does not align with the paper's content. The paper, titled "Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification," focuses on introducing a new image classification meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks. It does not explicitly discuss theoretical proofs or mathematical theorems that require detailed assumptions or complete proofs.<br><br>Inconsistencies in the original answer and justification:<br><br>1. The paper's primary contribution is the creation of a meta-dataset, not the derivation or proof of theoretical results related to algorithms or mathematical formulations.<br>2. The response does not specify where in the paper these theoretical proofs can be found, which sections they pertain to, or what types of theoretical results are included.<br>3. The guidelines require that all assumptions should be clearly stated and proofs numbered; the answer does not provide any evidence or reference that these criteria have been met.<br><br>Constructive suggestions for improvement:<br><br>1. Clarify if there are indeed any theoretical results presented in the paper. If there are, it would be helpful to point to the specific section(s) where these results and their proofs are discussed.<br>2. If the paper does involve theoretical results, ensure that all assumptions are explicitly mentioned within the text and that any proofs are either fully provided within the paper or additional material, or at least summarized with the intuition behind them clearly explained.<br>3. If the paper does not include theoretical results, the response to this question should be revised to "NA" per the given guidelines, explaining that the paper's focus is on the creation and utilization of the Meta-Album dataset rather than on presenting new theoretical findings.<br><br>These suggestions aim to align the paper's review with the actual content presented by the authors and ensure clear communication regarding the presence or absence of theoretical results in the paper.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Our work is proprietary and we are not making the code and data available for public</span><br><span style='color:#0000FF;'>Review: Justification: The answer provided by the authors is inconsistent with the detailed content of the paper. The paper extensively discusses the Meta-Album dataset and its use for few-shot learning, including the creation, preprocessing, and handling of the datasets. It also specifies the architecture and the experimental settings used for the baseline and meta-learning methods, making the main experimental results reproducible in principle. The provided GitHub links and extensive appendix offer additional resources for replication. The claim that the work is proprietary and code/data are not available contradicts the spirit and content of the paper.<br><br>Constructive and Actionable Suggestions:<br>1. Clarify the availability of the code and data. Given the detailed description in the paper and supplemental materials, it would be beneficial for the authors to explicitly state how researchers can access the Meta-Album dataset and any implementation code for the experiments. If there are any proprietary constraints, those should be clearly stated, alongside what is available to the public.<br><br>2. Provide explicit statements regarding the reproducibility of the experimental results. If all or parts of the experiments can be reproduced with the provided materials and instructions, this should be clearly indicated.<br><br>3. Update the repository documentation to ensure it includes detailed instructions on how to use the provided code and data to replicate the study's key findings. This includes specifying any dependencies, environmental setup, and the exact commands to run the experiments.<br><br>4. If possible, consider providing a Dockerfile or an environment setup script (e.g., a Conda environment file) that encapsulates all dependencies and software versions used in the experiments. This can significantly lower the barrier to reproduction.<br><br>5. In case there exist genuine proprietary constraints preventing the release of certain parts of the code or data, the authors should explain these limitations and suggest potential workarounds for researchers interested in replicating the study’s findings.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: Our work is proprietary and we are not making the code and data available for public</span><br><span style='color:#0000FF;'>Review: Justification: The provided justification is inconsistent with the content of the paper. The paper explicitly mentions the availability of the source code, data loaders, instructions for contributing datasets or algorithms, and the location where these resources can be accessed (https://meta-album.github.io/). This contradicts the answer stating that the work is proprietary and not available to the public.<br><br>Constructive and actionable suggestions:<br>1. Correct the answer to reflect the actual availability of the code and dataset as described in the paper. An accurate answer should indicate that the paper does provide open access to data and code with sufficient instructions to reproduce the main experimental results.<br>2. Include a brief description in the justification about how users can access the data and the code from the provided URL. Mention that the website contains links to the source code of challenge-winning methods, baseline methods, data loaders, and instructions for adding new datasets or algorithms.<br>3. Specify whether there are any portions of the experimental results or methods that are not entirely reproducible with the provided instructions and scripts, and if so, provide reasons.<br>4. If applicable, recommend improvements to the documentation on the provided website to ensure users can easily reproduce the experiments and understand how to contribute to the Meta-Album project.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have explained the experimental settings or details in Section 3.2: Experiments.</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors is consistent with the content of the paper. In Section 3.2, "Experiments," the paper discusses the experimental settings, including the comparison of various methods across different few-shot learning tasks, the datasets used, and aspects of the challenge design, like the cross-domain setting. However, while the paper does provide an overview of the experiments performed and mentions some settings like the "5-way 5-shot" setting and the algorithms compared, it could include more detailed specifications of the training and test details such as hyperparameters, specific data splits, the type of optimizer used, and how hyperparameters were chosen to allow for reproducibility and a deeper understanding of the results.<br><br>Constructive Suggestions:<br>1. Detailed Hyperparameter Specification: It would enhance the paper if the authors could include a table or an appendix that lists all the hyperparameters used for each method compared in the experiments. This includes learning rates, batch sizes, any regularization techniques, and optimizer types.<br><br>2. Data Split Details: More information about the data splits (e.g., the percentage of data used for training, validation, and testing, and whether data splits were stratified based on class labels) would provide a clearer understanding of how the experimental setup was designed and ensure reproducibility.<br><br>3. Optimizer and Strategy: Specifying the type of optimizer used (e.g., SGD, Adam) along with any scheduling strategies for learning rate adjustments could provide insights into the training process and potentially impact the results achieved.<br><br>4. Hyperparameter Selection Process: A brief discussion or subsection on how hyperparameters were chosen (e.g., through cross-validation, based on prior literature in similar contexts, or experimentation) would be beneficial. This could include any automated methods used for hyperparameter tuning.<br><br>5. Supplementary Material or Code: If additional space constraints prevent the inclusion of these details in the main paper, adding them as supplementary material or providing a link to a repository with the code and configurations used can be a good alternative. This would aid in the reproducibility of the experiments and allow other researchers to build upon the work more easily.<br><br>By addressing these suggestions, the paper could provide a fully comprehensive account of the experimental setup, thus enabling other researchers to replicate the study and understand the nuances that contributed to the results presented.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have reported error bars in Section 3.2: Experiments.</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for including error bars or other statistical significance information is vague and incomplete based on what is expected from the guidelines. While the authors mention that error bars are reported in Section 3.2, there is no detail on how these error bars were calculated, what they represent (e.g., standard deviation, standard error, confidence interval), or the assumptions made for these calculations. Such details are crucial for understanding the reliability and significance of the experimental results presented.<br><br>Constructive and Actionable Suggestions:<br><br>1. Clarity on Error Bars: Specify the type of error bars used in the experiments (e.g., standard deviation, standard error of the mean, or confidence intervals). This clarification is important for readers to accurately interpret the results.<br><br>2. Methodology for Error Calculation: Explain the methodology used for calculating the error bars. If a statistical test was used, specify which test it was and the reasoning behind choosing this particular test. If error bars were calculated using a library function or a formula, please provide details.<br><br>3. Assumptions Made: Clearly disclose any assumptions made during the error calculation process. If normality of errors was assumed to calculate confidence intervals, this must be stated explicitly.<br><br>4. Handling of Asymmetric Distributions: If relevant, discuss how asymmetric distributions were handled in the error bar calculations to avoid misleading representations, such as negative ranges for metrics that cannot be negative.<br><br>5. Reference Error Bars in the Text: Ensure that wherever error bars are presented (plots or tables), they are explicitly discussed in the text, referencing the specific figures or tables where they appear. This discussion should include an interpretation of what the error bars imply about the experimental results.<br><br>6. Variability Factors Capturing: Clearly state the factors causing variability that the error bars are capturing (e.g., variability across different training/testing splits, initialization seeds, or parameter settings).<br><br>By addressing these points, the authors can significantly improve the statistical representation and interpretation of their experiments, enhancing the paper's transparency and rigor.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning</span><br><span style='color:#0000FF;'>Review: Justification:<br>The answer provided by the authors claims that details about compute resources used were given in specified sections and appendices. However, upon review, the information provided in those sections is generic and lacks specificity. For example, while Appendix F mentions running times per technique in different cross-domain settings (Table 17), it fails to specify the type of compute workers (CPU or GPU), specific hardware configurations, or whether an internal cluster or cloud provider was used. Additionally, there is no clear indication of the memory and storage requirements for reproducing the experiments. Moreover, the paper does not provide an estimate of the total compute required for the full research project, including preliminary or failed experiments that were not included in the final paper. This lack of detailed information makes it challenging to accurately assess the compute resources needed to reproduce the experiments.<br><br>Constructive Suggestions:<br>1. Specify the type of compute workers used for the experiments: Indicate whether CPUs or GPUs were utilized and provide details on the specifications (e.g., model, number of cores, etc.).<br>2. Detail the hardware configurations: Include information on memory, storage, and any specific hardware configurations that were critical for the experiments.<br>3. Clarify the computing environment: State whether the experiments were conducted on an internal cluster, cloud provider (e.g., AWS, Google Cloud, Azure), or a combination of both. If cloud resources were used, mention the specific services and configurations.<br>4. Provide detailed compute requirements: For each experimental run, include the amount of compute required (in terms of CPU/GPU hours) and estimate the total compute resources expended for the entire research project, encompassing preliminary studies and failed experiments.<br>5. Make the information easily accessible: Ensure that the details on compute resources are clearly presented in a dedicated section or appendix and are not scattered throughout the paper. This will facilitate readers' understanding and reproducibility of the experiments.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We are not bound to follow the NeurIPS code of ethics</span><br><span style='color:#0000FF;'>Review: The authors' justification for not conforming to the NeurIPS Code of Ethics because they are "not bound to follow" it is inconsistent with standard expectations for submissions to the NeurIPS conference. All research submitted to NeurIPS is expected to adhere to its Code of Ethics, which outlines the principles of lawful, respectful, and collegial conduct. Therefore, the authors' reasoning is not valid and contradicts the submission requirements that aim to ensure all research respects ethical considerations, including the treatment of data, the welfare of subjects, and the integrity of the scientific process.<br><br>Constructive and Actionable Suggestions:<br>1. Review the NeurIPS Code of Ethics: The authors should carefully review the NeurIPS Code of Ethics to understand the expectations regarding ethical considerations in research, such as data privacy, consent for human subjects, and avoiding harm.<br>2. Provide a Detailed Ethical Consideration Section: Include a section detailing how the study complies with ethical standards, mentioning specific aspects such as data anonymization, consent procedures for any human data used, or measures taken to ensure the welfare of participants.<br>3. Justify Deviations: If there are any deviations from these ethical guidelines, clearly explain the reasons, ensuring that such deviations are warranted and ethically justified. If there are specific laws, regulations, or institutional review board requirements that have been followed, mention these explicitly.<br>4. Ensure Anonymity for Review: In adhering to the NeurIPS submission guidelines and ethical review process, ensure that any reference to the authors' own work is done in a way that preserves anonymity.<br>5. Consult with Ethics Boards or Committees: If there's any uncertainty about the ethical implications of your work, seek advice from institutional review boards (IRBs) or ethics committees that specialize in your research area.<br>6. Clarify Data Sources and Permissions: Outline the sources of all datasets used in the research, confirm that all necessary permissions for their use were obtained, and describe measures taken to protect the privacy and rights of individuals represented in the data.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have detailed potential impacts and recommended use in Section 1.3</span><br><span style='color:#0000FF;'>Review: Justification Review:<br>The justification provided by the authors for addressing societal impacts is not consistent with the content in the detailed review of the paper's sections. The mention of Section 1.3 as covering the societal impact does not clearly indicate if both positive and negative societal impacts are discussed. The conference guidelines stress the importance of discussing both potential positive and negative societal impacts thoroughly. It is necessary for the authors to articulate not only the positive outcomes but also any possible negative consequences that can arise from the deployment or misuse of the technology described in their work. This is crucial for a comprehensive understanding of the societal implications of their research.<br><br>Actionable Suggestions:<br>1. Explicitly Highlight Societal Impacts: Dedicate a section or a prominent part of the paper to discussing the societal impacts of the work. Clearly distinguish between potential positive impacts (e.g., advancements in image classification that can aid various domains such as healthcare, environmental monitoring, etc.) and potential negative impacts (e.g., privacy issues, misuse of the technology in creating misinformation, etc.).<br><br>2. Provide Examples: Include specific examples or scenarios that illustrate both positive and negative impacts. This will help readers better understand the practical implications of the work.<br><br>3. Consider Broader Consequences: Broaden the discussion to consider indirect societal impacts. For example, consider the environmental cost of training large models, potential biases in datasets, or how the technology could affect employment in certain sectors.<br><br>4. Discuss Mitigation Strategies: Where negative impacts are identified, the paper should also propose potential mitigation strategies. This can include technical solutions, ethical guidelines for use, or suggestions for policy and regulation.<br><br>5. Engage with Ethical Considerations: Reflect on ethical considerations related to the work, including data privacy, consent where human data is involved, and fairness. This should be integrated into the discussion of societal impacts.<br><br>By addressing these suggestions, the authors can provide a more balanced and thorough discussion of the societal impacts of their research, aligning with the expectations set out in the conference guidelines.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Indeed, we are releasing our datasets with proper license and terms of use. We have also provided recommended usage of these datasets.</span><br><span style='color:#0000FF;'>Review: The response provided by the authors does not fully address the guidelines mentioned for responsible release and data safety measures. While they mention the release of datasets with proper licenses and recommended usage guidelines, the specifics regarding safeguards for misuse or dual-use risks are not detailed.<br><br>Actionable Suggestions:<br>1. Clarify the types of licenses used for each dataset within Meta-Album and how they contribute to responsible use.<br>2. Outline any specific terms of use that address potential misuse, particularly for sensitive image data.<br>3. Provide details on any safeguards or access controls implemented to prevent misuse of the datasets, especially for those that could be sensitive or have dual-use implications.<br>4. Explain measures taken to ensure the datasets do not contain harmful or unsafe content, particularly for those scraped from the internet.<br>5. If applicable, describe how the team plans to monitor the usage of their datasets to ensure adherence to ethical guidelines and terms of use.<br>6. Lastly, for better transparency, it would be beneficial to include a section in the paper that discusses ethical considerations related to the datasets, especially given their diversity and the inclusion of images from various domains.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors correctly addresses the question regarding the proper crediting of the original owners of assets and the explicit mention of licenses and terms of use. The authors affirm that they release their datasets with an appropriate license and detail the original licenses for the datasets utilized in Appendix B of their paper. This approach is consistent with best practices for academic publications that leverage existing assets, whether they be datasets, code, or models.<br><br>However, while the authors' justification seems adequate, it could benefit from further elaboration to ensure complete transparency and compliance with licensing requirements. For example, the authors could provide more specific information about the types of licenses under which their datasets and those from other sources are released. Additionally, clarifications regarding any modifications made to original datasets and the implications for licensing would enhance the justification.<br><br>Constructive and actionable suggestions:<br>1. Explicit License Information: In addition to mentioning that the original licenses are detailed in an appendix, explicitly state the types of licenses (e.g., MIT, CC-BY 4.0) applicable to both the new dataset and the original datasets within the body of the paper or in a clearly visible section.<br>2. Clarification on Modifications: Provide a brief description of any modifications made to original datasets and discuss how these affect the licensing (if at all). This is critical where derivations or transformations of data might impact the reuse or distribution rights.<br>3. URLs and Citations: Ensure that direct citations to the original papers or sources of the datasets are included in the references section. Where possible, provide URLs or DOIs for readers to easily access the original assets and their licensing terms.<br>4. License for New Assets: Clearly specify the license under which the new datasets created as part of this research are released. This information should be easily accessible both within the paper and in the accompanying dataset documentation online.<br>5. Guidelines for Use and Citation: Offer guidelines for researchers who wish to use the presented datasets, including how they should cite the assets and any specific terms of use they need to adhere to.<br><br>By addressing these suggestions, the authors will not only comply with ethical standards and legal requirements but also foster an environment of openness and reproducibility in research.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices).</span><br><span style='color:#0000FF;'>Review: The authors' response to the question of whether new assets introduced in the paper are well documented is consistent with the content presented in the paper. The paper details the introduction of the Meta-Album, an extensive meta-dataset designed for few-shot learning research. This dataset is documented thoroughly, with descriptions of its composition, processing steps, and structure provided within the main text and appendices. Notably, the documentation includes:<br><br>1. Detailed Descriptions in the Main Text and Appendices: The paper describes the sources, preparation, and structuring of the datasets included in the Meta-Album. It discusses the motivation, data sourcing, preprocessing, and quality control measures. Appendices provide additional depth regarding the datasets' origins, processing steps, and structure.<br><br>2. Datasheets for Datasets: As mentioned in the justification, datasheets accompany the datasets, offering detailed information regarding the datasets' characteristics, preparation, limitations, and intended use cases. This aligns with best practices in ensuring new assets are well documented.<br><br>3. Structured Metadata and Accessibility: The paper mentions structured meta-data files and guidelines on how the datasets can be accessed and utilized by researchers, which is crucial for enabling reproducibility and facilitating further research.<br><br>4. Open Source and Accessibility: The provision of GitHub links and a dedicated website for the Meta-Album ensures that the datasets are not only documented but also accessible to the broader research community, further supporting the claim of proper documentation.<br><br>Despite agreeing with the authors' assertion, it's worth noting that the specifics of licensing and consent, particularly concerning individuals' data included in the datasets (if applicable), were not explicitly discussed in the provided content. Future iterations of the work could benefit from explicitly addressing these aspects to adhere fully to ethical guidelines and best practices in asset release.<br><br>Constructive Suggestions:<br>- Ethics and Consent: Include a discussion on the ethical considerations, including how consent was obtained for any personal data used in the datasets. Detail the efforts taken to anonymize sensitive information where applicable.<br>- Licensing Information: Clearly state the licensing terms for each dataset within the Meta-Album. This is crucial for researchers to understand the permissible uses of the data.<br>- Limitations and Biases: Although quality control measures are discussed, explicitly addressing known limitations and potential biases within the datasets would be beneficial. This would guide researchers in appropriately using the datasets.<br>- Examples and Tutorials: Providing examples or tutorials on using the Meta-Album for few-shot learning tasks would enhance the utility of the documentation, helping researchers to quickly leverage the dataset for their own work.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have mentioned all the crowdsourcing details in the main paper</span><br><span style='color:#0000FF;'>Review: Justification: The answer provided by the authors that they have mentioned all the crowdsourcing details in the main paper is too vague and does not specifically address the question's request for the full text of instructions given to participants, screenshots (if applicable), and details about compensation. The paper content does not explicitly include or reference the full text of instructions for participants involved in crowdsourcing experiments or research with human subjects. Additionally, there is no mention or evidence of screenshots or specifics regarding participant compensation, which is critical information to assess the ethical considerations and the quality of participant engagement.<br><br>Constructive and Actionable Suggestions:<br>1. In the revised version of the paper, explicitly include the full text of instructions given to participants in a supplementary section or as an appendix. This information is crucial for replicating studies and understanding the context in which data was collected.<br>2. If applicable, provide screenshots or visual aids that were used during the study or crowdsourcing experiments. These materials can offer valuable insights into the user interface and participant experience.<br>3. Detail the compensation model for participants involved in the study. Offering information on compensation is important for ethical transparency and to align with the NeurIPS Code of Ethics. If participants were unpaid, explain the rationale for this decision.<br>4. Consider adding a section in the paper or supplementary material that addresses these aspects explicitly. This would not only enhance the clarity and completeness of the study's reporting but also strengthen its adherence to ethical guidelines.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have received approvals from IRB and have mentioned in the main paper</span><br><span style='color:#0000FF;'>Review: Justification Review:<br>The provided justification by the authors is insufficient and generic, lacking specific details necessary for evaluating the ethical considerations related to human subjects research. Simply stating that IRB approvals were received without mentioning how potential risks to study participants were mitigated, or whether these risks were disclosed to subjects, does not fulfill the requirement for a comprehensive ethical review. Furthermore, the response does not include information on how the data was anonymized or secured, which is critical to understanding the privacy and confidentiality measures in place for protecting the participants involved in the research.<br><br>Constructive Suggestions:<br>1. Detailed Description of Ethical Considerations: Provide a more detailed section in the paper that covers the ethical considerations of your research. This should include the nature of potential risks to participants, steps taken to minimize these risks, and how participants were informed about these risks.<br><br>2. IRB Approval Details: If IRB approval was obtained, specify the date of approval and the institution that granted it, while maintaining anonymity if necessary due to NeurIPS submission guidelines. If the work did not require IRB approval, explicitly state why this was the case based on the nature of the research and the guidelines of your institution or country.<br><br>3. Participant Consent: Clarify how consent was obtained from participants, especially if the research involved sensitive data. Include whether the consent was informed and how the participants were briefed about the research purposes, their rights, and the confidentiality of their data.<br><br>4. Data Anonymization and Security: Describe the measures taken to ensure the anonymity and security of the participants' data. This should include details on data storage, access controls, and any data anonymization techniques applied.<br><br>5. Address Specificity in Risks and Mitigations: Clearly articulate specific risks associated with your research and provide detailed explanations of how these risks were mitigated or managed throughout the study.<br><br>These suggestions aim to enhance the transparency and ethical consideration of your research involving human subjects, aligning with the expectations of NeurIPS and the broader scientific community.<br></span><br>Correctness Score: 0<br>--------------------------------------<br><h1><strong>truth_adversarial</strong></h1><br>--------------------------------------<br>Question # 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: The abstract and introduction clearly state the main contributions and scope of the paper</span><br><span style='color:#0000FF;'>Review: The response provided is consistent with the paper's content. The abstract and introduction sections of the paper accurately outline the main contributions and the scope of the research, highlighting the development of the Meta-Album, a multi-domain meta-dataset designed for few-shot learning, and related tasks. These sections effectively communicate the intended purpose, the methodology, and the expected impact of the Meta-Album on few-shot learning and meta-learning research, matching the paper's contributions with theoretical motivations and anticipated applications.<br><br>However, for a more thorough review and to ensure constructive feedback:<br><br>1. Clarification of Contributions: While the response correctly states that the claims are accurately reflected, it might benefit from specifying which particular claims and contributions are correctly described. Specifically, it could mention the introduction of a novel multi-domain meta-dataset, the provision of three versions of each dataset to accommodate different computational resources, the inclusion of data from underrepresented domains, or the uniform preprocessing and formatting of the datasets for ease of use.<br><br>2. Scope Limitation: The response could have also suggested a discussion on any limitations or potential biases inherent in the dataset collection and processing methodologies that could affect the generalization of the findings or the applicability of the dataset. Mentioning the diversity of sources for the datasets or the criteria for dataset selection could further clarify the scope and the projected impact of this research contribution.<br><br>3. Expanded Impact Discussion: It would be constructive to recommend that the paper discusses specific examples or scenarios wherein the Meta-Album could significantly advance the fields of few-shot learning and meta-learning. This includes elaborating on how Meta-Album compares with existing benchmarks and datasets and identifying future research directions it opens up.<br><br>In conclusion, the justification provided aligns well with the content and contributions detailed in the paper. However, expanding the critique to cover the points mentioned above would strengthen the review by providing more detailed feedback and suggestions for enhancing the paper's clarity and impact.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 2: Does the paper discuss the limitations of the work performed by the authors?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We haven't clearly explained the limitations of our work in Section: Discussion and conclusion</span><br><span style='color:#0000FF;'>Review: Justification:<br>The response provided by the authors indicates that they have not discussed the limitations of their work clearly within the paper, which contradicts the guidelines provided for including limitations. Discussing limitations is crucial for scientific integrity, transparency, and contributing to the field's knowledge advancement by highlighting areas for further research or indicating the boundaries of the study's applicability.<br><br>Constructive and actionable suggestions:<br>1. Explicitly Address Limitations: Dedicate a subsection within the Discussion or Conclusion section to explicitly address the limitations of your study. This could cover aspects such as assumptions made, the applicability of the results to specific domains or datasets, and any constraints on computational resources.<br><br>2. Assumptions and Robustness: Discuss any strong assumptions made during your research. Reflect on how these assumptions might limit the applicability or robustness of your results in real-world scenarios. Suggest ways in which these assumptions could be tested or relaxed in future work.<br><br>3. Scope of Claims: Ensure that the claims made in the paper accurately reflect the scope of your experiments. If your approach was tested on a limited number of datasets or domains, acknowledge this as a limitation and suggest directions for future research to test the approach more broadly.<br><br>4. Factors Influencing Performance: Identify any factors that significantly influence the performance of your proposed approach. Discuss how changes in these factors could impact the effectiveness or reliability of your results.<br><br>5. Computational Efficiency: Comment on the computational efficiency of your algorithms and their scalability with respect to the size of the dataset or the complexity of the task. If there are significant computational demands, discuss potential ways to optimize or mitigate these demands in future implementations.<br><br>6. Privacy and Fairness Concerns: If applicable, address any potential limitations of your approach concerning privacy and fairness. Discuss any steps taken to minimize these concerns and suggest areas for improvement or further research.<br><br>7. Encouragement of Transparency: Reassure the reader of your commitment to scientific transparency by acknowledging the limitations and the potential impact on the study's findings. Emphasize the importance of such transparency for the advancement of the field.<br><br>By addressing these suggestions, the authors can improve the completeness and integrity of their paper, contributing more effectively to the scientific community's overall knowledge and understanding of the topic.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our work is not theoretical so this does not apply on our research</span><br><span style='color:#0000FF;'>Review: The authors have correctly identified that their paper does not include theoretical results, hence marking this question as not applicable (NA) aligns with the provided guidelines. Their research introduces Meta-Album, a meta-dataset designed to facilitate meta-learning research in the cross-domain few-shot setting, which is more practical and applied in nature rather than theoretical. Given this context, the expectation of providing theorems, formulas, or complete proofs as outlined in the guidelines does not apply to their work.<br><br>However, for the sake of completeness and to ensure a thorough review, the authors could consider the following suggestions to further strengthen their submission:<br><br>1. Clarify Methodological Foundations: Even though the research is not theoretical, briefly discussing the methodological foundations or the rationale behind the design choices in the Meta-Album dataset could provide deeper insights for readers.<br><br>2. Potential for Future Theoretical Work: Considering mentioning if there are any aspects of the dataset or its use cases that could inspire future theoretical investigations. This can encourage other researchers to explore theoretical frameworks or models that could be validated using Meta-Album.<br><br>3. Cross-Reference: While not directly related to theoretical proofs, ensuring that all elements (datasets, experiments, etc.) introduced in the paper are properly numbered and cross-referenced would aid in readability and clarity, adhering to best practices as outlined in the guidelines.<br><br>These suggestions are meant to guide the authors in potentially enhancing the presentation and impact of their work. The answer provided to the checklist question remains satisfactory given the applied nature of their research.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have made our code and data publicly available on our github repository and on our website</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors, while indicating a positive direction towards reproducibility, lacks the necessary detail to fully support their claim. The mere availability of code and data on a GitHub repository and website does not guarantee the reproducibility of the experimental results unless detailed instructions, environment details, and data preparation steps are explicitly mentioned and easy to follow. The justification provided by the authors does not address these critical components that ensure the reproducibility of their results, making it incomplete in the context of the guideline’s requirements for reproducibility.<br><br>Suggestions for Improvement:<br>1. Detailed Environment Setup: Provide a detailed description or a requirements file (e.g., a `requirements.txt` for Python projects) that lists all the libraries, packages, and their specific versions used in the experiments. This will help in creating an identical environment.<br><br>2. Step-By-Step Instructions: Include step-by-step instructions on how to run the experiments, including the exact commands needed to train the models, evaluate them, and reproduce the tables or figures presented in the paper.<br><br>3. Data Access and Preparation: Offer clear guidance on how to access both the raw and preprocessed data. If any preprocessing is required, provide scripts or detailed instructions on how to replicate this preprocessing.<br><br>4. Explanation for Any Omissions: If certain experiments are not covered by the provided scripts or if some part of the code/data cannot be shared, clearly explain these omissions and provide reasons.<br><br>5. Anonymity Preservation: Ensure that all provided resources (code, data, and instructions) adhere to the double-blind review policy by avoiding direct links that could reveal the identity of the authors or affiliations. Consider using a service that allows for anonymous sharing of links if necessary.<br><br>6. Supplemental Material: It is beneficial to consolidate all the above details into a supplementary material document (e.g., a README file in the repository) that is easy to navigate and understand.<br><br>By addressing these areas, the authors can significantly improve the reproducibility of their experimental results and comply with the guidelines provided.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: We have made our code and data publicly available on our github repository and on our website</span><br><span style='color:#0000FF;'>Review: The authors' assertion aligns with the content detailed in the paper, justifying a score of 1 for correctly addressing the question about providing open access to data and code, with sufficient instructions for reproducing the main experimental results.<br><br>Explanation:<br>The paper presents "Meta-Album: Multi-domain Meta-dataset for Few-Shot Image Classification" and emphasizes its commitment to enabling reproducibility and fostering community engagement. The inclusion of links to the Meta-Album website and GitHub repository is central to this commitment, providing access to the dataset, source code, and comprehensive instructions necessary for reproducing the results. This approach not only adheres to the guidelines specified for NeurIPS submissions but also enriches the value of the contribution by ensuring that the research community can leverage, extend, and critically evaluate the presented work.<br><br>Constructive suggestions for further enhancing reproducibility and usability include:<br><br>1. Detailed Documentation: Ensure that the GitHub repository contains exhaustive documentation covering the setup environment, dependencies, and step-by-step instructions for data preparation, training models, and evaluating them as per the experiments described in the paper.<br><br>2. Example Scripts and Notebooks: Offer example scripts or Jupyter notebooks demonstrating the process to reproduce specific experiments or results highlighted in the paper. This can significantly lower the barrier for others to verify findings and explore variations of proposed methods.<br><br>3. Reporting Issues and Contributing: Include guidelines on how researchers can report issues or contribute improvements to the dataset or codebase. A structured form of community feedback and contributions can greatly enhance the longevity and impact of the project.<br><br>4. Version Control: It would be beneficial if the authors maintain versioning for their dataset and code, especially as they welcome contributions for enlargement. This practice ensures that future researchers can access the specific versions of the dataset or codebase used in the original experiments, despite any subsequent updates or expansions.<br><br>By implementing these suggestions, the authors can further solidify the reproducibility of their work and encourage an active participation from the research community in extending and applying the Meta-Album dataset.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We have omitted many experimental details and hyper parameters</span><br><span style='color:#0000FF;'>Review: Explanation: The paper's content does provide extensive details on the preparation of the Meta-Album dataset, including the processing steps, the criteria for dataset selection, preprocessing strategies, and quality control measures. It discusses the challenge design for the NeurIPS 2021 and 2022 MetaDL challenges, including the problem settings for few-shot learning and how the datasets were used in these challenges. Furthermore, it elaborates on the experimental evaluation of the Meta-Album dataset, detailing the use cases, baseline methods, problem settings like "within domain" and "cross-domain" few-shot learning, and even the specific settings for "any-way any-shot" learning. However, while it does mention the use of challenges and baselines for evaluation, it falls short of extensively detailing the training hyperparameters or the specific types of optimizers used in the model development process within the main content of the paper. The answer provided in the review seems to ignore the detailed methodology provided for dataset creation, challenge design, and basic experimental setup and instead overemphasizes the lack of specifics on model training details, which is a narrower interpretation of the guideline question.<br><br>Constructive and Actionable Suggestions:<br>1. Consider including a subsection dedicated to the experimental setup for model evaluation, encompassing the choice of hyperparameters, the types of optimizers used, and any model-specific configurations that were applied. This would address the gap mentioned in the justification.<br>2. If these details are provided in the supplementary materials or code repositories, explicitly reference this within the paper, directing readers to where they can find the complete experimental setup details. This will clarify that all necessary details for understanding and replicating the results are indeed accessible.<br>3. For clarity and completeness, briefly summarize the key training and test details in the main content, even if detailed explanations are provided elsewhere. This approach would ensure that readers gain a comprehensive understanding of the experimental setup directly from the paper.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We haven't reported error bars for our experiments</span><br><span style='color:#0000FF;'>Review: Justification: The justification provided by the authors directly conflicts with the detailed information present in the paper. Throughout the detailed sections of the paper, especially in the various tables and figures (e.g., Table 5, Table 6, Table 7, Table 8, and Figures 11, 12, 13), error bars are explicitly reported, indicating the 95% confidence intervals computed at different levels of the experiments (e.g., per-task level over multiple runs per dataset). These specifics not only demonstrate the presence of statistical measures accompanying the experimental results but also provide clear insights into the variability factors (e.g., different random seeds) and the methods used for calculating the statistical significance (e.g., confidence intervals).<br><br>Constructive and actionable suggestions:<br>1. Revisit the Answer and Justification: The authors should correct the answer to this question since the paper indeed reports error bars alongside the experimental results, contradicting the current justification.<br>2. Clarify Error Bar Definition: Enhance the manuscript by explicitly stating the definition of the error bars (e.g., standard error of the mean vs. standard deviation) and the statistical assumptions made (e.g., normality of errors). Although error bars are reported, explicitly defining their nature and calculation methodology can further improve the paper's clarity.<br>3. Consistency Check: Ensure that all sections of the manuscript consistently report and discuss the inclusion of error bars or statistical significance measures. It seems there might be a disconnect between the provided justification and the actual content of the paper.<br>4. Enhanced Discussion on Statistical Measures: In sections where experiments are discussed, include a brief discussion on the observed error ranges and their potential implications on the reliability and generalizability of the proposed contributions. This adds depth to the overall analysis of the experimental results.<br>5. Reference Methodology for Error Bars: If applicable, include a reference to the methodology or library functions used for calculating error bars. This is particularly important when custom or less common statistical methods are employed.<br><br>These suggestions aim to ensure the manuscript accurately reflects the statistical analyses conducted in the study, thereby enhancing its scientific rigor and transparency.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We haven't provided details about compute resources used</span><br><span style='color:#0000FF;'>Review: Explanation: The provided justification does not match the content of the paper. The paper does indeed provide detailed information regarding the computational resources used for the experiments. In the supplementary material, particularly in sections F "Cross-Domain Few-shot learning: additional results" and G "Benchmarks/Datasets comparison", detailed information regarding the running times for all techniques across various settings is provided. This includes specific details on the meta-train, meta-validation, and meta-test phases, clearly indicating that the authors have disclosed the computational resources involved. Furthermore, Table 17 explicitly lists the running times for different techniques over all cross-domain settings, showing the authors' commitment to transparency regarding computational resources.<br><br>Constructive and actionable suggestions:<br>1. Clarify the main text to explicitly reference the supplementary material for details on computational resources, ensuring readers are aware of where to find this information.<br>2. Consider summarizing key details on computational resources directly in the main body of the paper for easier access, perhaps in a dedicated subsection within the methodology chapter.<br>3. Include additional information about the specific hardware used (type of GPUs/CPUs, memory capacity, etc.) and any software or framework specifics that might influence reproducibility.<br>4. If applicable, include a discussion on the scalability of the proposed methods and experiments, especially in relation to the computational resources required. This could help readers evaluate the feasibility of reproducing or building upon the work in different environments.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?<br><span style='color:#008000;'>Answer: Yes</span><br><span style='color:#008000;'>Justification: Our research is completely ethical and we conform to the NeurIPS code of ethics</span><br><span style='color:#0000FF;'>Review: The justification provided by the authors for the question regarding the conformity of their research with the NeurIPS Code of Ethics is too brief and lacks depth. Simply stating that the research is completely ethical without providing specific details or examples of how the research conforms to the NeurIPS Code of Ethics is insufficient. Furthermore, the justification does not address any specific aspects of the Code of Ethics such as data privacy, consent, potential biases, implications of the research, etc.<br><br>Constructive and actionable suggestions for the authors:<br>1. Provide Detailed Justification: Elaborate on how your research methodologies, data handling, and analysis conform to the NeurIPS Code of Ethics. Specific aspects to consider include data privacy, informed consent (if applicable), and measures taken to ensure the ethical treatment of data.<br>2. Address Potential Biases: Discuss any measures taken to identify and mitigate potential biases in your research, especially since the dataset involves images from diverse domains. Highlighting efforts to ensure fairness and inclusivity in data collection and analysis would strengthen the ethical considerations of your work.<br>3. Consider the Broader Implications: Reflect on and discuss the broader implications of your research on society, considering both positive outcomes and potential risks. Outline any steps taken to minimize negative impacts.<br>4. Review Specific Sections of the Code: Explicitly discuss how your research aligns with specific sections of the NeurIPS Code of Ethics, providing examples or explanations for each applicable section. This will demonstrate a thorough understanding and application of the Code in your research process.<br>5. Add a Section on Ethical Considerations: Consider adding a dedicated section or paragraph in your paper that specifically addresses how your research conforms to the NeurIPS Code of Ethics. This section can detail the considerations and actions taken to ensure ethical compliance throughout your research.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We haven't discussed the limitations in detail</span><br><span style='color:#0000FF;'>Review: Justification:<br>The given answer and justification provided by the authors are inconsistent with the expectations outlined in the guidelines for discussing societal impacts in the NeurIPS Paper Checklist. Simply stating that the limitations have not been discussed in detail is insufficient without explicitly addressing both potential positive and negative societal impacts. The NeurIPS guidelines emphasize considering and discussing the societal impacts of the work, even in foundational research. The response lacks a thorough assessment of how the introduced Meta-Album, designed to facilitate few-shot learning, meta-learning, transfer learning, and other tasks, could potentially impact society both positively and negatively.<br><br>Suggestions:<br>1. Positive Impacts: The authors should elaborate on how the Meta-Album can contribute positively to society. For example, they could discuss its potential to accelerate research in image classification, its applicability across various domains such as healthcare, environmental monitoring, or its role in promoting the development of more efficient and accurate machine learning models that require fewer data to learn, which might democratize access to cutting-edge technology.<br><br>2. Negative Impacts: The authors must consider and discuss potential negative societal impacts of their work. For instance, they could assess if the datasets contain any biases that could perpetuate discrimination or if the technology could be misused in ways that infringe on privacy or facilitate surveillance. The accessibility of a large, diverse, and easy-to-use dataset could also, in theory, accelerate the development of technology for malicious purposes (e.g., improving the realism of deepfakes).<br><br>3. Mitigation Strategies: Alongside identifying negative impacts, the authors should propose mitigation strategies or safeguards. For instance, they could recommend measures for continuous monitoring of the dataset usage, guidelines for ethical use, or developing methods to detect and mitigate biases within datasets.<br><br>By addressing these suggestions, the paper would not only fulfill the NeurIPS guidelines requirements but also contribute to a more responsible and aware AI research community, acknowledging both the potential benefits and risks associated with the advancement of machine learning technologies.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We haven't put any safeguards e.g. recommended use of datasets or license for datasets</span><br><span style='color:#0000FF;'>Review: The response provided by the authors accurately reflects their paper's approach regarding the responsible release of data and addresses the NeurIPS Paper Checklist question on safeguards against misuse. The authors openly acknowledge that specific safeguards, usage guidelines, or licensing restrictions tailored to mitigate the risk of misuse for the datasets or models they are releasing haven't been implemented. This honest acknowledgment aligns with the guidance provided, which emphasizes the importance of considering and, wherever possible, implementing measures to restrict misuse, especially for potentially sensitive or dual-use technologies.<br><br>However, given the nature of the paper—a presentation of a meta-dataset for few-shot learning across diverse domains—the primary risks associated with misuse might be less pronounced compared to, for instance, datasets or models with direct applications in surveillance, deepfakes, or other immediately contentious areas. Nonetheless, even datasets focused on seemingly benign applications can be misused if combined with sensitive personal data or utilized to train models for malicious purposes.<br><br>Constructive Suggestions:<br><br>1. Licensing Framework: It would be beneficial for the authors to consider a more comprehensive licensing framework for their dataset that outlines acceptable uses and explicitly prohibits malicious use. Open-source licenses that include ethical use clauses might provide a starting point, although such clauses can be challenging to enforce.<br><br>2. Ethical Guidelines: Develop and include a set of ethical usage guidelines alongside the dataset. These could outline recommended practices for responsible use and explicitly discourage applications that could infringe on privacy, human rights, or be otherwise deemed unethical.<br><br>3. Accessibility Controls: Evaluate the feasibility of implementing access controls for the dataset, especially if the dataset's scope expands to include potentially sensitive information or applications. Controlled access mechanisms could include requiring users to apply for access, outlining their intended use, and agreeing to ethical use standards.<br><br>4. Engagement with Ethical Review: Engage with an independent ethics review to assess potential risks of misuse associated with the dataset and explore additional safeguards that could be implemented. Insights from such a review could guide revisions to dataset documentation, access policies, and user guidelines.<br><br>5. Community Feedback: Open channels for feedback from the broader research community and users of the dataset. This could help identify unforeseen risks or misuse scenarios and enable a more dynamic approach to implementing safeguards.<br><br>6. Transparency and Reporting: Encourage transparency among users of the dataset regarding their applications and findings. Establishing a reporting mechanism for misuse or ethical concerns related to the dataset could also help the authors and the community address issues more proactively.<br><br>By considering these suggestions, the authors could enhance the responsible use and dissemination of their dataset, contributing positively to the AI and machine learning research landscapes.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We are releasing the datasets without any proper license</span><br><span style='color:#0000FF;'>Review: Justification for the score: The given answer in the review is inconsistent with the content of the paper. The paper explicitly mentions checking licenses of all datasets included in the Meta-Album and ensures that all datasets are free for academic research, providing their original licenses. This indicates that the creators or original owners of the assets used in the paper are properly credited, and the licenses and terms of use are explicitly mentioned and properly respected. Furthermore, the paper includes a dedicated section B titled "License information of Meta-Album datasets," detailing the sources of the datasets and their licenses. Therefore, the correct answer to the question should have been "Yes" with specific references to the sections where license information and proper credits are provided.<br><br>Itemized constructive and actionable suggestions:<br>1. Amend the justification to reflect the actual content of the paper, specifically pointing to the sections where license information and proper credit to the original authors of the datasets are given.<br>2. Reference Section B "License information of Meta-Album datasets" directly to provide evidence that the licenses and terms of use of the datasets are explicitly mentioned and that the paper respects these terms.<br>3. Highlight the paper's compliance with ethical and legal standards through its detailed attention to license information, demonstrating responsible and respectful use of external assets.<br>4. Encourage future submissions to maintain or improve the standard set by this paper in terms of explicitly mentioning and respecting the licenses and terms of use of any external assets utilized.<br><br>These suggestions aim to correct the oversight in the review and acknowledge the efforts made by the authors in ensuring proper credit and adherence to licensing conditions for the datasets used in their research.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?<br><span style='color:#008000;'>Answer: No</span><br><span style='color:#008000;'>Justification: We haven't provided proper documentation fo our new datasets in the paper</span><br><span style='color:#0000FF;'>Review: Explanation: The paper explicitly provides extensive documentation for the newly introduced Meta-Album meta-dataset. This is evident through the detailed description of the dataset's composition, including the number of domains, datasets, images, class hierarchies, preprocessing steps, and quality control processes. Additionally, the paper mentions the availability of the source code, baseline methods, data loaders, and instructions for contributing new datasets or algorithms on the Meta-Album website. This level of detail indicates a well-documented new asset, contrary to the justification provided in the original answer.<br><br>Suggestions:<br>1. In future iterations, it might be beneficial to explicitly mention the specific sections or appendices where the documentation can be found to provide direct guidance to the reader.<br>2. Including direct links to the repositories or web pages where the assets and their documentation are hosted could improve accessibility.<br>3. Discussing the license under which the Meta-Album meta-dataset is released would be beneficial for potential users to understand the permissions and restrictions associated with its use.<br>4. A brief discussion on consent obtained for using the datasets and how privacy concerns, if any, are addressed could enhance the understanding of the ethical considerations taken.<br>5. Highlighting any limitations of the current dataset and potential areas for expansion or improvement could guide future contributions and research in building upon the Meta-Album meta-dataset.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>--------------------------------------<br>Question # 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: Our work is not related to any crowdsourcing experiments.</span><br><span style='color:#0000FF;'>Review: The authors' answer is consistent with the content and scope of the paper. After a detailed review, it is evident that the paper introduces Meta-Album, a meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks, and does not involve crowdsourcing experiments or research with human subjects. Therefore, the requirement to include instructions to participants and details about compensation is not applicable to this paper.<br><br>However, it is crucial to confirm that all portions of the paper, including dataset collection and curation, are reviewed to ensure that no human subjects' involvement was overlooked. Based on the provided information and the nature of the research, the answer of "NA" for not applicable is correct, and no further action is required concerning this question.<br><br>---<br>The response provided aligns with the content and context of the paper under review, assuming the paper's focus is on developing and showcasing a meta-dataset for image classification tasks without involving human subjects directly in experiments or data curation processes that require crowdsourcing or explicit participation instructions.<br></span><br>Correctness Score: 1<br>--------------------------------------<br>--------------------------------------<br>Question # 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?<br><span style='color:#008000;'>Answer: NA</span><br><span style='color:#008000;'>Justification: This does not apply to our research</span><br><span style='color:#0000FF;'>Review: The answer provided by the authors, "NA," indicates that they believe the question does not apply to their research because it does not involve human subjects or crowdsourcing elements. This justification appears to be consistent with the content of the paper, which focuses on the development and utilization of a meta-dataset for few-shot image classification tasks. Given that the primary subjects of the research are datasets and image classification algorithms, there is no direct involvement of human subjects in experiments where there might be ethical concerns or the need for informed consent.<br><br>However, my criticism originates from a misinterpretation or a broader view of the term "potential risks incurred by study participants." In computational and data science research, especially concerning datasets, it’s crucial to consider potential secondary implications, such as privacy issues related to identifiable information within datasets, even if they are seemingly unrelated to direct human subject research. In this particular case, the paper extensively discusses the sourcing, preprocessing, and formatting of various datasets without mentioning any check or procedure to ensure that the images do not contain any sensitive or identifiable information. While it is likely that these concerns are minimized due to the nature of the datasets discussed (e.g., textures, animals), a brief acknowledgment of this aspect would have been appropriate.<br><br>Constructive suggestions for the authors:<br>1. Provide a brief statement in the manuscript regarding the evaluation of the datasets for any potential privacy concerns or sensitive information. Even a general acknowledgment helps to indicate awareness of these broader ethical considerations.<br>2. While it is understood that the research does not directly involve human subjects in a manner that traditionally necessitates IRB approval, clarifying any steps taken to ensure the ethical use of images (for example, confirming dataset licenses and usage rights, as done in this paper) enhances the readers' trust in the meticulousness of the research ethics.<br>3. For future work, when dealing with new datasets or extensions, consider including a standardized ethical review process as part of dataset inclusion criteria, focusing on privacy, consent, and data protection issues, even if IRB approval is not explicitly required.<br></span><br>Correctness Score: 0<br>--------------------------------------<br>