Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,Yes they do in the introduction and abstract.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the abstract and introduction do not include the claims made in the paper.
The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
","The proposed author's answer states a clear ""Yes"" to the question of whether the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. However, the justification provided, ""Yes they do in the introduction and abstract,"" does not adhere to the guidelines requiring authors to point to specific sections within the paper where related material for the question can be found.

**Actionable Feedback:**

1. **Explicitly Specify Sections:** To adhere to guidelines, the authors should specifically identify the sections in the paper where the claims made in the abstract and introduction are substantiated. For instance, if the paper claims to present a large-scale study examining citation bias in peer review, the authors should reference the ""Methods"" and ""Results"" sections where the study design and findings are detailed.

2. **Clarify Contributions:** The authors should ensure that the paper's introduction clearly delineates its unique contributions compared to existing literature on citation bias. Specific references to the ""Related Literature"" and ""Our contributions"" sections would be helpful in justifying the novelty and relevance of the study.

3. **Address Scope Appropriately:** The authors need to ensure that the scope mentioned in the abstract and introduction is consistent with the content and findings of the paper. If the paper explores citation bias across different academic fields or conferences, this scope should be clearly reflected in the ""Discussion"" or ""Conclusion"" sections.

4. **Inclusion of Limitations:** Although not directly asked in the question, discussing the limitations of their work in relation to the claimed contributions can strengthen the authors’ credibility. This discussion should be easily identifiable in the paper, likely in a ""Limitations"" subsection within ""Discussion"" or ""Conclusion.""

5. **Review for Overstatements:** The authors should critically assess whether any claims made in the abstract or introduction might be perceived as overstatements of the paper's findings. Adjustments should be made to ensure that the claims are fully supported by the empirical evidence presented in the paper.

6. **Improve Transparency:** Including a transparent discussion about how the study's findings contribute to the current understanding of citation bias in peer review - and the implications of these findings - can better align the introduction and abstract claims with the paper's contributions and scope.

By addressing these points, the authors can improve the alignment between their paper’s claimed contributions and scope with the actual content, enhancing its coherence and reception by the conference's reviewers and attendees.

**",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,"Yes, in the final section (Section 5)","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
The authors are encouraged to create a separate ""Limitations"" section in their paper.
The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
","The authors' proposed answer and justification seem to align well with the content of the paper, as the specified section (Section 5) directly addresses the limitations of the work performed by the authors. Given the detailed explanation in the provided content and based on the guidelines for addressing this specific checklist question, here's an organized feedback to ensure compliance and enhance the quality of the paper:

1. **Clarity and Depth of Limitations**: Ensure the limitations section provides a comprehensive and transparent account of potential weaknesses, including but not limited to the methodological limitations, the scope of generalizability for the findings, and any specific conditions or contexts in which the results might not hold. 

2. **Assumptions Discussed**: Verify that all underlying assumptions in the methodologies used are acknowledged and discussed regarding their potential impact on the findings and interpretations.

3. **Potential for Bias**: The paper should discuss any sources of bias, especially those related to the peer review and citation analysis processes. Given the nature of the study, it would be vital to assess the reflexivity in the research process and address how any biases inherent in the authors' positions or methodologies have been addressed or mitigated.

4. **Data Representativeness and Limitations**: Confirm the paper fully discusses the representativeness of the data used in the study, including any limitations due to sample size, selection criteria, and potential impacts on the study's generalizability.

5. **Implications of Findings**: While not a limitation per se, contextualizing the limitations within the broader implications of the study's findings would be beneficial. This could include discussing how the limitations might affect the interpretation of the results and any recommendations for future research or policy derived from the study.

6. **Future Directions**: Accompany the limitations with suggestions for future research that could address these limitations. This may include proposing specific methodologies, data sets, or analytical techniques that future studies could use to build on the current work.

7. **Ethical Considerations**: If not already included, ensure any ethical considerations, especially those pertinent to the study's limitations, are discussed. This includes, but is not limited to, considerations around privacy, consent, and the potential impact of the findings on the communities or subjects involved.

Feedback focused on these areas would significantly augment the paper's quality and transparency, demonstrating thoroughness in addressing the research's limitations.

",1
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,There is no theory.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include theoretical results. 
All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced. 
","The authors have identified a discrepancy in the proposed author's Answer and Justification regarding theoretical results in their study titled ""Cite-seeing and Reviewing: A Study on Citation Bias in Peer Review"". Despite claiming ""NA"" due to no theory present, the paper extensively discusses statistical testing frameworks, observational study design, and inferential analysis related to citation bias in peer review. This involves theoretical underpinnings and assumptions critical for their analyses and conclusions.

**Actionable Feedback:**

1. **Clarify Theoretical Contributions:**
   - Reassess areas in the paper where theoretical frameworks or models are implicitly utilized. This includes the design of observational studies, the statistical analysis approach, and the inference procedure. 
   - Explicitly detail any theoretical models or statistical frameworks employed in the paper, including assumptions made for parametric and non-parametric analysis.

2. **Include Theoretical Assumptions in the Paper:**
   - Clearly state the assumptions behind the statistical tests and models used for analyzing citation bias. This includes assumptions about reviewer independence, linearity of models, and matching procedures in non-parametric analysis.
   - Offer a dedicated section or appendix that discusses the theoretical basis of the chosen statistical methods and any potential limitations stemming from these theoretical choices.

3. **Provide Rationale for Theoretical Choices:**
   - Justify the selection of specific statistical models and tests in the context of your study objectives. Explain how these choices align with the nature of your data and the hypothesis you are testing.
   - Discuss the implications of your theoretical framework on the interpretation of your results. This is particularly important given the observational design of your study and its reliance on statistical inference.

4. **Revise the Answer or Justification Based on Theoretical Inclusion:**
   - Correct the initial response to reflect the presence of theoretical content in the paper. Consider a more nuanced response that acknowledges the theoretical underpinnings of your statistical analysis and study design, even if traditional ""theoretical results"" like new mathematical theorems are not present.
   - Update the paper to include a reference to the sections or appendices where these theoretical frameworks and assumptions are discussed.

",0.5
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,In Section 3.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
    - If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
    - If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
    - If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
    - We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

","The authors' proposed answer (""Yes"") and the justification provided (""In Section 3"") are brief and require elaboration to ensure full compliance with the guidelines outlined for NeurIPS submissions regarding reproducibility and detailed experimental methodology. Upon evaluating the provided sections against the checklist question, the following discrepancies and areas for improvement are identified:

1. **Experimental Details**: Section 3 outlines the experimental procedure and analysis methodology. However, the justification provided by the authors does not explicitly mention if all details necessary for reproducing the experimental results, such as hyperparameters, software versions, and detailed statistical analysis methods, are fully disclosed.

2. **Data Accessibility and Processing**: While Section 3 discusses the experiment conducted, it does not elaborate on the accessibility of the data used for the observational study. For reproducibility, it is critical to provide or describe access to the datasets or, if not possible, detailed descriptions of the data to enable similar data collection. 

3. **Code Availability**: The authors do not mention whether the code for the experiments and analysis is available. Providing access to code, or at least providing pseudo-code or detailed algorithms used for the analyses, is essential for reproducibility.

4. **Additional Details for Robustness Checks**: Given the observational nature of the study, it is vital to disclose any robustness checks or sensitivity analyses conducted. This helps in assessing the validity of the model assumptions and the generalizability of the findings.

Itemized, actionable feedback for the authors to improve paper quality and alignment with the guideline requirements includes:

1. **Expand the Experimental Details**: Include comprehensive descriptions of all experimental settings, hyperparameters used in your analysis, software versions, and any specific configurations relevant to reproducing your results. 

2. **Data Description and Access**: Provide explicit details on the data used, including how it can be accessed or replicated. If direct access cannot be provided, offer a detailed description that would allow others to collect or create a comparable dataset.

3. **Code Publication**: If possible, share the code repository link in the paper or supplementary materials. If direct sharing is not possible, provide detailed algorithms or pseudo-code for the key methodologies adopted in the study.

4. **Incorporate Robustness and Sensitivity Analysis**: Include a subsection dedicated to robustness checks or sensitivity analyses, explaining how variations in model assumptions or parameters affect the outcomes. This will enhance trust in your findings and their applicability.

5. **Clarification of Methodologies**: For your analytical methods, particularly statistical tests or machine learning models, provide justification for your choices and discuss any limitations they may pose to the reproducibility or validity of your results.

6. **Illustrative Examples or Supplementary Materials**: Consider adding illustrative examples of the experiments in supplementary materials to provide readers with a practical understanding of implementing your methodology for reproducibility.

By addressing these improvements, the authors will significantly enhance the reproducibility and clarity of their experimental results, aligning with the NeurIPS guidelines and the broader scientific principles of transparency and openness.

**",0.5
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,No,"We have extensively detailed our experimental procedure in section 3 and the appendix, sufficient for anyone to reproduce it. However, the peer review data is extremely sensitive and we do not have permissions from the participants nor from the IRB to release anything except the aggregate statistics.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
","The authors' proposed answer to the checklist question regarding open access to data and code is ""No"" due to the sensitive nature of the peer review data and restrictions based on permissions from participants and the IRB. This response is acceptable per the guidelines, given a proper justification is provided, which in this case, it is.

**Feedback for Improvement:**
1. **Clarify Dataset Anonymization Possibilities:** While sensitivity and permissions restrict open access to individual-level data, authors could explore and detail potential anonymization or data obfuscation techniques that preserve privacy while still enabling reproducibility. Mention efforts made in this direction or why such anonymization is not possible.

2. **Provide Dummy Dataset and Analysis Scripts:** Where the actual data cannot be shared, authors can create and share a dummy dataset structured similarly to the original, alongside the analysis code. This dummy dataset can demonstrate data formats and enable users to run and understand analysis pipelines, improving transparency around methodologies used.

3. **Release Aggregate Data and Meta-Data:** Since aggregate statistics are permissible for release, ensure that these are provided in a structured and comprehensive manner. Include meta-data or data dictionaries explaining the structure, contents, and any coding used within these aggregates to aid in understanding and potential replication studies.

4. **Detailed Reproduction Instructions:** Authors should provide thorough instructions for reproducing the experimental results. This includes detailed methodologies for data collection, analysis procedures, and any computational models employed. Where specific software or custom scripts are used, include version numbers and access instructions, even if the software/scripts themselves cannot be shared.

5. **Address Limitations Explicitly:** Discuss explicitly in the paper the limitations that the unavailability of the full dataset and code imposes on the reproducibility and generalizability of the results. Suggest ways in which other researchers might overcome or work around these limitations in future studies.

6. **Open Source Software Utilized:** If any open source software or publicly available tools were used in the analysis, provide detailed references and instructions on how these were employed. This information can be valuable for reproducibility even without direct access to the original data.

7. **Engagement with Community:** Encourage and outline ways the research community can engage with and extend the work, perhaps through simulation studies or by applying the methodology to openly available datasets in related domains. 

8. **Future Accessibility Plans:** If there’s any possibility that the data could be made available in the future (e.g., after a certain time period or once additional anonymization procedures are developed), this should be stated, along with any conditions or processes researchers would need to follow to gain access.

**
",0.5
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Not Applicable,We do not train or test any ML models and hence there are no hyperparameters or data splits etc.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental material.
","The authors have answered ""NA"" to the question regarding the specification of training and test details necessary to understand the results, justifying this by stating that their study does not involve training or testing any ML models. This response aligns with the nature of their observational study, which investigates citation bias in peer review rather than the performance of machine learning algorithms. Following guidelines and considering the actual content of the paper, the provided answer and justification are consistent with the study's focus and methodology. Hence, the detailed documentation of citation bias analysis, including the study design, methods for data collection and analysis, and clear delineation of the experimental and control groups, effectively replace the need for traditional training and testing details relevant to computational experiments. Thereby, the authors' response is appropriate, and their justification accurately reflects the paper's content, which primarily encompasses an observational study within a real-world peer review process rather than a computational model development and evaluation.

**Actionable Feedback:**
1. **Clarification of the Observational Study Nature**: Although the authors' response appropriately reflects the paper’s focus, clarifying within the main text that the study's methodology diverges from typical computational experiments could help readers better understand the reasoning for the ""NA"" response. Including a brief description of the study’s observational nature in the introduction or methods section could preempt similar confusion.
   
2. **Detailed Methodology Description**: Ensure the methods section comprehensively describes the observational design, data collection, and analysis methods, including any statistical models or algorithms used for analyzing the observational data. This description is crucial for ensuring replicability and transparency, akin to providing training/test details in computational studies.

3. **Rationale for Experimental Design Choices**: Expand on why certain methodological choices were made, especially regarding the selection of conferences, the construction of control and experimental groups, and the statistical techniques applied. This addition would parallel the way hyperparameter choices are justified in computational experiments.

4. **Supplementary Material**: If any supplementary analyses were conducted that did not make it into the main paper (e.g., additional tests for validating the identification of cited vs. uncited reviewers), consider including these as supplementary material. This approach can provide further confidence in the results, akin to including additional test sets or validation experiments in computational studies.

5. **Code and Data Availability**: While the study may not involve ML models, any scripts used for data analysis or statistical testing should be shared, if possible, alongside the dataset or a subset/sample of it used in the study, with due anonymization to respect reviewer confidentiality. Mentioning the availability of these materials in the paper would align with best practices for transparency and reproducibility.

**",1
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,Section 4,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error of the mean.
It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
","Based on the provided “Paper” content and the instructions for assessing the authors’ “Answer” and “Justification” regarding the reporting of error bars or appropriate statistical significance information, the following itemized feedback aimed at improving the paper quality is provided:

1. **Explicit Mention and Explanation of Error Bars**: While the paper's results section discusses statistical significance and presents confidence intervals, there is a lack of explicit mention and detailed explanation of error bars, whether they are included in the visual data representations (e.g., graphs) or how they are calculated for the tabulated results. Ensure that the methods section clearly describes the calculation of error bars or standard errors, including any assumptions made (such as data normality).

2. **Clarification on Statistical Tests**: The paper should clearly describe the statistical tests used for determining the significance of the reported results. Specifically, while p-values are mentioned, it's important to detail the types of tests (e.g., t-tests, ANOVA) that were applied, including any assumptions of these tests and how they align with the data characteristics.

3. **Detailing Factors of Variability**: The guidelines underline the importance of stating the factors of variability captured by the error bars. The paper should explicitly articulate these aspects, whether it's variability due to different initializations, random sampling of data, or variations across multiple runs of experiments. Including this information would significantly enhance the understanding of the results’ robustness.

4. **Assumptions and Data Distribution**: The assumption of normally distributed errors is critical when deciding on the error bar representation. If the errors are not normally distributed, the paper should provide an explanation of how this aspect was handled or considered in the analysis, perhaps by using other forms of data representation or statistical measures that better capture the distribution characteristics.

5. **Reference to Figures and Tables**: Ensure that all figures and tables that include error bars or show results with statistical significance are explicitly referenced in the text where these results are discussed. This would help readers easily locate the visual representation of the data supporting the text-based findings.

6. **Improvement in Data Visualization**: If error bars are not suitably incorporated in the currently provided figures or tables, action should be taken to revise these visual elements. This could include adding error bars to graphs, ensuring they are symmetric or asymmetric based on the actual data distribution, and adding legends or notes that explain what these error bars represent (e.g., standard deviation, standard error, confidence intervals).

7. **Discussion on Result Interpretation**: Incorporate a discussion that explicitly interprets the results in the context of the stated error bars or statistical significance measures. This discussion should address how the variability affects the conclusions drawn from the data and the extent to which these results support the paper's main claims.

To summarize, enhancing the transparency and explicit detailing of statistical measures used in the paper, including error bars and their calculations, will strengthen the credibility and clarity of the presented results. The improvements should ensure that readers have a full understanding of how the results were quantified, the variability inherent in the data, and how these aspects influence the conclusions of the study.

**",0.5
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,Not Applicable,Not applicable. We do not have experiments that depend on compute.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
","The authors' proposed answer to the checklist question is ""NA,"" stating that the paper does not include experiments that depend on compute resources. However, the paper clearly outlines an observational study executed in conjunction with the review process of two flagship conferences. This involvement of data collection and analysis inherently requires computational resources for processing and analysis, thereby contradicting the authors' response. The detailed sections on ""Methods,"" ""Analysis Procedure,"" and data filtering directly imply the use of computational tools and resources. 

**Actionable Feedback:**

1. **Clarify Computational Resource Usage:** The authors should accurately describe the computational resources involved in the observational study. This includes detailing the specific software (e.g., statistical packages used for analysis), computation platforms (local machines, cloud-based services), and the approximate computational time and memory requirements. 

2. **Revise NA Answer:** Change the NA response to ""Yes"" and provide a justification that directly references the sections of the paper discussing the observational study, particularly focusing on data analysis aspects that involve computational resources.

3. **Detailed Methodology Section:** Augment the ""Methods"" section with more specifics on the computational aspects, including any scripts, code, or algorithms used in the study. If proprietary or custom-developed tools were used, this should be explicitly stated, alongside any implications this might have for replicability by independent researchers.

4. **Explain Data Handling:** Given the study's scale, involving over a thousand papers and reviewers, explain how data was managed, processed, and stored, including any databases or storage solutions utilized. This will provide insight into the infrastructure required for replicating similar studies.

5. **Discuss Computational Limitations:** If there were any computational limitations that affected the study's design or scope (e.g., limits on the number of analyses due to computational time or resource constraints), these should be disclosed. Discussing limitations is crucial for understanding the context within which the study's findings apply.

6. **Supplementary Material:** Consider including any scripts or code as supplementary material or in a public repository under an open-source license. This aids in the transparency and reproducibility of the research.

By providing specific details on the computational resources used, the paper will offer a more comprehensive view that aligns with the experimental descriptions provided, enhancing its reproducibility and ensuring adherence to the NeurIPS submission guidelines regarding the disclosure of computational aspects of studies.

**",0.5
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,It conforms to the code of ethics.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
","**Feedback for Improvement:**

1. **Provide Clear References to the NeurIPS Code of Ethics**: The authors' proposed justification for confirming adherence to the NeurIPS Code of Ethics is overly broad and lacking in specificity. They should reference specific sections of their paper where ethical considerations, directly relevant to the NeurIPS Code of Ethics, are discussed. This will offer clear evidence that not only was the Code of Ethics reviewed but was also actively considered and adhered to throughout their research. For example, they could discuss how they dealt with data privacy concerns, consent, and the ethical use of AI in Section 3 ""Methods"" and Section 4 ""Results"", if applicable.

2. **Detail Ethical Considerations in the Experimental Design**: Given the observational study's potential impact on perceptions of peer review integrity, it is paramount to thoroughly discuss ethical considerations specific to their experimental design in Section 3 ""Methods"". The authors should explain any potential ethical risks identified and how they were mitigated. This might include measures taken to ensure that reviewer and author anonymity was maintained or how the findings were communicated to prevent misuse.

3. **Address the Use of Data and Participant Consent**: If the paper involves human data (from reviewers and authors), the authors should clarify how this data was collected and used, ensuring it aligns with the ethical use of data as per the NeurIPS guidelines. This information is crucial and should be explicitly stated, potentially in Section 3.1 ""Experimental Procedure"". If consent was obtained from individuals whose data was studied (even if retrospectively in the case of public or archival data), this needs to be clearly outlined.

4. **Discuss Potential Misuse of Research Findings**: Research on citation bias in peer review could inadvertently encourage manipulative citing practices if not carefully positioned. Section 5 ""Discussion"" should explicitly discuss potential misuses of their research findings and offer guidance on ethical citation practices. This proactive approach will demonstrate a thorough consideration of the NeurIPS Code of Ethics, particularly regarding the responsible dissemination of research findings.

5. **Clarify the Review and Approval Process**: The paper vaguely mentions that the ""experiment was reviewed and approved by an Institutional Review Board"" in the acknowledgments. The authors should provide more details about this process, including any ethical review conducted, in the experimental sections of the paper. This will reinforce the responsible execution of research, in line with ethical standards.

6. **Expand the Justification for Ethical Compliance**: Lastly, the authors should not only state compliance but also critically engage with the ethical dimensions of their study in the context of the NeurIPS Code of Ethics. This could involve a dedicated subsection in the ""Discussion"" where the implications of their findings are considered against the backdrop of ethical research conduct and publication practices in AI and machine learning.

**
These suggested improvements, when addressed, will significantly elevate the ethical rigor of the paper and ensure full compliance with the NeurIPS Code of Ethics.",0.5
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,Yes,"Yes, in section 5","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
","The author's proposed answer and justification appear congruent with the actual content outlined in the provided paper. The paper thoroughly investigates citation bias within peer review in major conferences, a subject with clear societal implications, both positive and negative. Given that the paper is positioned within the context of academic peer review, it indeed addresses societal impacts by examining fairness and bias, which are significant issues within the academic community and beyond. These impacts include the potential positive societal impact of fostering a more equitable and unbiased scientific review process and the negative societal impact of perpetuating an unfair academic environment where citation manipulation can influence career trajectories unjustly.

However, for constructive feedback aimed at enhancing paper quality with respect to discussing societal impacts specifically:

1. **Expand on societal implications:** While the paper effectively identifies the presence of citation bias in peer review processes and hints at its broader implications for fairness in academic careers, it could benefit from a deeper exploration of the societal implications. Elaborate on how these biases could impact the distribution of academic resources, contribute to the perpetuation of inequity in academia, and potentially stifle innovative research by emphasizing popularity over quality.

2. **Include broader societal impact discussion:** Link the findings more explicitly to broader societal impacts, such as the potential for these biases to influence public policy, funding allocations, or the general public's perception of scientific credibility.

3. **Mitigation strategies:** The paper would be strengthened by a dedicated discussion on mitigation strategies for the identified biases and their potential societal impacts. While the identification of the problem is crucial, proposing solutions or considerations for future systems could provide a more rounded discussion on societal impact.

4. **Examples of broader impacts:** Incorporate examples or hypothetical scenarios demonstrating how citation bias within peer review can ripple out to affect societal issues. This could include impacts on public health, technology policy, or educational disparities.

5. **Engage with literature on societal impacts:** Enhance the section discussing societal impacts by engaging more deeply with existing literature on the effects of academic publishing practices on society. This could help situate the paper within a broader discourse on scientific communication and its role in society.

By addressing these points, the authors can provide a more nuanced and comprehensive discussion on the potential societal impacts of their findings, aligning better with NeurIPS guidelines on discussing both positive and negative societal ramifications.

",0.5
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,There is no release of any data or models.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").
""
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
","Based on the provided paper content and the NeurIPS checklist question regarding the responsible release of data or models that have a high risk for misuse, the authors' assertion that such considerations are not applicable (NA) appears to be consistent with the subject and methodology of their study, which focuses on an observational study evaluating citation bias within the peer-review process. Since the study does not involve the release of any data or models, especially ones that could be considered high risk for misuse, the answer provided by the authors aligns with the actual paper content.

However, given the nature of the study and its reliance on observational data from peer review processes, it is crucial to ensure ethical considerations are fully addressed, particularly if any data, even in aggregated or anonymized form, were to be shared in the future or if the methodology could be applied in a manner that involves sensitive personal information. For the sake of thoroughness and in adherence to best practices in research ethics and transparency, the authors could consider expanding their discussion to include any potential ethical considerations relevant to their research design and methodology, even if these considerations do not directly involve the release of high-risk data or models.

**Feedback for Improvement:**

1. **Ethical Considerations:** Although the paper correctly states the NA response to the question on the release of high-risk data or models, it would benefit from a brief discussion or acknowledgment of any broader ethical considerations related to their research methodology, especially if there's potential for future release of data or analytic tools developed during the research.

2. **Data Privacy and Anonymity:** While not directly related to the checklist question, ensuring that the paper discusses safeguards for any data involved in the study, especially concerning reviewer and author anonymity in the peer-review process, would reinforce the paper's adherence to ethical standards.

3. **Future Work Implications:** A forward-looking statement about the responsible handling of any potential data or models that could be developed or released as a follow-up to this research could preemptively address concerns about ethical implications and responsible research practices.

4. **Transparency and Reproducibility:** If applicable, offering guidance on how similar observational studies could be conducted responsibly, including considerations for data privacy, consent (where applicable), and ethical review, might enhance the paper's contribution to the field's discourse on research ethics.

In conclusion, while the authors' response is aligned with the guidelines for the specific question, incorporating consideration for broader ethical implications extends the paper's commitment to responsible research practices.

**",1
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,We have used algorithms such as TPMS and PeerReview4All. We have cited,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
If this information is not available online, the authors are encouraged to reach out to the asset's creators.
","The authors' proposed answer to the checklist question about crediting the creators or original owners of assets used in the paper states ""Yes"" but the justification only mentions that algorithms such as TPMS and PeerReview4All are cited, without pointing to the specific sections where these citations are made. Moreover, the justification fails to mention whether the details about the licenses and terms of use for these assets, or any other used assets, are explicitly stated in the paper, which is a key part of the question.

### Actionable Feedback:

1. **Explicitly State Citations and License Information:** Ensure that the citations for TPMS, PeerReview4All, and any other used assets are clearly mentioned in the paper with specific section references in the justification. For example, ""Citations for TPMS and PeerReview4All can be found in Section X. The licensing information is discussed in Section Y.""

2. **Provide Details About Licenses and Terms of Use:** If not already included, the paper should be revised to explicitly mention the licenses and terms of use for all the third-party assets (data, models, code, etc.) utilized in the study. This should include the name of the license, any modifications made to the assets, and how the terms of use have been complied with.

3. **Include URLs for Assets, If Available:** For assets that are publicly accessible, include URLs or DOIs alongside the citations. This aids in transparency and allows readers to easily locate the original assets.

4. **Detail Any Correspondence with Asset Creators:** If there were any efforts to reach out to the asset creators for clarifications on usage or licensing that aren't publicly documented, briefly summarize these communications in the paper or supplementary materials.

5. **Revise the Justification in the Checklist:** Update the checklist's justification section to fully reflect these inclusions, ensuring it clearly states where in the paper each element of the question (citation of assets, license details, compliance with terms of use) is addressed.

6. **Check for Other Assets:** Review the paper to ensure that all used assets (including datasets and software tools not mentioned in the justification) are properly credited and that their licensing terms are respected and documented. Repeat steps 1-5 for each of these assets.

7. **Document Any Data Scraping Activities:** For any data collected via scraping or similar methods, ensure to document the process and mention the copyright and terms of service of the source websites, discussing the compliance with these terms.

### 0.5",0.5
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Not Applicable,No new assets,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
The paper should discuss whether and how consent was obtained from people whose asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
","**Feedback for Improvement:**

1. **Clarification on Assets Documentation:** The paper discusses an observational study on citation bias in peer review without introducing new datasets, code, or models traditionally considered as assets. However, the methodology and tools developed for identifying cited reviewers and matching (submission, reviewer) pairs can be viewed as procedural assets. The authors should clarify whether any part of their methodology, including software tools for reviewer-paper matching and analysis, is made available. If so, documentation for these procedural assets should be included or referenced explicitly in the paper.

2. **Description of Tools and Methodology:** Assuming the tools or methodologies used for data collection (e.g., the custom parsing tool mentioned for identifying citation relationships) are novel contributions of this work, these should be detailed in the methodology section. This includes the algorithm or approach for matching reviewers to papers based on citations, handling data missingness, and ensuring the reliability of citation identification. Specifications on how the tool avoids false positives/negatives in identifying cited reviewers could be considered as valuable documentation accompanying the study.

3. **Anonymized Data Release:** While the primary data (reviews and reviewer identities) cannot be shared due to confidentiality, the authors might consider releasing anonymized datasets of their findings, especially the aggregated data on citation bias effect sizes, without revealing specific identities or submissions. This would allow for secondary analysis by other researchers and increase the trust in the original findings.

4. **Licensing and Limitations:** If any software was developed as part of the study, the authors should provide information on licensing, limitations of the tool, and guidelines for use by other researchers. This would also apply to any novel statistical methodologies or simulation scripts developed for the analysis. 

5. **Documentation on Ethical Considerations:** The method section briefly mentions IRB approval, which is commendable. Expanding on ethical considerations, especially on the consent obtained or presumed from reviewers and authors for this kind of meta-analysis, would enrich the documentation of the study. 

6. **Addressing the NA Answer:** While the initial 'NA' answer is technically correct under the traditional view of assets, the authors should re-evaluate their contributions to consider if procedural methodologies and tools introduced in their study could be viewed as assets in the broader sense of supporting reproducibility and transparency in research methods. 

**",0.5
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,No crowdsourcing. The experiment was on observational data.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
","The authors have correctly marked the question as NA (""not applicable"") since their study focuses on observational data concerning citation bias in peer review processes and does not involve crowdsourcing experiments or research with human subjects in the manner typically associated with experimental psychology or social sciences. The mentioned observational study relies on the analysis of data from the review processes of two conferences without direct interaction with human subjects in the form of experiments where instructions and compensation details would be relevant. Therefore, the requirements to include full text of instructions given to participants, screenshots, and compensation details do not apply here.

However, to ensure full compliance with the Review Process Checklist and further improve the paper's quality and clarity concerning ethical considerations, the authors could consider the following actionable feedback based on the NeurIPS guidelines:

1. Ethical Considerations Reflection: Although not directly applicable, it would strengthen the paper to include a brief section or statement reflecting on any ethical considerations related to the observational study, especially since it deals with citation practices that could influence peer review processes and, by extension, career trajectories in academia.

2. Data Privacy and Anonymization: Clarify how the data was anonymized and confirm that there was no risk of identifying the participants (reviewers and authors) within the observational study. This is particularly relevant given the study's focus on potentially sensitive aspects of the peer-review process.

3. Institutional Review Board (IRB) Approval: If applicable, mention whether the observational study of citation bias received any form of ethical review or exemption from an Institutional Review Board (IRB) or equivalent ethics committee. This would provide reassurance that the study was conducted in accordance with ethical research practices.

4. Conference Collaboration Detail: Expand on the nature of collaboration with the conferences (ICML 2020 and EC 2021) and ensure that there was full compliance and agreement with using the review data for this study, highlighting any ethical guidelines followed by these conferences that pertain to data use.

By addressing these points, the paper would not only adhere more closely to NeurIPS guidelines but also reassure readers that all ethical implications and privacy concerns related to handling sensitive review data were thoroughly considered and addressed.

",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Yes,We got IRB approval as mentioned in the Acknowledgments.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
","The authors' proposed answer and justification generally adhere to the guidelines, mentioning that IRB approval was obtained and referenced in the Acknowledgments section. However, some critical elements are missing for a comprehensive review in alignment with the specific checklist question regarding the description of potential risks, disclosure to study participants, and IRB approvals.

Feedback for Improvement:
1. **Explicit Disclosure of Potential Risks**: The authors should detail any potential risks encountered by study participants within the paper, ideally in a dedicated section or within the methodology. This is crucial for transparency and ethical rigor, even if risks are minimal. 

2. **Details of Disclosure to Participants**: It should be explicitly stated how and what was disclosed to the study participants regarding the potential risks. This could include the process of informed consent or any measures taken to mitigate the identified risks. Simply stating that IRB approval was obtained is not sufficient to infer full compliance with ethical standards.

3. **Specifics of IRB Approval**: While mentioning IRB approval in the Acknowledgments is a good practice, the paper should also include specific details about the IRB approval process in the methodology section. This includes the IRB identifier, any conditions or stipulations made by the IRB, and how the study complied with these conditions. This adds to the credibility and replicability of the research.

4. **Inclusion of Ethics Statement**: The paper should include an ethics statement or subsection within the methodology section. This statement should cover the IRB approval, the consent process, and any potential risks and mitigation strategies, providing a concise yet comprehensive view of the ethical considerations of the study.

5. **Avoiding Assumptions on Reader's Part**: The authors assume readers will infer ethical adherence from the brief mention of IRB approval. A more detailed account ensures the reader does not have to make such assumptions and directly understands the ethical landscape of the study.

6. **Anonymity and Privacy Considerations**: Considering the study involves human subjects research, even if indirectly through data analysis, ensure that privacy and confidentiality measures are explicitly discussed. This is significant if identifiable information could be inferred from the data.

Conclusion:
The paper requires improvements to explicitly address the ethical considerations and IRB protocols as per the guidelines. Incorporating these details will not only fulfill the checklist requirement but also enhance the paper's integrity and ethical rigor. 

**",0.5
