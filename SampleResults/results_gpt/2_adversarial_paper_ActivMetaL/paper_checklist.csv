Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the paper's main contributions and scope.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the abstract and introduction do not include the claims made in the paper.
The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
","1. The abstract and introduction sections give a broad overview of the goal and mechanisms of the proposed algorithm recommendation approach using active meta-learning. However, they fail to accurately reflect the conceptual setting and aspirations introduced in the section on the state of the art and problem definition. Specifically, the introduction whimsically references ""Mystic Computation"" and ""Synthetic Cognition,"" setting a narrative tone that diverges significantly from the technical and empirical content discussed in later sections of the paper such as the active meta-learning problem formulation and algorithm implementation details. This discrepancy might mislead readers about the paper's contributions and the seriousness of its scientific inquiry.
   
2. Additionally, while the abstract succinctly presents the use of the CofiRank algorithm within an active meta-learning framework for algorithm recommendation, it does not sufficiently detail the novelty or the specific contributions beyond what existing approaches like AUTOSKLEARN and ALORS offer. Given that the justification for the 'Yes' answer points to the abstract and introduction for clear statements of contributions, it falls short by not explicitly contrasting these contributions with the state of the art in a manner that would be immediately clear to readers unfamiliar with the nuances of the field.

3. The abstract and introduction could be improved by:
   - Clarifying the distinction between the proposed active meta-learning approach and prior works, identifying what specific gaps are being addressed.
   - Reducing the use of thematic storytelling in the introduction that does not directly contribute to understanding the research contributions.
   - Including a brief gist of empirical findings and their significance relative to existing methods, which could provide a clearer picture of the paper's scope and contributions.

Feedback for Improvement:
- Amend the abstract and introduction to directly link the claims to explicit sections or findings in the paper, enhancing transparency and ease of verification for readers.
- Ensure consistency throughout the paper by avoiding thematic flourishes in one section that are not maintained or justified in the subsequent exposition of the methodology and results.
- Include a concise comparison or tabulation within the introduction that readily distinguishes this work from existing solutions, directly addressing the paper's novel contributions to the field of active meta-learning and algorithm recommendation.

",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,Yes,"The limitations are discussed in the section “Discussion and conclusion"".","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
The authors are encouraged to create a separate ""Limitations"" section in their paper.
The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
","The authors have indicated that the paper discusses the limitations of the work performed, as outlined in the ""Discussion and conclusion"" section. However, based on the guidelines, a more thorough evaluation of their response and provided justification is needed. The guidelines suggest that stating limitations should also encompass a range of aspects such as assumptions made, the robustness of results, the scope of the claims, performance influencers, computational efficiency, privacy, and fairness concerns, etc.

Actionable feedback includes:

1. **Clear Articulation of Assumptions**: Authors should explicitly state any assumptions relating to their active meta-learning approach. This could include assumptions about the data, the computational framework, or the applicability of the CofiRank algorithm across different settings.

2. **Robustness of Results**: It would enhance the paper's quality if the authors discussed the robustness of their experimental results. This involves considering how small changes in the data or model parameters might impact the outcome of their algorithm recommendations.

3. **Scope of Claims**: The paper should clearly demarcate the boundaries within which their claims hold true. For example, if the approach was tested only on a limited number of datasets or within certain types of tasks, this limitation needs to be explicitly stated.

4. **Discussion on Performance Factors**: The authors should include a discussion on factors that might affect the performance of their proposed approach, such as dataset size, feature diversity, or computational constraints.

5. **Computational Efficiency**: A detailed discussion regarding the computational efficiency of their methods, especially how it scales with the size of the dataset, would be beneficial. This discussion should include any potential bottlenecks and how they might be addressed.

6. **Privacy and Fairness**: If applicable, the authors should contemplate the implications of their approach on privacy and fairness. Given the rise of these concerns within the machine learning community, acknowledging these aspects could significantly enhance the paper's relevance and ethical considerations.

7. **Transparency and Norm Development**: The authors are encouraged to be transparent about any limitations in their work to contribute positively to the norm development within the scientific community. This transparency includes acknowledging when the approach might not be the best choice or highlighting any failures discovered during the experimental phase.

Implementing these feedback points will not only address possible discrepancies in the justification provided but also significantly enrich the paper's discussion section, thereby improving its quality and contribution to the field.

",0.5
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,[TODO],"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include theoretical results. 
All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced. 
","The authors' proposed answer to the specific checklist question regarding the provision of the full set of assumptions and a complete (and correct) proof for each theoretical result is marked as ""NA"" (Not Applicable), suggesting that the paper does not include theoretical results. However, the authors did not provide a justification for this answer.

Actionable feedback:

1. **Clarify the Presence or Absence of Theoretical Results**: The paper should clearly state whether it includes any theoretical results. If there are none, this should be explicitly stated in the section of the paper where such results might ordinarily be expected (e.g., after introducing the main algorithm or method). This clarification will directly address the checklist question and prevent any assumptions from readers expecting theoretical foundations to be presented.

2. **Provide Justification for NA Answer**: If the paper indeed does not contain theoretical results, the authors need to provide a justification for the ""NA"" answer, aligned with the guidelines provided. This could simply involve stating that the paper focuses on empirical validation of the proposed method rather than theoretical analysis, which is common in papers that introduce novel algorithms or applications.

3. **Discuss Assumptions Informally**: Even if there are no formal theoretical results, the paper likely operates under certain assumptions for its empirical methods to work correctly or efficiently. These assumptions should be detailed in the text, possibly within the sections describing the problem setting or algorithm development. For example, assumptions about the nature of the datasets, algorithm performance characteristics, or expected computational complexity could provide valuable context for readers and reviewers.

4. **Consider Including a Discussion on Potential Theoretical Analysis**: If appropriate and relevant, the authors might add a discussion or future work section suggesting how theoretical analysis could be approached in future research. This would not only address the question indirectly but also demonstrate the authors' awareness of the importance of theoretical foundations in algorithmic research.

5. **Cross-Reference Relevant Sections**: Ensure that any discussion related to the empirical settings, assumptions inherent in the algorithm design, or experimental setup is correctly cross-referenced within the paper. This will help readers navigate the paper more effectively and understand the basis on which the empirical validations are built.

Given the absence of justification for the NA answer and unaddressed assumptions that may underpin the empirical methodology, the feedback above aims to improve the paper's quality by clarifying these points.

",0
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,Yes,All information needed to reproduce the experimental results is given in form of pseudo-code of provided Algorithms and their code accessible to public.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
    - If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
    - If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
    - If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
    - We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

","The Authors' Proposed Answer states that all information necessary to reproduce the experimental results is given, citing the availability of pseudo-code and publicly accessible code. However, the provided justification does not specify which sections of the paper the readers can find the detailed descriptions of the experimental setup, parameters, datasets, and any preprocessing steps necessary for reproducing the results. This overlooks the comprehensive requirements outlined in the Guidelines for ensuring reproducibility. 

1. Specify sections: The answer should explicitly point out the sections or parts of the paper where the information supporting reproducibility can be found. For instance, if experimental design is elaborated in Section 5, and the algorithms are detailed in Section 3 with pseudo-code, this should be clearly stated in the justification.

2. Detail on datasets: While the paper mentions the use of three real-world datasets and artificial data, it does not provide enough details on how these datasets can be accessed, their preprocessing steps, or how to exactly replicate the experimental conditions. Providing access to the datasets used (or detailed specifications if they cannot be made public) and any preprocessing steps are crucial for reproducibility.

3. Experimental setup and parameters: For full reproducibility, the paper should describe the experimental setup in great detail, including the specification of any hyperparameters, the computational environment, and the version of software tools used. If certain parameters are selected based on the dataset characteristics, this decision process should be described, possibly in a supplementary file if space is a concern.

4. Access to code: Stating that the code is publicly accessible is positive but insufficient. The paper should include a direct link to where the code can be found, ideally in a stable repository where it can be accessed by readers. Additionally, ensuring the code is well-documented facilitates understanding and replication of the results.

5. Clarification on pseudo-code: Although the pseudo-code is provided for the algorithms, the paper should also discuss any variations or specific implementations details that are crucial for reproducing the results. If the pseudo-code is simplified for presentation, the actual implementation might differ; those differences and their impact should be disclosed.

6. Handling of exceptions or special cases: If there were any exceptions, special cases in the data processing, or if certain results could not be replicated in repeated experiments, these should be openly discussed. Transparency about the challenges faced during experimentation not only aids in reproducibility but also enriches the scientific discourse.

In conclusion, the paper needs significant improvements in detailing the reproducibility of its experiments. The authors should thoroughly address the guidelines for reproducibility by providing precise references to sections, ensuring access to data and code, detailing the computational environment, and describing the experimental procedures comprehensively.

",0.5
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,Yes,"The github repo for reproducibility is given, in which you can find the code and dataset.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
","The authors' proposed answer to the checklist question regarding open access to data and code with sufficient instructions for reproducing the main experimental results is affirmative. However, their justification simply states that the GitHub repository for reproducibility is provided without specifying the section in the paper where this information can be found. This answer does not comply fully with the guidelines, which require pointing to the specific sections containing related material for the question.

#### Actionable Feedback:
1. **Specify Section with URL or Details:** Authors should amend their justification to include a reference to the exact section(s) of the paper where the GitHub repository link is provided. This will help readers and reviewers easily locate the materials needed for replication.

2. **Include Detailed Reproduction Instructions:** Ensure that the GitHub repository includes exhaustive instructions for reproducing the results. This should not only encompass the running commands but also details regarding the computational environment (dependencies, versions), data preprocessing steps, and any parameter settings relevant to the experiments.

3. **Address Data License Information:** If not already included, the authors need to ensure that information regarding the license under which the dataset is distributed is mentioned. If the dataset cannot be made openly accessible, a detailed explanation and instructions for obtaining the data should be provided.

4. **Clarify Subset of Reproducible Experiments:** If there are any experiments whose results cannot be fully reproduced due to reasons such as computational expense or data privacy, this should be explicitly stated, along with the rationale.

5. **Ensure Anonymity for Submission:** Since the paper is under review for the NeurIPS conference, ensure any links to data or code repositories are anonymized to comply with the double-blind review policy. Consider using anonymized links or a statement promising to provide the links upon acceptance.

6. **Comprehensive Testing Before Submission:** Lastly, to ensure that the replication package is complete and functional, it would be advisable for the authors or someone not involved in the project to attempt reproducing the results using only the provided instructions and materials. This step will help identify any gaps in the replication documentation.

By addressing these points, the authors will significantly improve the reproducibility of their work, aligning their submission with the best practices for computational research and the guidelines provided by the NeurIPS conference.

",0.5
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,"These details are given in the section of “Results"".","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental material.
","Feedback on the ""Answer"" and ""Justification"" for the Checklist Question:

1. The answer provided is ""Yes,"" claiming that the necessary training and test details are specified in the ""Results"" section. However, typically, the ""Methods"" or ""Experimental Setup"" sections are more appropriate for detailing the training and test configurations like data splits, hyperparameters, type of optimizer, etc., rather than the ""Results"" section. The ""Results"" section usually focuses on the outcomes of the experiments rather than the setup.

2. The justification should clearly guide the reader to specific subsections or paragraphs where the training and test details are comprehensively described. Simply pointing to the ""Results"" section might not be sufficient if these details are scattered across the paper or are not thoroughly covered.

3. If the experimental setup details are indeed in the ""Results"" section, it might indicate an unconventional organization of the paper, potentially making it harder for readers to follow the methodology behind the experiments. This could affect the reproducibility of the study.

4. It is not clear if the paper specifies how the hyperparameters were chosen, which is crucial for understanding the experiments and reproducing the results. Hyperparameter selection can significantly impact performance, and its disclosure is essential for a transparent evaluation of the methodology.

5. The type of optimizer and its configuration is foundational information for algorithm training details within machine learning research. Failure to adequately describe the optimizer may limit the paper's technical rigor and reproducibility.

Actionable Feedback:
- Move the detailed training and test configurations to a dedicated ""Experimental Setup"" or ""Methodology"" section, if not already done. Ensure this section clearly outlines data splits, hyperparameter selection processes, optimizer types, and any other relevant computational details.
- Provide specific references (section numbers, subsection titles) in the justification to direct reviewers and readers precisely to the parts of the paper where these details can be found. 
- Enhance the description of the process for hyperparameter selection and the rationale behind choosing the particular type of optimizer used in the study. Details about the computational framework (software, version, hardware specifications) can also support reproducibility.
- Consider appending supplementary material or an appendix if the paper's length constraints limit the depth of methodological explanations in the main text.
- Review and ensure that the organization of the paper follows conventional scientific reporting standards for ease of readability and reproducibility.

",0.5
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"These details are given in the section of “Results"" and the figures in this section.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error of the mean.
It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
","The authors' proposed answer is ""Yes,"" and their justification correctly points to the ""Results"" section and the accompanying figures for details about error bars and statistical significance of the experiments. According to the guidelines, the factors of variability that the error bars are capturing should be clearly stated, the method for calculating the error bars should be explained, and the assumptions made should be given. Furthermore, it should be clear whether the error bar represents the standard deviation or the standard error of the mean, or if it is using a 1-sigma or 2-sigma error bar.

After reviewing the provided information, here is the feedback aiming to improve the paper quality:

1. Clarify the Factors of Variability: The authors should explicitly state the factors of variability captured by the error bars in the ""Results"" section. This might include details on whether the variability is due to train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions.

2. Method for Calculating Error Bars: The paper should clearly describe the method used to calculate error bars (e.g., closed-form formula, call to a library function, bootstrap method, etc.). This is crucial for ensuring the reproducibility of the results and allowing readers to fully understand the significance of the reported performance metrics.

3. Assumptions Underlying the Error Calculations: Any assumptions made during the error bar calculations should be explicitly stated. This might include assumptions about the normal distribution of errors, which is particularly important if 2-sigma error bars or confidence intervals are being reported.

4. Explanation of Error Bar Type: It should be clearly stated whether the reported error bars represent the standard deviation, the standard error of the mean, or another metric. If a 1-sigma or 2-sigma error bar is reported, it should be specifically mentioned along with a justification, especially if the normality of errors cannot be assumed.

5. Address Asymmetric Distributions: If the data has asymmetric distributions, it is important to ensure that symmetric error bars do not lead to misleading interpretations of the data, potentially yielding results that are out of range (e.g., negative error rates). The paper should discuss how this issue is handled.

6. Reference Figures or Tables in Text: The textual description in the ""Results"" section should more clearly reference the specific figures or tables where error bars or statistical significance information can be found, improving the readability and navigability of the paper.

By addressing these points, the authors will enhance the clarity and comprehensiveness of the statistical reporting in their paper, thereby boosting its overall quality and contributing to a better understanding of the research findings.

",0.5
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,No,We only discussed the related computational expenses.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
","Actionable Feedback:

1. **Explicitly Disclose Computational Resources**: The authors must provide explicit details regarding the computational resources used for each experiment, following the NeurIPS guidelines. This includes specifying the types of compute workers (e.g., GPU, CPU), whether these were part of an internal cluster or utilized cloud services, and relevant memory and storage capabilities. Amend the paper to include a new subsection in the experimental setup that clearly outlines these details for reproducibility.

2. **Estimate of Total Compute**: Given the active meta-learning nature of the proposed approach, it is crucial to estimate and report the total compute required for both the preparatory phase (benchmarking existing algorithms on datasets) and the computational cost for running CofiRank under the active meta-learning framework. This estimation should be added to the experimental results section.

3. **Account for Preliminary or Failed Experiments**: The authors should discuss whether the computational resources reported include only those related to successful experiments or if preliminary or failed experiments that did not make it into the final paper were also considered. This is crucial for a comprehensive understanding of the computational budget required for similar future projects. If not already included, a discussion around this point should be appended to the section discussing computational considerations.

4. **Relate Computational Cost to Experimental Insights**: Link the discussion of computational resources to the insights and findings of the paper. For example, if certain algorithms were selected or discarded based on their computational efficiency or cost, this rationale should be clearly explained. Insights into how computational considerations might influence the choice of algorithms in practice could significantly enhance the contribution of the paper.

",0
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We reviewed the code of ethics and our paper conforms with it.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
","The authors claimed that their paper conforms to the NeurIPS Code of Ethics but have not provided specific sections or explanations demonstrating how their work complies with the ethical guidelines. This lack of detail makes it difficult to assess the validity of their claim thoroughly.

Feedback:

1. **Clarify Ethical Considerations**: The authors should specify sections in their paper where ethical considerations, related to their research, are discussed. If there are direct implications, such as data privacy concerns, potential biases in algorithm recommendations, or impacts on individuals and society, these should be clearly addressed with corresponding mitigation strategies.

2. **Data Use and Privacy**: Given the use of real-world datasets (StatLog, OpenML, and AutoML), the authors must clarify how they ensured the privacy and proper use of the data, especially if any of these datasets contain personally identifiable information or could raise privacy concerns.

3. **Algorithmic Bias and Fairness**: The paper should include a discussion on the potential for algorithmic bias, given that machine learning models can inadvertently perpetuate or amplify biases present in the data. The authors should discuss any steps taken to assess and mitigate bias within their model recommendations.

4. **Transparency and Reproducibility**: While the supplemental material mentions data and code availability on GitHub, ensuring adherence to the ethical guideline of transparency involves more than just providing code. The authors should discuss how they have ensured that their experiments are reproducible, and any limitations or dependencies that might affect the generalizability or applicability of their method.

5. **Potential Misuse**: Any technology, especially those relating to algorithm recommendations, can be potentially misused. The paper should discuss any foreseeable misuse of their proposed methods and any measures that could be taken to prevent such misuse.

6. **Impact Statement**: The authors are encouraged to include an impact statement discussing the broader implications of their work on society, the environment, and the field of AI. This should cover both positive impacts and potential negative consequences, reflecting a comprehensive understanding of their work in real-world contexts.

By addressing these points, the paper will not only better comply with the NeurIPS Code of Ethics but also contribute to a more responsible advancement of the field.

",0.5
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,No,Our paper doesn't discuss potential positive societal impacts and negative societal impacts of the work performed.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
","The authors have adequately identified that their paper does not discuss both the pros and cons of the societal impacts stemming from their work on algorithm recommendation with active meta-learning, aligning with the response to the checklist question. This acknowledgment follows the NeurIPS guidelines, which accept a “No” answer provided that a proper justification is presented. However, there are several ways the authors can enhance their paper in relation to this question:

1. **Expand on Justification**: Although stating that the paper does not discuss societal impacts suffices, the authors could enrich their justification by briefly explaining WHY discussing societal impact might not be directly relevant or providing insight into how future work could address this gap. For example, they could speculate on the broader implications of making algorithm recommendations more efficient and adaptive, noting both positive aspects (e.g., accelerating scientific research, optimizing computational resources) and potential negative repercussions (e.g., misuse in sensitive or biased applications, reinforcing existing biases).

2. **Mitigation Strategies**: Even if a full discussion of societal impacts is outside the scope of the paper, the authors could contribute a small section or a paragraph on possible mitigation strategies for any negative impacts they can foresee. This does not have to be exhaustive but should demonstrate awareness and responsibility towards the dual-use nature of machine learning technologies.

3. **Implications on Fairness, Privacy, and Security**: The authors might briefly consider the fairness implications of their approach, privacy concerns related to data used in meta-learning, and any security considerations. Even if these are not deeply analyzed, acknowledging their existence would show a comprehensive understanding of the potential societal footprint of their work.

4. **Literature on Societal Impacts**: They could reference existing literature that discusses the societal impacts of machine learning technologies broadly, and more specifically, areas relevant to their work such as algorithm recommendation systems. This would not only strengthen their paper but also provide readers with resources to understand the broader context.

5. **Future Work**: Adding a section on future work that includes investigation into the societal impacts of their proposed system could be beneficial. This indicates to readers and reviewers that the authors are thinking critically about the broader implications of their work beyond the immediate technical achievements.

In summary, while the authors' current response meets the basic guideline requirements, there are several opportunities for enhancing the paper’s quality and depth by integrating discussions on societal impacts more thoroughly, even if these discussions are speculative or brief. This addition would demonstrate a thoughtful consideration of the responsibilities that come with developing advanced machine learning technologies.

",0.5
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,Our paper poses no such risks.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").
""
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
","The authors have stated ""NA"" (Not Applicable) for the question about describing safeguards for responsible release of data or models that have a high risk for misuse, with the justification that their paper poses no such risks. However, after reviewing the paper details provided, it seems that this justification might not fully consider the potential implications of their work. The paper is focused on ""Algorithm Recommendation with Active Meta Learning"" and involves comparing performance of algorithms on different datasets, including some real-world datasets gathered from various sources. While the authors might argue that their core contribution (a meta-learning approach for model selection) does not directly pose risks of misuse, the application field itself and the utilization of real-world datasets potentially engage with questions of data privacy, security, and the responsible use of algorithm recommendations. 

Here is the actionable feedback based on the review of the paper content and considering the NeurIPS guidelines:
1. **Re-evaluate the Potential for Misuse**: The authors should critically assess not only the immediate outcomes of their research but also how it could be situated within a broader context. This includes questioning how the algorithm recommendations and active meta-learning approaches might be applied and what safeguards could be pertinent to ensure responsible usage.
   
2. **Clarify Data Utilization Practices**: Given the use of real-world datasets, it would be beneficial for the authors to clarify any data handling practices, especially regarding privacy and data security. Even if the datasets are publicly available, discussing the ethical considerations and any data anonymization or aggregation steps would add value.

3. **Discuss General Responsible AI Practices**: The paper could be enhanced by including a section discussing general responsible AI practices within the context of their proposed methodology. This can cover aspects like transparency in algorithm recommendations, accountability in model selection processes, and considerations for fairness and bias evaluation in the context of active meta-learning.

4. **Potential Misuse in the Application Domain**: While the paper focuses on a technical solution, a brief discussion on the potential misuse scenarios in the application domain of their algorithm recommendation system could be insightful. Addressing how the technology could be misapplied and suggesting measures to mitigate such risks (e.g., guidelines for usage, ethical considerations in algorithm selection) would strengthen the paper.

5. **License and Accessibility Issues**: If any part of the proposed system, datasets, or models is meant to be released, the authors should discuss licensing issues. This includes whether the datasets used are under licenses that allow for the type of use described in the paper and any restrictions on the distribution of the models or algorithms developed.

By addressing these points, the authors can provide a more comprehensive view of the ethical and responsible research dimensions tied to their work. This improvement will align the paper more closely with the conference guidelines and the broader expectations of the research community regarding responsible AI.

",0.5
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,Yes,"creators or original owners of assets (e.g., code, data, models), used in the paper are cited.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
If this information is not available online, the authors are encouraged to reach out to the asset's creators.
","The authors claim to properly credit and respect the terms of use for the assets utilized in their paper. However, their justification merely stating that ""creators or original owners of assets (e.g., code, data, models), used in the paper are cited"" is insufficient without specifying the sections where these citations can be found, or detailing the licenses and terms of use as per the guidelines.

**Feedback:**

1. **Explicit Citations and License Information:** Ensure that each utilized asset (code, data, models) is not only cited but is accompanied by explicit mention of the original authors, the version of the asset, and a URL if applicable. This detail should be clearly presented in the relevant sections where each asset is mentioned for the first time.

2. **Section References:** Update the justification to include specific section references where the assets are cited. This will assist readers and reviewers in quickly verifying the adherence to this checklist item.

3. **License Details:** For each cited asset, include the name of the license (e.g., MIT, CC-BY 4.0) directly in the text where the asset is mentioned. If different parts of your work use assets under different licenses, those distinctions must be clear and unambiguous.

4. **Terms of Use Compliance:** Specifically address how the terms of use for each asset have been respected. This could involve mentioning any permissions obtained for assets without clear or open licenses, especially for proprietary data or code.

5. **Derived Works:** If any asset was modified or included in a composite or derived work, explain the nature of the modifications and how the license of the original asset applies to the derivative work. Include both the original and the derived asset's license details.

6. **Accessibility Information:** For complete transparency and to assist replicability, add an appendix or a section within the supplementary material that comprehensively lists all assets used in the study along with their citation, license information, and a URL for access. If assets were developed in-house or are being released as part of the paper’s contributions, clearly state the license under which these new assets are being made available.

7. **Data Scraping Compliance:** If any data were scraped from websites or other sources, include a statement confirming compliance with the terms of service/copyright of the source websites. Provide a rationale for why the scraping approach respects legal and ethical standards.

Implementing these suggestions will enhance the paper's compliance with the NeurIPS requirements for asset crediting and license transparency, providing clear benefits for reproducibility and ethical considerations in research dissemination.

**",0.5
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Yes,"This is given in the github repo, whose link is given in the paper.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
The paper should discuss whether and how consent was obtained from people whose asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
","The authors' proposed answer states that the new assets introduced in the paper are well documented, with the documentation provided on a GitHub repository, the link to which is given in the paper. However, the guidelines specify that for a ""Yes"" answer, the authors should point to the section(s) where related material for the question can be found within the paper itself rather than external sources.

**Actionable feedback:**

1. **Reference Section**: Explicitly state in the paper (ideally in the Introduction or Conclusion sections) the exact location where the GitHub repository link can be found. This could be along the lines of ""See Section X for link to the GitHub repository containing all assets related to this study.""

2. **Documentation Details in Paper**: Incorporate a subsection within the Methodology or Supplementary Material sections that briefly describes what the GitHub repository contains. This should cover datasets, code (including the implementation of algorithms), and any other assets introduced in the paper. Provide details such as the structure of the repository, contents of each folder, how to navigate the assets, and any prerequisites needed to use the assets (such as CofiRank installation).

3. **Licensing Information**: Ensure that the paper mentions the license under which the assets are released. This is crucial for reuse and replication of the results presented. If this information is only available in the GitHub repository, briefly summarize it in the paper and direct readers to the repository for more details.

4. **Anonymization During Review Process**: If applicable, confirm whether the GitHub link provided in the submission is anonymized or if arrangements have been made to maintain the double-blind review process (such as providing an anonymized version of the repository).

5. **Limitations and Preconditions**: Include a discussion in the paper about any limitations or preconditions associated with using the provided assets. This will help readers and potential users set appropriate expectations about the utility and applicability of the assets.

6. **Consent for Use of External Assets**: If any external assets or data were used as part of the newly introduced assets, the paper should explicitly state how consent was obtained, or how these assets were deemed appropriate for use.

By addressing these feedback points, the authors will improve the accessibility and reproducibility of their research, aligning better with NeurIPS submission guidelines for documenting and sharing new assets. 

**",0.5
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
","The Paper explicitly discusses the development and testing of a metalearning algorithm for algorithm selection using benchmark datasets, which do not involve human subjects or crowdsourcing experiments. The experiments are computational and data-driven, based on existing datasets and algorithmic performance evaluation, aligning with the Authors' Proposed Answer of NA to the question about including information on human subjects or crowdsourcing.

Given the nature of the work presented:
- The methodology section details an algorithmic approach and computational experiments without mentioning any involvement of human subjects or the need for crowdsourced data collection.
- The experimental validation uses pre-existing datasets, focusing on algorithm performance rather than data collected through human interaction or participation.

## Actionable Feedback:

1. **Clarify the nature of data and experiments**: Although the NA response seems appropriate based on the provided content, ensure the paper explicitly states that the research did not involve human participants or crowdsourcing at any stage if not already clearly mentioned. This dispels any ambiguity regarding ethical considerations or the need for participant consent forms.

2. **Dataset source and ethical use**: Even though no crowdsourcing or human subjects were involved, ensure that the use of any third-party datasets (StatLog, OpenML, AutoML, and artificial data) adheres to their respective licenses and ethical guidelines. A brief acknowledgment of the ethical use and sourcing of these datasets can enhance the paper's integrity.

3. **Justification of NA response**: While the response is supported by the paper's content, a brief sentence could be added in the methodology section or as a footnote, reiterating that human participation was not involved, thus supporting the NA response directly within the paper.

4. **Supplemental material check**: Ensure that all supplemental materials, including datasets and code made available, do not inadvertently contain personal information or data collected from humans that would contradict the NA answer. This is crucial for maintaining ethical transparency and respecting privacy.

5. **Future Work Section**: If future directions of this research might involve human subjects—for example, in evaluating the user experience of the model selection tool or crowdsourcing for algorithm performance validation—mentioning potential ethical considerations proactively can be beneficial.

In conclusion, the paper's focus on algorithmic methodologies and computational experiments with pre-existing datasets aligns with the NA response to the checklist question. Ensuring clear communication around the nature of experiments and data use enhances transparency and addresses any potential ethical queries from reviewers or readers.

",1
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This paper does not involve crowdsourcing nor research with human subjects.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
","The authors' proposed answer of ""NA"" citing that the paper ""does not involve crowdsourcing nor research with human subjects"" seems consistent with the content of the paper as it primarily focuses on algorithm recommendation utilizing active meta-learning without directly mentioning any involvement of human participants or crowdsourcing efforts in its methodology or data collection processes. The justification aligns with the guidelines that stipulate a necessity for disclosing potential risks to study participants, IRB approvals, or equivalent if the study involves human subjects. 

However, to improve the paper quality and address potential considerations regarding the ethical aspects and broader impacts of their research, it might be beneficial for the authors to contemplate and mention any indirect implications their study might have on individuals or society, especially given the application of their research in algorithm recommendation which could potentially influence decision-making processes in various domains. While not directly involving human subjects, the application and outcomes of their research could have implications that merit a brief discussion or acknowledgment.

**Actionable Feedback:**

1. **Clarify the Scope of Ethical Considerations:** Even if the study does not involve human subjects directly, consider adding a brief section discussing any broader ethical implications the application of this technology might have. This could be particularly relevant in ensuring transparency and fairness in algorithm recommendations and addressing potential biases in meta-learning models.

2. **Discuss Data Source Ethical Considerations:** If your algorithm recommendation method utilizes datasets obtained from external sources (e.g., OpenML, AutoML), ensure that these datasets were collected and shared ethically, even if an IRB review is not applicable. A brief statement on the ethical use and provenance of these datasets can enhance the paper's commitment to ethical research practices.

3. **Broader Impacts Statement:** Although not directly related to the use of human subjects, NeurIPS encourages the discussion of broader impacts. Including a section on the possible societal impacts, both positive and negative, of your research findings could provide a more comprehensive overview of the ethical landscape surrounding your work.

4. **Transparency in Methodological Choices:** Ensure that any methods involving user data, even indirectly through datasets, are transparent about the potential biases and limitations this may introduce to the study. This addresses ethical considerations by promoting awareness and discussions on mitigation strategies.

By addressing these areas, the authors can enhance the ethical clarity and societal considerations of their research, aligning with the growing emphasis on responsible AI research practices.

**",1
