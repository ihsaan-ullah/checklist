<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }
    .container {
        margin: 20px auto;
        padding: 0 20px;
        position: relative;
    }
    .button {
        padding: 10px;
        font-size: 16px;
        text-align: center;
        background-color: #f93361;
        color: #fff;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        text-decoration: none;
    }
    .button:hover {
        background-color: #bc0530;
    }
    .content {
        padding-top: 60px;
        padding-bottom: 40px;
        margin-bottom: 20px;
    }
    h1 {
        margin-top: 0;
        margin-bottom: 30px;
    }
    hr {
        margin-top: 50px;
        margin-bottom: 50px;
    }
    .review {
        margin-bottom: 30px;
        border: 1px solid #ccc;
        padding: 20px;
        border-radius: 5px;
        background-color: #f9f9f9;
    }
    .review h2 {
        margin-top: 0;
    }
    .review p {
        margin: 10px 0;
    }
    .question {
        /* color: #0033ff; */
        color: #000;
    }
    .answer {
        /* color: #28a745; */
        color: #000;
    }
    .justification {
        /* color: #de750b; */
        color: #000;
    }
    .user_input {
        padding: 20px;
        border-radius: 5px;
        background-color: #fff;
        border: 1px solid #3a3a3a;
    }
    .llm_review {
        color: #000;
        padding: 20px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .llm_review-red {
        background-color: #eacfcf;
        border: 1px solid #FF0000;
    }
    .llm_review-green {
        background-color: #c6e9c6;
        border: 1px solid #008000;

    }
    .llm_review-orange {
        background-color: #ebdecf;
        border: 1px solid #FF8C00;
    }
    table {
        border-collapse: collapse;
    }
    th, td {
        padding: 8px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }
    th {
        background-color: #f2f2f2;
    }
    .score-label {
        display: inline-block;
        padding: 5px 15px;
        border-radius: 5px;
        text-decoration: none;
    }
    .score-green {
        background-color: #c6e9c6;
        color: #000;
        border: 1px solid #008000;
    }
    .score-red {
        background-color: #eacfcf;
        color: #000;
        border: 1px solid #FF0000;
    }
    .score-orange {
        background-color: #ebdecf;
        color: #000;
        border: 1px solid #FF8C00;
    }
    .score-blue {
        background-color: #c8d8e6;
        color: #1b455e;
        border: 1px solid #1b455e;
    }
    .score-purple {
        background-color: #cac4e7;
        color: #271b5e;
        border: 1px solid #271b5e;
    }
    .scroll-button {
        padding: 5px 15px;
        font-size: 14px;
        cursor: pointer;
        text-decoration: none;
        border-radius: 3px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .button-green{
        background-color: #c6e9c6;
        color: #000;
        border: 1px solid #008000;
    }
    .button-orange{
        background-color: #ebdecf;
        color: #000;
        border: 1px solid #FF8C00;
    }
    .button-orange:hover {
        background-color: #623600;
        color: #fff;
        border: none;
    }
    .button-green:hover {
        background-color: #015f01;
        color: #fff;
        border: none;
    }
    .move-to-top {
        padding: 5px 10px;
        font-size: 12px;
        color: #212121;
        border: 1px solid #212121;
        border-radius: 3px;
        cursor: pointer;
        text-decoration: none;
    }
    .move-to-top:hover {
        background-color: #212121;
        color: #fff;
    }
    .legend{
        margin-bottom: 10px;
    }
</style>
</head>
<body>

<div class="container">
    <div class="content">
        <h1>Auto-survey Challenge: Advancing the Frontiers of Automated Literature Review∗</h1>

        <hr>

        <!-- <h2>Scores</h2>
        <table>
            <tr>
                <td><strong>Paper Quality Score:</strong></td>
                <td><span class="score-label score-blue">0.33</span></td>
            </tr>
            <tr>
                <td><strong>LLM Accuracy:</strong></td>
                <td><span class="score-label score-purple">0.23</span></td>
            </tr>
        </table>

        <hr> -->

        <h2>Review Summary</h2>
        <div>
            <span class="legend score-label score-green">&nbsp; &nbsp;</span> The LLM found no significant concern
            <br>
            <span class="legend score-label score-orange">&nbsp; &nbsp;</span> The LLM proposes possible improvements
        </div>
        <table>
            <tr>
              <th>Question</th>
              <th></th>
            </tr>
            
            <tr id="summary-question-1">
                <td>1. Claims</td>
                <td><a href="#question-1" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-2">
                <td>2. Limitations</td>
                <td><a href="#question-2" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-3">
                <td>3. Theoritical assumptions and proofs</td>
                <td><a href="#question-3" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-4">
                <td>4. Experiments reproducibility</td>
                <td><a href="#question-4" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-5">
                <td>5. Code and data accessibility</td>
                <td><a href="#question-5" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-6">
                <td>6. Experimental settings/details</td>
                <td><a href="#question-6" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-7">
                <td>7. Error bars</td>
                <td><a href="#question-7" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-8">
                <td>8. Compute resources</td>
                <td><a href="#question-8" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-9">
                <td>9. NeurIPS code of ethics</td>
                <td><a href="#question-9" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-10">
                <td>10. Impacts</td>
                <td><a href="#question-10" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-11">
                <td>11. Safeguards</td>
                <td><a href="#question-11" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-12">
                <td>12. Credits</td>
                <td><a href="#question-12" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-13">
                <td>13. Documentation</td>
                <td><a href="#question-13" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-14">
                <td>14. Human subjects</td>
                <td><a href="#question-14" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-15">
                <td>15. Risks</td>
                <td><a href="#question-15" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
        </table>

        <!-- Survey -->
        <div>
            <p>After reading through the LLM’s response below, <strong>please complete the following survey about your experience:</strong></p>
            <a class="button" href="https://docs.google.com/forms/d/e/1FAIpQLScr4fjvUGhtiTzBfsqm5CCVvAGafp3sLSSB_Txz2YHhnLiiyw/viewform?usp=pp_url&entry.1830873891=QXV0by1zdXJ2ZXkgQ2hhbGxlbmdlOiBBZHZhbmNpbmcgdGhlIEZyb250aWVycyBvZiBBdXRvbWF0ZWQgTGl0ZXJhdHVyZSBSZXZpZXfiiJc=" target="_blank">Click to submit post submission survey</a>
            <p>Your feedback is critical to evaluate the effectiveness of this LLM checklist assistant!</p>
        </div>

        <hr>

        <h2>Detailed Review</h2>
        
        
        <div class="review" id="question-1">
            <div class="user_input">
                <p><strong class="question">Question # 1:</strong> Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> The abstract and introduction clearly state the main contributions and scope of the paper</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The provided "Answer" and "Justification" lack the necessary specificity and direct linkage to the paper sections that validate the authors' claims made in the abstract and introduction against their contributions and scope as outlined in the NeurIPS "Guidelines". The abstract and introduction set high expectations regarding the innovative evaluation platform for LLMs, but a thorough matching of these claims against the subsequent sections that detail the design, implementation, and evaluation outcomes is absent in the justification.<br><b>Actionable Feedback:</b><br><ol><li> <b>Direct References to Sections:</b> Revise the justification to include precise references to the paper sections that support the claims. For example, specify that the claim about the novel platform is detailed in Section 2 "Challenge Design," and the implementation and evaluation methods supporting this claim are extensively described in Sections 3 "Evaluation methods" and 4 "Baseline Results."</li><br><li> <b>Clarify Scope vs. Contributions:</b> Ensure that the distinction between the paper's scope and its contributions is clear and explicitly addressed in both the abstract/introduction and the justification. The scope refers to the overall aim of the project (e.g., evaluating LLMs' ability to perform literature reviews), while the contributions are specific advancements or findings (e.g., the design of the competition, creation of evaluation criteria, and baseline results).</li><br><li> <b>Highlight Innovations and Limitations:</b> In the justification, briefly highlight how the paper meets its innovative aims (e.g., by describing the autonomous peer-review system for AI-generated literature reviews) and where it might fall short or what limitations exist. Explicitly stating limitations in the justification (and the paper) can fortify the argument that the main claims accurately reflect the paper's contributions and scope.</li><br><li> <b>Incorporate Feedback from Results:</b> Use insights from Section 4 "Baseline Results" and Section 5 "Conclusion" to demonstrate how the paper's contributions have practical implications and future directions for research. This could strengthen the argument that the claims made initially are substantiated by the experimental outcomes and analysis presented.</li><br></ol>By addressing these points, the authors can significantly improve the integrity and the impact of their submission, assuring reviewers and readers that the claims made in the abstract and introduction are robustly supported throughout the paper.<br></div>
            <a class="move-to-top" href="#summary-question-1">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-2">
            <div class="user_input">
                <p><strong class="question">Question # 2:</strong> Does the paper discuss the limitations of the work performed by the authors?</p>
                <p><strong class="answer">Author's answer:</strong> No</p>
                <p><strong class="justification">Author's justification:</strong> in the conclusion it's mentionned room for improvement but not the limitations</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed "Answer" and "Justification" by the authors suggest a discrepancy with the "Guidelines" provided for addressing the question about the discussion of limitations in their work. The authors answer "No" regarding whether the paper discusses the limitations, stating in their justification that the conclusion mentions room for improvement but not the limitations explicitly. This response does not fully align with the expectation set by the "Guidelines" for a comprehensive exploration of limitations, including strong assumptions, scope of claims, factors influencing performance, computational efficiency, and potential issues related to privacy and fairness.<br>Feedback:<br><ol><li> <b>Explicitly Address Limitations:</b> The authors should include a dedicated section or substantially elaborate within the conclusion about the limitations of their work. This should not only mention "room for improvement" but also discuss specific limitations related to the design and implementation of the evaluation framework, any potential biases or deficiencies in the AI models used, and the generalizability of the findings.</li><br><li> <b>Discuss Assumptions and Robustness:</b> The paper should detail any strong assumptions made during the research and the development of the AI-Author and AI-Reviewer tasks. It should also reflect on how robust the results are to violations of these assumptions.</li><br><li> <b>Scope of Claims:</b> The authors should critically evaluate the scope of their claims, especially considering the paper discusses novel platforms for evaluating LLMs in composing and critiquing survey papers. Any limitations on the datasets, disciplines covered, or the nature of prompts used should be clearly stated.</li><br><li> <b>Performance Factors:</b> The paper could benefit from a discussion on factors that might influence the performance of the proposed AI models. For example, the complexity of prompts, diversity of disciplines, and characteristics of the baseline ChatGPT model used should be considered.</li><br><li> <b>Computational and Practical Limitations:</b> An evaluation of computational efficiency and practical limitations of implementing the proposed system at a larger scale or in real-world academic settings should be provided.</li><br><li> <b>Address Potential Ethical Concerns:</b> Given the application of AI in academic content generation and review, the paper should outline potential ethical concerns and limitations, such as privacy issues with data used, potential biases in model output, and the impact on academic integrity.</li><br></ol>By addressing these feedback points, the paper would significantly improve in quality by providing a balanced and comprehensive view of its contributions and limitations, thus adhering to the spirit of transparency and integrity in scientific reporting.<br><br></div>
            <a class="move-to-top" href="#summary-question-2">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-3">
            <div class="user_input">
                <p><strong class="question">Question # 3:</strong> For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This does not apply to our research work because our research is not theoretical.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer of "NA" to the question regarding the provision of full sets of assumptions and complete (and correct) proofs for theoretical results is inconsistent with the content of the actual paper. Although the authors claim their research does not contain theoretical outcomes, thus suggesting no need for proofs or assumptions, the paper does indeed touch upon methodology evaluation, baseline results, and introduces a challenge framework which, by its scientific nature, implies the presence of underlying assumptions in the design and execution of the experiments. Therefore, the claim of the question being not applicable ('NA') seems to overlook the fact that the methodologies and evaluation strategies employed within the paper do have a theoretical underpinning that requires clear articulation of assumptions for the validity and reproducibility of the reported results.<br><h3>Feedback:</h3><ol><li> <b>Clarify Assumptions in Evaluation Methodology:</b> The authors should revisit Section 3 on "Evaluation methods" to clearly articulate the assumptions behind the evaluation criteria (Relevance, Contribution, Soundness, Clarity, Responsibility). Each criterion assumes certain theoretical foundations about what constitutes "good" vs. "bad" survey papers which should be made explicit.</li></ol><ol><li> <b>Provide Justification for AI-Review Criteria Selection:</b> A theoretical justification for the selection of these specific evaluation criteria, their relevance to the challenge, and how they map to the broader objectives of evaluating AI-generated literature would enhance the paper's theoretical grounding.</li><br><li> <b>Articulate Underlying Theories or Models:</b> Although the paper presents itself as application-focused, it implicitly relies on theoretical models of text generation, evaluation, and machine learning. A section discussing these models, even if they are widely accepted or standard within the field, along with any assumptions made, would strengthen the paper.</li><br><li> <b>Reference or Append Formal Methodological Proofs If Applicable:</b> If the challenge design or the AI-Referee-Reviewer algorithm includes novel contributions or adaptations, any theoretical claims or expectations (e.g., the effectiveness of contrastive evaluation) should be backed by appropriate proofs or at least a sketch of the logical or theoretical reasoning supporting such claims.</li><br><li> <b>Redefine Answer to the Checklist Question:</b> Given the presence of algorithmic methodologies and evaluation criteria that have theoretical bearings, the paper cannot accurately be categorized as devoid of theoretical results. Even if formal proofs are not necessary, a comprehensive discussion on the assumptions and rationale behind methodological choices is essential.</li><br></ol>Following these recommendations would not only align the authors' response more accurately with the actual content of the paper but also improve the overall quality and rigor of the manuscript by providing readers with a clearer understanding of the theoretical foundations underpinning the work presented.<br><br></div>
            <a class="move-to-top" href="#summary-question-3">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-4">
            <div class="user_input">
                <p><strong class="question">Question # 4:</strong> Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
                <p><strong class="answer">Author's answer:</strong> No</p>
                <p><strong class="justification">Author's justification:</strong> We don't provide the information to reproduce the experiments, just the information for the challenge.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> Given the content provided within the paper titled "Auto-survey Challenge: Advancing the Frontiers of Automated Literature Review", the proposed author's answer to the reproducibility question appears to lack alignment with the expectations set out by the NeurIPS guidelines for reproducibility. The authors explicitly state a "No" response, suggesting that they have not provided sufficient details to enable reproduction of their main experimental results. To conform to the NeurIPS guidelines and improve the paper's adherence to reproducibility standards, the following itemized, actionable feedback is provided:<br><ol><li> <b>Detailed Description of Algorithms and Models</b>: To enhance reproducibility, authors should provide a clear and comprehensive description of the algorithms and models employed, including the version of ChatGPT used (e.g., GPT-3.5) as explicitly mentioned. However, more specifics on the configuration, parameter settings, and any modifications or tuning carried out for the competition should be included.</li><br><li> <b>Data Availability and Processing Steps</b>: Although the paper does not directly mention the use of a dataset, generating prompts and evaluating AI-generated survey papers likely involves initial datasets or prompt generation methods and evaluation benchmarks. The authors should describe these datasets, the prompt generation process, and any preprocessing steps. If any datasets are proprietary or unavailable, this should be explicitly stated with reasons.</li><br><li> <b>Exact Evaluation Criteria and Metrics</b>: The paper outlines the evaluation framework based on five key metrics but lacks explicit details on how these metrics are quantitatively assessed. Providing definitions, thresholds, or scoring techniques for "Relevance," "Contribution," "Soundness," "Clarity," and "Responsibility" would significantly enhance reproducibility. Additionally, details on the construction of "good" and "bad" versions of papers for contrastive evaluation are necessary.</li><br><li> <b>Baseline and Reference Implementations</b>: The presence of baseline implementations, such as the AI-Author and AI-Referee-Reviewer, is beneficial. Yet, to ensure reproducibility, the authors should offer access to these baseline codes or detailed algorithms that allow readers to reimplement them. If releasing the code is not possible, a step-by-step guide or pseudo-code that outlines the implementations would be a valuable substitute.</li><br><li> <b>Accessibility and Ethical Considerations</b>: When discussing models and datasets, the authors should address any ethical considerations related to the content generated by AI models and the use of datasets, including but not limited to biases, privacy, and consent if applicable.</li><br><li> <b>Supplementary Materials</b>: Consider adding supplementary materials or appendices that provide in-depth methodological details or additional results that could not be included in the main paper due to space constraints. This can include but is not limited to, extended tables, figures, or experiment details that further elucidate the study's findings and reproducibility.</li><br></ol>By addressing these points, the authors will significantly improve the paper's clarity regarding experimental reproducibility, aligning with the expectations for contributions to the NeurIPS conference and enhancing the overall quality and impact of the paper.<br><br></div>
            <a class="move-to-top" href="#summary-question-4">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-5">
            <div class="user_input">
                <p><strong class="question">Question # 5:</strong> Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
                <p><strong class="answer">Author's answer:</strong> No</p>
                <p><strong class="justification">Author's justification:</strong> The paper does not provides the link to open access to all relevant data and code</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer to the checklist question regarding the provision of open access to data and code is “No,” with the justification that the paper does not provide links to open access for all relevant data and code.<br>Upon thorough analysis of the provided paper content alongside the guidelines for the NeurIPS conference regarding open access to data and code for reproducibility, the following key points and discrepancies are identified:<br><ol><li> The paper outlines the challenge design including the evaluation methods (Section 3) and baseline results (Section 4) but does not include explicit information or links to the data and code base used for the AI-Author and AI-Reviewer tasks. According to the NeurIPS guidelines, the paper should ideally facilitate reproduction of the results through open access to data and code unless there is a valid justification for not doing so.</li><br><li> While the authors provide a detailed methodology and baseline results, there's an absence of direct instructions, scripts, or environment details necessary for reproducing experiments. The NeurIPS guidelines emphasize the importance of including exact commands and environments needed.</li><br><li> The acknowledgment section and the proposal for future tutorials and hackathons suggest that the authors are keen on community engagement and dissemination of knowledge. However, the current absence of data and code accessibility limits such engagement’s potential effectiveness and the reproducibility of the paper's findings.</li><br></ol>Actionable Feedback:<br><ol><li> <b>Provide Access or Detailed Justification:</b> If possible, offer anonymized versions of the data and the code used in the challenges, along with sufficient instructions for reproduction in the supplementary material. If there are privacy, proprietary, or ethical constraints that prevent open access, these should be explicitly stated to justify the absence of shared data and code.</li><br><li> <b>Supplementary Material Detailing Reproduction Steps:</b> In the absence of direct data and code sharing, authors could enhance the supplementary material with a more detailed description of the methodology, including pseudo-code, algorithmic specifications, and the computational environment (libraries, versions, hardware specifics) to aid replication efforts with alternative datasets.</li><br><li> <b>Link to External Repositories (if applicable and possible):</b> Consider providing links to external repositories (GitHub, etc.) with anonymized or synthetic data examples and code (while preserving double-blind review guidelines). Authors could update the paper or supplementary material with specific URLs post-review, maintaining anonymity during the review process.</li><br><li> <b>Clarification on Data and Code Accessibility in Conclusion:</b> Enhance the conclusion section by discussing the implications of data and code accessibility on the validity and reproducibility of the presented challenge and its results. This could involve a reflection on the feasibility and avenues for interested researchers to engage with and build upon the presented work.</li><br><br></ol></div>
            <a class="move-to-top" href="#summary-question-5">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-6">
            <div class="user_input">
                <p><strong class="question">Question # 6:</strong> Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> The testing details is mentioned in the baseline result</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors assert that the paper specifies all necessary training and test details to understand the results, referencing the baseline results section for this information. However, upon examination, the "Baseline Results" section primarily discusses the evaluation of AI-Author and AI-Reviewer tasks baselines with only limited information provided on the experiment setup specifics. Key training and testing details such as data splits, hyperparameters, reasoning behind their choice, and type of optimizer are not explicitly discussed in the cited sections or elsewhere in the paper. To improve the paper according to the guidelines provided:<br><ol><li> <b>Specify Data Splits</b>: Clearly outline how the data was divided for training, validation, and testing phases. If the data splits are unconventional or designed specifically for the challenge, provide rationale for such choices.</li><br><li> <b>Detail Hyperparameters</b>: Include a comprehensive list of hyperparameters used for the ChatGPT baseline and AI-Referee-Reviewer, such as learning rates, batch sizes, and any model-specific parameters. Moreover, discuss the process or criteria used to select these hyperparameters, whether they were obtained through grid search, literature best practices, or another methodology.</li><br><li> <b>Type of Optimizer and its Rationale</b>: Elucidate on the type of optimizer employed in the baseline models. Provide justification for its selection over other potential choices, based on either literature review or experimental validation specific to the challenge's tasks.</li><br><li> <b>Expand Justification Provided</b>: While the baseline result section does offer insights into the evaluation of submissions, comprehensive details relevant to understanding and replicating the experimental results are missing. Enhance this section by incorporating the aforementioned details directly into the paper or appendices, ensuring the reader has full transparency on how results were achieved.</li><br><li> <b>Supplemental Materials</b>: If space constraints limit the amount of detail that can be provided in the main paper body, consider including thorough appendices or supplemental materials hosted on a platform like arXiv or the conference's supplementary material submission option. These should include detailed configurations, code snippets, or even pseudocode for critical implementation aspects.</li><br><li> <b>Clarification on Evaluation Framework</b>: While the evaluation methods for AI-authored and AI-reviewed papers are described, it's crucial to detail the specific setup used for these evaluations, including any computational platforms, software versions, or external libraries leveraged. This is particularly important for ensuring replicability and understanding the constraints under which the evaluation was conducted.</li><br></ol>By addressing these points, the paper can significantly improve its compliance with the NeurIPS guidelines concerning the detailing of experiments, thereby offering a robust foundation for readers to appreciate, replicate, or build upon the reported work.<br><br></div>
            <a class="move-to-top" href="#summary-question-6">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-7">
            <div class="user_input">
                <p><strong class="question">Question # 7:</strong> Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Error bars are properly reported with clear definitions, capturing variability factors, and the methodology for their calculation is thoroughly explained, supporting the paper's main experimental claims.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors stated that error bars were correctly reported and well-defined within the paper, supporting the main experimental claims. However, upon review of the paper's content, there is no explicit mention or display of error bars, confidence intervals, or statistical tests that would allow for an assessment of the statistical significance of the experimental results presented. The sections that would conventionally contain this information (e.g., Section 4: Baseline Results) discuss results and comparisons between different baseline approaches using scores and visual representations like figures. Still, they lack the explicit inclusion of error bars or any statistical measures of variability or significance.<br>Feedback to improve paper quality:<ol><li> Clarify and Include Statistical Measures: The paper should explicitly state and include error bars, confidence intervals, or the results of statistical significance tests for the experimental results. This would involve revisiting the experimental data and calculating these statistics as per the norms in the field.</li><li> Methodological Details: Provide a subsection or paragraph in the methodology section (or wherever appropriate, such as within Section 4: Baseline Results) detailing the statistical methods used to derive error bars or confidence intervals. This should include the assumptions made (e.g., normality of data distribution) and the specific formulas or statistical software/functions used.</li><li> Referencing Figures and Tables: If error bars or statistical measures are reported in figures or tables, explicitly reference these in the text, explaining how these values were calculated and what they signify about the variability and reliability of the results.</li><li> Address Variability Factors: Identify and clearly describe the factors of variability considered in the experimental design and how they are captured by the statistical measures provided (e.g., between different runs of the models, variability due to differences in prompts, or other experimental conditions).</li><li> Improve Presentation of Results: Alongside the error bars or confidence intervals, provide a nuanced discussion in the text on what these measures imply for the reliability and significance of the reported findings, especially in supporting the paper's main claims.</li><br></ol>The inclusion of these details would greatly enhance the paper's adherence to scientific standards for reporting experimental results, providing clearer insight into the significance and reliability of the findings.<br><br></div>
            <a class="move-to-top" href="#summary-question-7">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-8">
            <div class="user_input">
                <p><strong class="question">Question # 8:</strong> For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
                <p><strong class="answer">Author's answer:</strong> No</p>
                <p><strong class="justification">Author's justification:</strong> The paper does not provide any type of information regarding the computer resources</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed author's answer states that the paper does not provide any information regarding computer resources used for the experiments, which is consistent with the content described in the "Paper" section. The response complies with the guidelines for a "No" answer by offering a clear justification. However, to improve the quality of the paper and adhere to NeurIPS guidelines more closely, consider the following itemized, actionable feedback:<br><ol><li> <b>Introduce a new subsection in the methodology or in the experimental setup sections:</b> This subsection should detail the computational resources used. Include information on the type of compute workers (CPU/GPU), specific models if applicable, memory, runtime for each experiment, and whether these experiments were run on an internal cluster or a cloud provider. If multiple configurations were used, delineate the resources per each significant experiment or task (e.g., AI-Author Task, AI-Reviewer Task).</li><br><li> <b>Estimate the total compute required:</b> Given that this paper introduces a novel platform evaluating LLMs across tasks, estimating the overall compute in terms of GPU/CPU hours or by other means would provide clarity on the scalability and accessibility of reproducing such a framework.</li><br><li> <b>Discuss the compute beyond the reported experiments:</b> If applicable, discuss whether the full research project required more compute than the experiments reported. This includes the computation for preliminary tests, failed experiments, or data preprocessing steps that didn't make it into the final paper but were essential for the experimental setup.</li><br><li> <b>Rationalize the non-disclosure, if any, of certain computational details:</b> If there are valid reasons for not disclosing specific details about the compute resources—such as proprietary information restrictions, nondisclosure agreements, or if the compute resources are standard and well-known within the community—providing this context can help readers understand the constraints.</li><br><li> <b>Enhance the reproducibility:</b> Apart from just disclosing the compute resources, consider providing configurations, scripts, or environment details that helped run these experiments. Sharing code in a supplementary material or through a well-documented repository can significantly enhance reproducibility and adherence to NeurIPS guidelines.</li><br></ol>By addressing these points, the authors can substantially enhance the quality and reproducibility of the paper, aligning it more closely with both the NeurIPS guidelines and the broader scientific community's expectations for experimental transparency and reproducibility.<br><br></div>
            <a class="move-to-top" href="#summary-question-8">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-9">
            <div class="user_input">
                <p><strong class="question">Question # 9:</strong> Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We review the code of ethics and our paper conform with it.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed author's answer asserts compliance with the NeurIPS Code of Ethics without specifying where in the paper these considerations are addressed. This approach is insufficient under NeurIPS guidelines, which require specific sections to be pointed out, especially concerning ethical adherence. There are several areas within the paper where ethical considerations inherently arise, notably in the usage of Large Language Models (LLMs) for generating survey papers and evaluating them. The implications of AI in academic research, the potential for AI-generated misinformation, and the measures taken to ensure the AI's outputs are responsible, need direct addressing in relation to ethical guidelines.<br>Feedback:<br><ol><li> <b>Specify Sections for Ethical Considerations</b>: Revise the justification to include specific sections of the paper where ethical considerations, especially those related to the use of LLMs in academic settings, are discussed. If such sections are currently absent, they should be explicitly added to the paper.</li><br><li> <b>AI-generated Content Scrutiny</b>: Given the paper presents a framework for AI-generated academic reviews, it's important to discuss the integrity of AI-generated content, potential biases in language models, and measures in place to mitigate these biases, aligning with ethical guidelines.</li><br><li> <b>Ethical Implications of Automated Review Systems</b>: Add a section discussing the ethical implications of using AI for peer review, including the potential impact on the academic publication process, information dissemination, and trustworthiness.</li><br><li> <b>Data Privacy and Consent</b>: If any real data (e.g., existing survey papers, reviewer comments) were used to train or validate the AI models, the paper should discuss how data privacy was ensured and consent obtained, if applicable.</li><br><li> <b>Transparency in AI Assessments</b>: Incorporate a discussion on the transparency of the AI assessment process, including how the models make decisions and the availability of these models for scrutiny, to adhere to ethical principles of transparency and accountability.</li><br><li> <b>Impact Assessment</b>: Consider adding an ethical impact assessment, evaluating potential negative outcomes of deploying such AI systems in academic settings and strategies to mitigate these outcomes.</li><br></ol>By addressing these points, the paper will provide a comprehensive view of how the research aligns with ethical guidelines, substantiating the authors' claim of compliance with the NeurIPS Code of Ethics.<br><br></div>
            <a class="move-to-top" href="#summary-question-9">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-10">
            <div class="user_input">
                <p><strong class="question">Question # 10:</strong> Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
                <p><strong class="answer">Author's answer:</strong> No</p>
                <p><strong class="justification">Author's justification:</strong> The paper comprehensively discusses the potential positive and limitation but not the negative societal impacts</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors incorrectly stated that their paper does not discuss the negative societal impacts of the work performed, yet the content and implications of their work inherently carry potential negative impacts, especially considering the uses of Large Language Models (LLMs) in generating academic content which could include misuse such as generating misleading information or academic dishonesty. The justification provided does not align with the guidelines which require discussing both potential positive and negative societal impacts if applicable.<br>Actionable Feedback:<ol><li> <b>Explicitly Address Negative Societal Impacts</b>: Amend the text to include a dedicated section or paragraph that discusses potential negative societal impacts of the use of LLMs in automating literature reviews. This could include the creation of misinformation, misuse in academic settings to generate non-original research, and the ethical considerations around replacing human intellectual labor with AI.</li><br><li> <b>Consideration of Mitigation Strategies</b>: After identifying potential negative impacts, the authors should propose mitigation strategies. This could include methods to verify the authenticity and originality of the generated content, ethical guidelines for the use of such technology, or the development of detection tools for automated vs. human-created content.</li><br><li> <b>Balance in Discussion</b>: Ensure that the discussion of societal impacts is balanced, offering insights into both the positive aspects (as already present) and the newly addressed negative consequences. This will provide a more nuanced view of the technology's role and potential effects on society.</li><br><li> <b>Citations and References</b>: Incorporate references to existing literature on the ethical and societal implications of LLMs and AI in scholarly work. This not only strengthens the paper's credibility but also situates the discussion within the broader discourse on AI ethics and governance.</li><br><li> <b>Clarify Justification</b>: Revise the justification in the checklist to accurately reflect the paper’s content. If negative impacts are discussed following the feedback, the justification should be updated to indicate where in the paper these discussions can be found.</li><br></ol>These changes would greatly improve the quality of the paper by providing a comprehensive view of the technology's potential societal impacts, in line with the conference guidelines for full consideration of ethical implications.<br><br></div>
            <a class="move-to-top" href="#summary-question-10">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-11">
            <div class="user_input">
                <p><strong class="question">Question # 11:</strong> Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> Our models do not present such risks.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> While the authors have argued that their models do not present such risks for misuse and thus answered "NA" to the checklist question regarding safeguards for responsible release, this answer might not fully consider the broader implications of LLM application in automated literature review tasks. Given the nature of the paper, which involves the use of Large Language Models (LLMs) for generating academic content, several potential issues deserve attention:<br><ol><li> <b>Ethical implications</b>: The use of LLMs in academic settings, especially for composing literature reviews or critique, inherently bears the risk of generating content that could be incorrect or misleading. This is particularly relevant as the models might synthesize information from a vast pool of data that could include biased or unverified sources.</li><br><li> <b>Misuse potential</b>: Given that the paper discusses the development and use of AI systems for generating survey papers, there is a potential for misuse in creating fraudulent or misleading academic content. This could impact the integrity of academic literature and scholarly communications.</li><br><li> <b>Data provenance and scraping</b>: While the paper does not explicitly mention the origin of the data used to train the models, the potential use of scraped datasets for training LLMs introduces risks related to privacy, copyright, and data safety. This is especially relevant if the datasets include academic papers or materials that are not open access or freely available.</li><br></ol>To address these concerns and improve the paper's adherence to the NeurIPS checklist guidelines:<br><ul><li><b>Add a section on Ethical Considerations</b>: It would be beneficial to include a detailed discussion about the ethical implications of using LLMs in generating and critiquing academic content. This section should assess potential biases in model outputs and strategies to mitigate them.</li><br><li><b>Discuss Misuse and Safeguards</b>: Even if the authors believe their specific application does not present risks of misuse, a discussion on the general potential for misuse of similar technologies in academic literature creation and review, alongside proposed or implemented safeguards, would strengthen the paper.</li><br><li><b>Clarify Data Provenance</b>: If any part of the model training involved datasets that could raise safety risks, the paper should discuss how these risks were mitigated. This includes clarifying the sources of training data, any scraping methods used, and compliance with data use agreements or licenses.</li><br><li><b>Implementation of Safety Filters</b>: As part of responsible AI development, the authors could explore the implementation of safety filters that prevent the generation of harmful, biased, or misleading content.</li><br></ul>These improvements would not only align the paper more closely with responsible AI practices but also enhance the transparency and accountability of the research presented. The authors should consider these aspects even if their initial assessment led them to answer "NA" to the checklist question.<br></div>
            <a class="move-to-top" href="#summary-question-11">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-12">
            <div class="user_input">
                <p><strong class="question">Question # 12:</strong> Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
                <p><strong class="answer">Author's answer:</strong> No</p>
                <p><strong class="justification">Author's justification:</strong> The paper doesn't mention the name of the licence</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors of the paper "Auto-survey Challenge: Advancing the Frontiers of Automated Literature Review" state that they did not mention the name of the license for the assets used in their work (i.e., code, data, models). However, there appears to be no explicit mention or discussion in the provided text regarding efforts to credit original owners or clarify the licensing and terms of use for the assets exploited in their research.<br><b>Actionable Feedback:</b><br><ol><li> <b>Cite Original Sources:</b> Ensure all external assets (datasets, code libraries, or models) are accompanied by proper citations in the text or bibliography. If you derived your work from existing assets, credit the original work explicitly.</li><br><li> <b>License Information:</b> Specify the license under which each external asset is distributed. This includes the version of datasets, code libraries, or models used in your research. If the assets are publicly available software or datasets, include direct references to their source and the specific licenses they are distributed under. For instance, if GPT-3.5 was used, mention the OpenAI's terms of use or license explicitly.</li><br><li> <b>Dataset Versions and URLs:</b> When mentioning the usage of specific versions of software or datasets, such as ChatGPT-3.5, ensure to include version numbers and, if possible, a URL or DOI to where the asset can be accessed. This helps in ensuring reproducibility and verifying the terms of use compliance.</li><br><li> <b>Respect for Terms of Use:</b> Clearly state how the terms of use for each asset were followed in the context of your research. This is crucial when dealing with web-scraped data or proprietary models. If modifications were made to existing assets, clarify the nature of these changes and ensure they comply with the original terms of use.</li><br><li> <b>Include an Ethics Statement:</b> If any of the datasets or codes were custom-developed or involved proprietary or sensitive information, include an ethics statement in the paper discussing data collection methods, privacy considerations, and compliance with relevant regulations (GDPR, etc.).</li><br><li> <b>Check Additional Resources:</b> For widely used datasets or models, consult curated repositories or platforms like paperswithcode.com, which often list licensing information that may not be easily found elsewhere. This could help in accurately reporting the licenses.</li><br><li> <b>Provide Supplementary Material:</b> If detailed discussions on licensing and asset accreditation clutter the main text, consider including this information in a supplementary material section or appendix. This preserves the readability of the paper while ensuring full transparency and reproducibility.</li><br></ol>By addressing these points, the authors can significantly improve the legal and ethical credibility of their paper, ensuring respectful and transparent use of third-party assets.<br></div>
            <a class="move-to-top" href="#summary-question-12">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-13">
            <div class="user_input">
                <p><strong class="question">Question # 13:</strong> Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> We don't introduce the new assents.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> Feedback:<br><ol><li> The authors claim that they do not introduce new assets, which conflicts with the description of implementing baselines, methods of evaluations, and generating "good" and "bad" versions of papers for evaluation in Sections 2 and 3. These components can be considered as assets developed during the research.</li><br><li> The paper needs to clarify the documentation and availability of these assets, particularly:</li></ol>- The baseline implementation based on ChatGPT for AI-Author tasks.- The AI-Referee-Reviewer for evaluating submissions.- The generated dataset of "good" and "bad" versions of survey papers, used for the evaluation of AI-Reviewers.<br><ol><li> For each asset mentioned, include:</li></ol>- A separate section or appendix detailing their design, implementation, and any limitations or considerations.- Clear instructions on how to access these assets, including URLs for any repositories, anonymized or not, depending on conference submission policies. If assets are not directly accessible, provide a justification.- Information on licensing or any usage restrictions.<br><ol><li> Ensure that consent or authorization is documented for any data or pre-existing assets used in creating these new assets.</li><br><li> Clarify whether any of these assets, such as the AI models or generated datasets, will be made available to other researchers. If so, detail the conditions under which they can be used.</li><br><li> If anonymization is required for submission, confirm that any links or references to these assets in the paper are also anonymized.</li><br></ol>These improvements will enhance the reproducibility and transparency of the research, aligning the paper with best practices for asset documentation and sharing within the research community.<br><br></div>
            <a class="move-to-top" href="#summary-question-13">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-14">
            <div class="user_input">
                <p><strong class="question">Question # 14:</strong> For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This does not apply on our research as we are not doing any crowdsourcing experiments</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer is marked as NA with the justification that their research does not involve crowdsourcing experiments or research with human subjects. However, upon a careful examination of the "Paper" content, it's evident that while the study primarily focuses on the application of Large Language Models (LLMs) for automatically composing and critiquing academic surveys, there is a mention of a competition organized as part of the AutoML conference 2023, implying potential involvement or interaction with human participants in areas such as competition organization, baseline submissions, evaluations, and the implementation of a simulated peer-review mechanism.<br>Despite the primary emphasis on AI and machine learning models, the inclusion of a human jury for the final evaluation phase and human organizers in an editorial oversight capacity could warrant further clarification regarding whether any human subject research principles were considered, especially concerning compensation and guidelines for ethical treatment, even if those humans are not the primary data source or subjects of the research.<br>Given the ambiguity and potential oversight regarding the involvement of human participants in the competition and editorial processes, which indirectly contributes to the research outcomes, the following itemized, actionable feedback is provided:<br><ol><li> Clarify the role of human participants within the context of the AutoML conference competition, especially in the final evaluation phase conducted by a human jury and any other phases where human judgment or interaction might have been necessary.</li><li> If human participants were involved in any capacity beyond what's traditionally considered "research subjects," such as contributing intellectual labor for critical evaluation, organizing, or any form of data interpretation, explicitly acknowledge this involvement and detail any ethical considerations, including compensation or volunteer acknowledgment.</li><li> In the case where human involvement was strictly limited to oversight or organizational roles without direct interaction or data collection related to crowdsourcing or subject research, explicitly state these details to eliminate ambiguity and ensure the justification for NA is fully grounded in the context of the guidelines provided.</li><br><br></ol></div>
            <a class="move-to-top" href="#summary-question-14">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-15">
            <div class="user_input">
                <p><strong class="question">Question # 15:</strong> Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This does not concerns us as we are not dealing with human subjects in our research.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The provided answer "NA" with the justification that the research does not involve human subjects seems primarily appropriate given the paper's description of a computational challenge designed for Large Language Models (LLMs). However, while direct human subjects may not be involved, the use of human-generated content, ethical implications of AI-generated literature, and potential biases in LLM outputs could be considered as indirect factors affecting broader societal contexts or groups. These considerations are especially relevant given NeurIPS's emphasis on ethical responsibility.<br><h3>Actionable Feedback</h3><ol><li> <b>Clarify the Scope of 'Human Subjects'</b>: The authors might consider expanding their justification to explicitly state that their research's primary subjects are AI models, not humans, yet briefly acknowledge any indirect human-related ethical considerations given the nature of AI work.</li></ol><ol><li> <b>Ethical Implications of AI-generated Content</b>: Given the platform's intent to evaluate LLM capabilities in academic literature generation, it would be prudent for the authors to discuss or at least acknowledge potential ethical implications. This includes but is not limited to the propagation of biases, the integrity of academic discourse, and the potential misuse of autonomously generated content. A short section or paragraph addressing these aspects could enhance the paper's consideration of broader ethical impacts.</li><br><li> <b>Potential Biases in Model Outputs</b>: The paper should address how biases in training data for LLMs and resultant outputs were considered or mitigated. This is important as biases in AI outputs can present risks to societal groups, even if not directly researched with human subjects. A brief overview of steps taken or considerations made regarding data diversity and bias mitigation strategies would be beneficial.</li><br><li> <b>Institutional Review Board (IRB) Considerations for AI Research</b>: While IRB approval may not be directly applicable, discussing any consultations or ethical review processes undertaken, even informally, could provide transparency. If no such processes were involved, simply stating this fact provides clarity to readers about the ethical oversight of the research.</li><br><li> <b>Reference to AI Ethics Guidelines</b>: Briefly linking the research approach to broader AI ethics guidelines, potentially including those suggested by NeurIPS or other relevant entities, would demonstrate due diligence in considering the ethical landscape of AI research.</li><br></ol>By addressing these points, the authors could significantly strengthen the ethical considerations section of their paper, demonstrating a thorough understanding and proactive stance on the potential indirect impacts of their research on human subjects and society at large. This would align with NeurIPS's emphasis on ethical responsibility in AI research.<br></div>
            <a class="move-to-top" href="#summary-question-15">↑ Back to summary</a>
        </div>
        

        <!-- Survey -->
        <div>
            <p>Please complete the following survey about your experience:</p>
            <a class="button" href="https://docs.google.com/forms/d/e/1FAIpQLScr4fjvUGhtiTzBfsqm5CCVvAGafp3sLSSB_Txz2YHhnLiiyw/viewform?usp=pp_url&entry.1830873891=QXV0by1zdXJ2ZXkgQ2hhbGxlbmdlOiBBZHZhbmNpbmcgdGhlIEZyb250aWVycyBvZiBBdXRvbWF0ZWQgTGl0ZXJhdHVyZSBSZXZpZXfiiJc=" target="_blank">Click to submit post submission survey</a>
            <p>Your feedback is critical to evaluate the effectiveness of this LLM checklist assistant!</p>
        </div>
        
    </div>
</div>

</body>
</html>