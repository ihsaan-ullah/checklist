Question,Question_Title,Answer,Justification,Guidelines,Review,Score
Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?,Claims,Yes,The abstract and introduction clearly state the main contributions and scope of the paper,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the abstract and introduction do not include the claims made in the paper.
The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
","The provided ""Answer"" and ""Justification"" lack the necessary specificity and direct linkage to the paper sections that validate the authors' claims made in the abstract and introduction against their contributions and scope as outlined in the NeurIPS ""Guidelines"". The abstract and introduction set high expectations regarding the innovative evaluation platform for LLMs, but a thorough matching of these claims against the subsequent sections that detail the design, implementation, and evaluation outcomes is absent in the justification. 

**Actionable Feedback:**

1. **Direct References to Sections:** Revise the justification to include precise references to the paper sections that support the claims. For example, specify that the claim about the novel platform is detailed in Section 2 ""Challenge Design,"" and the implementation and evaluation methods supporting this claim are extensively described in Sections 3 ""Evaluation methods"" and 4 ""Baseline Results.""

2. **Clarify Scope vs. Contributions:** Ensure that the distinction between the paper's scope and its contributions is clear and explicitly addressed in both the abstract/introduction and the justification. The scope refers to the overall aim of the project (e.g., evaluating LLMs' ability to perform literature reviews), while the contributions are specific advancements or findings (e.g., the design of the competition, creation of evaluation criteria, and baseline results).

3. **Highlight Innovations and Limitations:** In the justification, briefly highlight how the paper meets its innovative aims (e.g., by describing the autonomous peer-review system for AI-generated literature reviews) and where it might fall short or what limitations exist. Explicitly stating limitations in the justification (and the paper) can fortify the argument that the main claims accurately reflect the paper's contributions and scope.

4. **Incorporate Feedback from Results:** Use insights from Section 4 ""Baseline Results"" and Section 5 ""Conclusion"" to demonstrate how the paper's contributions have practical implications and future directions for research. This could strengthen the argument that the claims made initially are substantiated by the experimental outcomes and analysis presented.

By addressing these points, the authors can significantly improve the integrity and the impact of their submission, assuring reviewers and readers that the claims made in the abstract and introduction are robustly supported throughout the paper.

**",0.5
Does the paper discuss the limitations of the work performed by the authors?,Limitations,No,in the conclusion it's mentionned room for improvement but not the limitations,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
The authors are encouraged to create a separate ""Limitations"" section in their paper.
The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
","The proposed ""Answer"" and ""Justification"" by the authors suggest a discrepancy with the ""Guidelines"" provided for addressing the question about the discussion of limitations in their work. The authors answer ""No"" regarding whether the paper discusses the limitations, stating in their justification that the conclusion mentions room for improvement but not the limitations explicitly. This response does not fully align with the expectation set by the ""Guidelines"" for a comprehensive exploration of limitations, including strong assumptions, scope of claims, factors influencing performance, computational efficiency, and potential issues related to privacy and fairness.

Feedback:

1. **Explicitly Address Limitations:** The authors should include a dedicated section or substantially elaborate within the conclusion about the limitations of their work. This should not only mention ""room for improvement"" but also discuss specific limitations related to the design and implementation of the evaluation framework, any potential biases or deficiencies in the AI models used, and the generalizability of the findings.

2. **Discuss Assumptions and Robustness:** The paper should detail any strong assumptions made during the research and the development of the AI-Author and AI-Reviewer tasks. It should also reflect on how robust the results are to violations of these assumptions.

3. **Scope of Claims:** The authors should critically evaluate the scope of their claims, especially considering the paper discusses novel platforms for evaluating LLMs in composing and critiquing survey papers. Any limitations on the datasets, disciplines covered, or the nature of prompts used should be clearly stated.

4. **Performance Factors:** The paper could benefit from a discussion on factors that might influence the performance of the proposed AI models. For example, the complexity of prompts, diversity of disciplines, and characteristics of the baseline ChatGPT model used should be considered.

5. **Computational and Practical Limitations:** An evaluation of computational efficiency and practical limitations of implementing the proposed system at a larger scale or in real-world academic settings should be provided.

6. **Address Potential Ethical Concerns:** Given the application of AI in academic content generation and review, the paper should outline potential ethical concerns and limitations, such as privacy issues with data used, potential biases in model output, and the impact on academic integrity.

By addressing these feedback points, the paper would significantly improve in quality by providing a balanced and comprehensive view of its contributions and limitations, thus adhering to the spirit of transparency and integrity in scientific reporting.

",0.5
"For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?",Theoritical assumptions and proofs,Not Applicable,This does not apply to our research work because our research is not theoretical.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include theoretical results. 
All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced. 
","The authors' proposed answer of ""NA"" to the question regarding the provision of full sets of assumptions and complete (and correct) proofs for theoretical results is inconsistent with the content of the actual paper. Although the authors claim their research does not contain theoretical outcomes, thus suggesting no need for proofs or assumptions, the paper does indeed touch upon methodology evaluation, baseline results, and introduces a challenge framework which, by its scientific nature, implies the presence of underlying assumptions in the design and execution of the experiments. Therefore, the claim of the question being not applicable ('NA') seems to overlook the fact that the methodologies and evaluation strategies employed within the paper do have a theoretical underpinning that requires clear articulation of assumptions for the validity and reproducibility of the reported results.

### Feedback:
1. **Clarify Assumptions in Evaluation Methodology:** The authors should revisit Section 3 on ""Evaluation methods"" to clearly articulate the assumptions behind the evaluation criteria (Relevance, Contribution, Soundness, Clarity, Responsibility). Each criterion assumes certain theoretical foundations about what constitutes ""good"" vs. ""bad"" survey papers which should be made explicit.
  
2. **Provide Justification for AI-Review Criteria Selection:** A theoretical justification for the selection of these specific evaluation criteria, their relevance to the challenge, and how they map to the broader objectives of evaluating AI-generated literature would enhance the paper's theoretical grounding.

3. **Articulate Underlying Theories or Models:** Although the paper presents itself as application-focused, it implicitly relies on theoretical models of text generation, evaluation, and machine learning. A section discussing these models, even if they are widely accepted or standard within the field, along with any assumptions made, would strengthen the paper.

4. **Reference or Append Formal Methodological Proofs If Applicable:** If the challenge design or the AI-Referee-Reviewer algorithm includes novel contributions or adaptations, any theoretical claims or expectations (e.g., the effectiveness of contrastive evaluation) should be backed by appropriate proofs or at least a sketch of the logical or theoretical reasoning supporting such claims.

5. **Redefine Answer to the Checklist Question:** Given the presence of algorithmic methodologies and evaluation criteria that have theoretical bearings, the paper cannot accurately be categorized as devoid of theoretical results. Even if formal proofs are not necessary, a comprehensive discussion on the assumptions and rationale behind methodological choices is essential.

Following these recommendations would not only align the authors' response more accurately with the actual content of the paper but also improve the overall quality and rigor of the manuscript by providing readers with a clearer understanding of the theoretical foundations underpinning the work presented.

",0.5
Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?,Experiments reproducibility,No,"We don't provide the information to reproduce the experiments, just the information for the challenge.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
    - If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
    - If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
    - If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
    - We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

","Given the content provided within the paper titled ""Auto-survey Challenge: Advancing the Frontiers of Automated Literature Review"", the proposed author's answer to the reproducibility question appears to lack alignment with the expectations set out by the NeurIPS guidelines for reproducibility. The authors explicitly state a ""No"" response, suggesting that they have not provided sufficient details to enable reproduction of their main experimental results. To conform to the NeurIPS guidelines and improve the paper's adherence to reproducibility standards, the following itemized, actionable feedback is provided:

1. **Detailed Description of Algorithms and Models**: To enhance reproducibility, authors should provide a clear and comprehensive description of the algorithms and models employed, including the version of ChatGPT used (e.g., GPT-3.5) as explicitly mentioned. However, more specifics on the configuration, parameter settings, and any modifications or tuning carried out for the competition should be included.

2. **Data Availability and Processing Steps**: Although the paper does not directly mention the use of a dataset, generating prompts and evaluating AI-generated survey papers likely involves initial datasets or prompt generation methods and evaluation benchmarks. The authors should describe these datasets, the prompt generation process, and any preprocessing steps. If any datasets are proprietary or unavailable, this should be explicitly stated with reasons.

3. **Exact Evaluation Criteria and Metrics**: The paper outlines the evaluation framework based on five key metrics but lacks explicit details on how these metrics are quantitatively assessed. Providing definitions, thresholds, or scoring techniques for ""Relevance,"" ""Contribution,"" ""Soundness,"" ""Clarity,"" and ""Responsibility"" would significantly enhance reproducibility. Additionally, details on the construction of ""good"" and ""bad"" versions of papers for contrastive evaluation are necessary.

4. **Baseline and Reference Implementations**: The presence of baseline implementations, such as the AI-Author and AI-Referee-Reviewer, is beneficial. Yet, to ensure reproducibility, the authors should offer access to these baseline codes or detailed algorithms that allow readers to reimplement them. If releasing the code is not possible, a step-by-step guide or pseudo-code that outlines the implementations would be a valuable substitute.

5. **Accessibility and Ethical Considerations**: When discussing models and datasets, the authors should address any ethical considerations related to the content generated by AI models and the use of datasets, including but not limited to biases, privacy, and consent if applicable. 

6. **Supplementary Materials**: Consider adding supplementary materials or appendices that provide in-depth methodological details or additional results that could not be included in the main paper due to space constraints. This can include but is not limited to, extended tables, figures, or experiment details that further elucidate the study's findings and reproducibility.

By addressing these points, the authors will significantly improve the paper's clarity regarding experimental reproducibility, aligning with the expectations for contributions to the NeurIPS conference and enhancing the overall quality and impact of the paper.

",0
"Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?",Code and data accessibility,No,The paper does not provides the link to open access to all relevant data and code,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
","The authors' proposed answer to the checklist question regarding the provision of open access to data and code is “No,” with the justification that the paper does not provide links to open access for all relevant data and code. 

Upon thorough analysis of the provided paper content alongside the guidelines for the NeurIPS conference regarding open access to data and code for reproducibility, the following key points and discrepancies are identified:

1. The paper outlines the challenge design including the evaluation methods (Section 3) and baseline results (Section 4) but does not include explicit information or links to the data and code base used for the AI-Author and AI-Reviewer tasks. According to the NeurIPS guidelines, the paper should ideally facilitate reproduction of the results through open access to data and code unless there is a valid justification for not doing so.

2. While the authors provide a detailed methodology and baseline results, there's an absence of direct instructions, scripts, or environment details necessary for reproducing experiments. The NeurIPS guidelines emphasize the importance of including exact commands and environments needed.

3. The acknowledgment section and the proposal for future tutorials and hackathons suggest that the authors are keen on community engagement and dissemination of knowledge. However, the current absence of data and code accessibility limits such engagement’s potential effectiveness and the reproducibility of the paper's findings.

Actionable Feedback:

1. **Provide Access or Detailed Justification:** If possible, offer anonymized versions of the data and the code used in the challenges, along with sufficient instructions for reproduction in the supplementary material. If there are privacy, proprietary, or ethical constraints that prevent open access, these should be explicitly stated to justify the absence of shared data and code.

2. **Supplementary Material Detailing Reproduction Steps:** In the absence of direct data and code sharing, authors could enhance the supplementary material with a more detailed description of the methodology, including pseudo-code, algorithmic specifications, and the computational environment (libraries, versions, hardware specifics) to aid replication efforts with alternative datasets.

3. **Link to External Repositories (if applicable and possible):** Consider providing links to external repositories (GitHub, etc.) with anonymized or synthetic data examples and code (while preserving double-blind review guidelines). Authors could update the paper or supplementary material with specific URLs post-review, maintaining anonymity during the review process.

4. **Clarification on Data and Code Accessibility in Conclusion:** Enhance the conclusion section by discussing the implications of data and code accessibility on the validity and reproducibility of the presented challenge and its results. This could involve a reflection on the feasibility and avenues for interested researchers to engage with and build upon the presented work.

",0.5
"Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?",Experimental settings/details,Yes,The testing details is mentioned in the baseline result,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental material.
","The authors assert that the paper specifies all necessary training and test details to understand the results, referencing the baseline results section for this information. However, upon examination, the ""Baseline Results"" section primarily discusses the evaluation of AI-Author and AI-Reviewer tasks baselines with only limited information provided on the experiment setup specifics. Key training and testing details such as data splits, hyperparameters, reasoning behind their choice, and type of optimizer are not explicitly discussed in the cited sections or elsewhere in the paper. To improve the paper according to the guidelines provided:

1. **Specify Data Splits**: Clearly outline how the data was divided for training, validation, and testing phases. If the data splits are unconventional or designed specifically for the challenge, provide rationale for such choices.

2. **Detail Hyperparameters**: Include a comprehensive list of hyperparameters used for the ChatGPT baseline and AI-Referee-Reviewer, such as learning rates, batch sizes, and any model-specific parameters. Moreover, discuss the process or criteria used to select these hyperparameters, whether they were obtained through grid search, literature best practices, or another methodology.

3. **Type of Optimizer and its Rationale**: Elucidate on the type of optimizer employed in the baseline models. Provide justification for its selection over other potential choices, based on either literature review or experimental validation specific to the challenge's tasks.

4. **Expand Justification Provided**: While the baseline result section does offer insights into the evaluation of submissions, comprehensive details relevant to understanding and replicating the experimental results are missing. Enhance this section by incorporating the aforementioned details directly into the paper or appendices, ensuring the reader has full transparency on how results were achieved.

5. **Supplemental Materials**: If space constraints limit the amount of detail that can be provided in the main paper body, consider including thorough appendices or supplemental materials hosted on a platform like arXiv or the conference's supplementary material submission option. These should include detailed configurations, code snippets, or even pseudocode for critical implementation aspects.

6. **Clarification on Evaluation Framework**: While the evaluation methods for AI-authored and AI-reviewed papers are described, it's crucial to detail the specific setup used for these evaluations, including any computational platforms, software versions, or external libraries leveraged. This is particularly important for ensuring replicability and understanding the constraints under which the evaluation was conducted.

By addressing these points, the paper can significantly improve its compliance with the NeurIPS guidelines concerning the detailing of experiments, thereby offering a robust foundation for readers to appreciate, replicate, or build upon the reported work.

",0.5
Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?,Error bars,Yes,"Error bars are properly reported with clear definitions, capturing variability factors, and the methodology for their calculation is thoroughly explained, supporting the paper's main experimental claims.","If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The authors should answer ""Yes"" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error of the mean.
It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
","The authors stated that error bars were correctly reported and well-defined within the paper, supporting the main experimental claims. However, upon review of the paper's content, there is no explicit mention or display of error bars, confidence intervals, or statistical tests that would allow for an assessment of the statistical significance of the experimental results presented. The sections that would conventionally contain this information (e.g., Section 4: Baseline Results) discuss results and comparisons between different baseline approaches using scores and visual representations like figures. Still, they lack the explicit inclusion of error bars or any statistical measures of variability or significance.

Feedback to improve paper quality:
1. Clarify and Include Statistical Measures: The paper should explicitly state and include error bars, confidence intervals, or the results of statistical significance tests for the experimental results. This would involve revisiting the experimental data and calculating these statistics as per the norms in the field.
2. Methodological Details: Provide a subsection or paragraph in the methodology section (or wherever appropriate, such as within Section 4: Baseline Results) detailing the statistical methods used to derive error bars or confidence intervals. This should include the assumptions made (e.g., normality of data distribution) and the specific formulas or statistical software/functions used.
3. Referencing Figures and Tables: If error bars or statistical measures are reported in figures or tables, explicitly reference these in the text, explaining how these values were calculated and what they signify about the variability and reliability of the results.
4. Address Variability Factors: Identify and clearly describe the factors of variability considered in the experimental design and how they are captured by the statistical measures provided (e.g., between different runs of the models, variability due to differences in prompts, or other experimental conditions).
5. Improve Presentation of Results: Alongside the error bars or confidence intervals, provide a nuanced discussion in the text on what these measures imply for the reliability and significance of the reported findings, especially in supporting the paper's main claims.

The inclusion of these details would greatly enhance the paper's adherence to scientific standards for reporting experimental results, providing clearer insight into the significance and reliability of the findings.

",0
"For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?",Compute resources,No,The paper does not provide any type of information regarding the computer resources,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
","The proposed author's answer states that the paper does not provide any information regarding computer resources used for the experiments, which is consistent with the content described in the ""Paper"" section. The response complies with the guidelines for a ""No"" answer by offering a clear justification. However, to improve the quality of the paper and adhere to NeurIPS guidelines more closely, consider the following itemized, actionable feedback:

1. **Introduce a new subsection in the methodology or in the experimental setup sections:** This subsection should detail the computational resources used. Include information on the type of compute workers (CPU/GPU), specific models if applicable, memory, runtime for each experiment, and whether these experiments were run on an internal cluster or a cloud provider. If multiple configurations were used, delineate the resources per each significant experiment or task (e.g., AI-Author Task, AI-Reviewer Task).

2. **Estimate the total compute required:** Given that this paper introduces a novel platform evaluating LLMs across tasks, estimating the overall compute in terms of GPU/CPU hours or by other means would provide clarity on the scalability and accessibility of reproducing such a framework.

3. **Discuss the compute beyond the reported experiments:** If applicable, discuss whether the full research project required more compute than the experiments reported. This includes the computation for preliminary tests, failed experiments, or data preprocessing steps that didn't make it into the final paper but were essential for the experimental setup.

4. **Rationalize the non-disclosure, if any, of certain computational details:** If there are valid reasons for not disclosing specific details about the compute resources—such as proprietary information restrictions, nondisclosure agreements, or if the compute resources are standard and well-known within the community—providing this context can help readers understand the constraints.

5. **Enhance the reproducibility:** Apart from just disclosing the compute resources, consider providing configurations, scripts, or environment details that helped run these experiments. Sharing code in a supplementary material or through a well-documented repository can significantly enhance reproducibility and adherence to NeurIPS guidelines.

By addressing these points, the authors can substantially enhance the quality and reproducibility of the paper, aligning it more closely with both the NeurIPS guidelines and the broader scientific community's expectations for experimental transparency and reproducibility.

",0.5
"Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?",NeurIPS code of ethics,Yes,We review the code of ethics and our paper conform with it.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
","The proposed author's answer asserts compliance with the NeurIPS Code of Ethics without specifying where in the paper these considerations are addressed. This approach is insufficient under NeurIPS guidelines, which require specific sections to be pointed out, especially concerning ethical adherence. There are several areas within the paper where ethical considerations inherently arise, notably in the usage of Large Language Models (LLMs) for generating survey papers and evaluating them. The implications of AI in academic research, the potential for AI-generated misinformation, and the measures taken to ensure the AI's outputs are responsible, need direct addressing in relation to ethical guidelines.

Feedback:

1. **Specify Sections for Ethical Considerations**: Revise the justification to include specific sections of the paper where ethical considerations, especially those related to the use of LLMs in academic settings, are discussed. If such sections are currently absent, they should be explicitly added to the paper.

2. **AI-generated Content Scrutiny**: Given the paper presents a framework for AI-generated academic reviews, it's important to discuss the integrity of AI-generated content, potential biases in language models, and measures in place to mitigate these biases, aligning with ethical guidelines.

3. **Ethical Implications of Automated Review Systems**: Add a section discussing the ethical implications of using AI for peer review, including the potential impact on the academic publication process, information dissemination, and trustworthiness.

4. **Data Privacy and Consent**: If any real data (e.g., existing survey papers, reviewer comments) were used to train or validate the AI models, the paper should discuss how data privacy was ensured and consent obtained, if applicable.

5. **Transparency in AI Assessments**: Incorporate a discussion on the transparency of the AI assessment process, including how the models make decisions and the availability of these models for scrutiny, to adhere to ethical principles of transparency and accountability.

6. **Impact Assessment**: Consider adding an ethical impact assessment, evaluating potential negative outcomes of deploying such AI systems in academic settings and strategies to mitigate these outcomes.

By addressing these points, the paper will provide a comprehensive view of how the research aligns with ethical guidelines, substantiating the authors' claim of compliance with the NeurIPS Code of Ethics.

",0
Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?,Impacts,No,The paper comprehensively discusses the potential positive and limitation but not the negative societal impacts,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
","The authors incorrectly stated that their paper does not discuss the negative societal impacts of the work performed, yet the content and implications of their work inherently carry potential negative impacts, especially considering the uses of Large Language Models (LLMs) in generating academic content which could include misuse such as generating misleading information or academic dishonesty. The justification provided does not align with the guidelines which require discussing both potential positive and negative societal impacts if applicable.

Actionable Feedback:
1. **Explicitly Address Negative Societal Impacts**: Amend the text to include a dedicated section or paragraph that discusses potential negative societal impacts of the use of LLMs in automating literature reviews. This could include the creation of misinformation, misuse in academic settings to generate non-original research, and the ethical considerations around replacing human intellectual labor with AI.

2. **Consideration of Mitigation Strategies**: After identifying potential negative impacts, the authors should propose mitigation strategies. This could include methods to verify the authenticity and originality of the generated content, ethical guidelines for the use of such technology, or the development of detection tools for automated vs. human-created content.

3. **Balance in Discussion**: Ensure that the discussion of societal impacts is balanced, offering insights into both the positive aspects (as already present) and the newly addressed negative consequences. This will provide a more nuanced view of the technology's role and potential effects on society.

4. **Citations and References**: Incorporate references to existing literature on the ethical and societal implications of LLMs and AI in scholarly work. This not only strengthens the paper's credibility but also situates the discussion within the broader discourse on AI ethics and governance.

5. **Clarify Justification**: Revise the justification in the checklist to accurately reflect the paper’s content. If negative impacts are discussed following the feedback, the justification should be updated to indicate where in the paper these discussions can be found.

These changes would greatly improve the quality of the paper by providing a comprehensive view of the technology's potential societal impacts, in line with the conference guidelines for full consideration of ethical implications.

",0.5
"Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?",Safeguards,Not Applicable,Our models do not present such risks.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").
""
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
","While the authors have argued that their models do not present such risks for misuse and thus answered ""NA"" to the checklist question regarding safeguards for responsible release, this answer might not fully consider the broader implications of LLM application in automated literature review tasks. Given the nature of the paper, which involves the use of Large Language Models (LLMs) for generating academic content, several potential issues deserve attention:

1. **Ethical implications**: The use of LLMs in academic settings, especially for composing literature reviews or critique, inherently bears the risk of generating content that could be incorrect or misleading. This is particularly relevant as the models might synthesize information from a vast pool of data that could include biased or unverified sources.

2. **Misuse potential**: Given that the paper discusses the development and use of AI systems for generating survey papers, there is a potential for misuse in creating fraudulent or misleading academic content. This could impact the integrity of academic literature and scholarly communications.

3. **Data provenance and scraping**: While the paper does not explicitly mention the origin of the data used to train the models, the potential use of scraped datasets for training LLMs introduces risks related to privacy, copyright, and data safety. This is especially relevant if the datasets include academic papers or materials that are not open access or freely available.

To address these concerns and improve the paper's adherence to the NeurIPS checklist guidelines:

- **Add a section on Ethical Considerations**: It would be beneficial to include a detailed discussion about the ethical implications of using LLMs in generating and critiquing academic content. This section should assess potential biases in model outputs and strategies to mitigate them.

- **Discuss Misuse and Safeguards**: Even if the authors believe their specific application does not present risks of misuse, a discussion on the general potential for misuse of similar technologies in academic literature creation and review, alongside proposed or implemented safeguards, would strengthen the paper.

- **Clarify Data Provenance**: If any part of the model training involved datasets that could raise safety risks, the paper should discuss how these risks were mitigated. This includes clarifying the sources of training data, any scraping methods used, and compliance with data use agreements or licenses.

- **Implementation of Safety Filters**: As part of responsible AI development, the authors could explore the implementation of safety filters that prevent the generation of harmful, biased, or misleading content.

These improvements would not only align the paper more closely with responsible AI practices but also enhance the transparency and accountability of the research presented. The authors should consider these aspects even if their initial assessment led them to answer ""NA"" to the checklist question.

**",0.5
"Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?",Credits,No,The paper doesn't mention the name of the licence,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
If this information is not available online, the authors are encouraged to reach out to the asset's creators.
","The authors of the paper ""Auto-survey Challenge: Advancing the Frontiers of Automated Literature Review"" state that they did not mention the name of the license for the assets used in their work (i.e., code, data, models). However, there appears to be no explicit mention or discussion in the provided text regarding efforts to credit original owners or clarify the licensing and terms of use for the assets exploited in their research.

**Actionable Feedback:**

1. **Cite Original Sources:** Ensure all external assets (datasets, code libraries, or models) are accompanied by proper citations in the text or bibliography. If you derived your work from existing assets, credit the original work explicitly.

2. **License Information:** Specify the license under which each external asset is distributed. This includes the version of datasets, code libraries, or models used in your research. If the assets are publicly available software or datasets, include direct references to their source and the specific licenses they are distributed under. For instance, if GPT-3.5 was used, mention the OpenAI's terms of use or license explicitly.

3. **Dataset Versions and URLs:** When mentioning the usage of specific versions of software or datasets, such as ChatGPT-3.5, ensure to include version numbers and, if possible, a URL or DOI to where the asset can be accessed. This helps in ensuring reproducibility and verifying the terms of use compliance.

4. **Respect for Terms of Use:** Clearly state how the terms of use for each asset were followed in the context of your research. This is crucial when dealing with web-scraped data or proprietary models. If modifications were made to existing assets, clarify the nature of these changes and ensure they comply with the original terms of use.

5. **Include an Ethics Statement:** If any of the datasets or codes were custom-developed or involved proprietary or sensitive information, include an ethics statement in the paper discussing data collection methods, privacy considerations, and compliance with relevant regulations (GDPR, etc.).

6. **Check Additional Resources:** For widely used datasets or models, consult curated repositories or platforms like paperswithcode.com, which often list licensing information that may not be easily found elsewhere. This could help in accurately reporting the licenses.

7. **Provide Supplementary Material:** If detailed discussions on licensing and asset accreditation clutter the main text, consider including this information in a supplementary material section or appendix. This preserves the readability of the paper while ensuring full transparency and reproducibility.

By addressing these points, the authors can significantly improve the legal and ethical credibility of their paper, ensuring respectful and transparent use of third-party assets.

**",0
Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?,Documentation,Not Applicable,We don't introduce the new assents.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
The paper should discuss whether and how consent was obtained from people whose asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
","Feedback:

1. The authors claim that they do not introduce new assets, which conflicts with the description of implementing baselines, methods of evaluations, and generating ""good"" and ""bad"" versions of papers for evaluation in Sections 2 and 3. These components can be considered as assets developed during the research. 

2. The paper needs to clarify the documentation and availability of these assets, particularly:
    - The baseline implementation based on ChatGPT for AI-Author tasks.
    - The AI-Referee-Reviewer for evaluating submissions.
    - The generated dataset of ""good"" and ""bad"" versions of survey papers, used for the evaluation of AI-Reviewers.

3. For each asset mentioned, include:
    - A separate section or appendix detailing their design, implementation, and any limitations or considerations.
    - Clear instructions on how to access these assets, including URLs for any repositories, anonymized or not, depending on conference submission policies. If assets are not directly accessible, provide a justification.
    - Information on licensing or any usage restrictions.

4. Ensure that consent or authorization is documented for any data or pre-existing assets used in creating these new assets.

5. Clarify whether any of these assets, such as the AI models or generated datasets, will be made available to other researchers. If so, detail the conditions under which they can be used.

6. If anonymization is required for submission, confirm that any links or references to these assets in the paper are also anonymized. 

These improvements will enhance the reproducibility and transparency of the research, aligning the paper with best practices for asset documentation and sharing within the research community.

",0
"For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?",Human subjects,Not Applicable,This does not apply on our research as we are not doing any crowdsourcing experiments,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
","The authors' proposed answer is marked as NA with the justification that their research does not involve crowdsourcing experiments or research with human subjects. However, upon a careful examination of the ""Paper"" content, it's evident that while the study primarily focuses on the application of Large Language Models (LLMs) for automatically composing and critiquing academic surveys, there is a mention of a competition organized as part of the AutoML conference 2023, implying potential involvement or interaction with human participants in areas such as competition organization, baseline submissions, evaluations, and the implementation of a simulated peer-review mechanism.

Despite the primary emphasis on AI and machine learning models, the inclusion of a human jury for the final evaluation phase and human organizers in an editorial oversight capacity could warrant further clarification regarding whether any human subject research principles were considered, especially concerning compensation and guidelines for ethical treatment, even if those humans are not the primary data source or subjects of the research. 

Given the ambiguity and potential oversight regarding the involvement of human participants in the competition and editorial processes, which indirectly contributes to the research outcomes, the following itemized, actionable feedback is provided:

1. Clarify the role of human participants within the context of the AutoML conference competition, especially in the final evaluation phase conducted by a human jury and any other phases where human judgment or interaction might have been necessary.
2. If human participants were involved in any capacity beyond what's traditionally considered ""research subjects,"" such as contributing intellectual labor for critical evaluation, organizing, or any form of data interpretation, explicitly acknowledge this involvement and detail any ethical considerations, including compensation or volunteer acknowledgment.
3. In the case where human involvement was strictly limited to oversight or organizational roles without direct interaction or data collection related to crowdsourcing or subject research, explicitly state these details to eliminate ambiguity and ensure the justification for NA is fully grounded in the context of the guidelines provided.

",0.5
"Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?",Risks,Not Applicable,This does not concerns us as we are not dealing with human subjects in our research.,"If you answer Yes to a question, in the justification please point to the section(s) where related material for the question can be found.
While ""Yes"" is generally preferable to ""No"", it is perfectly acceptable to answer ""No"" provided a proper justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or ""we were unable to find the license for the dataset we used"").

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
","The provided answer ""NA"" with the justification that the research does not involve human subjects seems primarily appropriate given the paper's description of a computational challenge designed for Large Language Models (LLMs). However, while direct human subjects may not be involved, the use of human-generated content, ethical implications of AI-generated literature, and potential biases in LLM outputs could be considered as indirect factors affecting broader societal contexts or groups. These considerations are especially relevant given NeurIPS's emphasis on ethical responsibility. 

### Actionable Feedback
1. **Clarify the Scope of 'Human Subjects'**: The authors might consider expanding their justification to explicitly state that their research's primary subjects are AI models, not humans, yet briefly acknowledge any indirect human-related ethical considerations given the nature of AI work.
   
2. **Ethical Implications of AI-generated Content**: Given the platform's intent to evaluate LLM capabilities in academic literature generation, it would be prudent for the authors to discuss or at least acknowledge potential ethical implications. This includes but is not limited to the propagation of biases, the integrity of academic discourse, and the potential misuse of autonomously generated content. A short section or paragraph addressing these aspects could enhance the paper's consideration of broader ethical impacts.

3. **Potential Biases in Model Outputs**: The paper should address how biases in training data for LLMs and resultant outputs were considered or mitigated. This is important as biases in AI outputs can present risks to societal groups, even if not directly researched with human subjects. A brief overview of steps taken or considerations made regarding data diversity and bias mitigation strategies would be beneficial.

4. **Institutional Review Board (IRB) Considerations for AI Research**: While IRB approval may not be directly applicable, discussing any consultations or ethical review processes undertaken, even informally, could provide transparency. If no such processes were involved, simply stating this fact provides clarity to readers about the ethical oversight of the research.

5. **Reference to AI Ethics Guidelines**: Briefly linking the research approach to broader AI ethics guidelines, potentially including those suggested by NeurIPS or other relevant entities, would demonstrate due diligence in considering the ethical landscape of AI research.

By addressing these points, the authors could significantly strengthen the ethical considerations section of their paper, demonstrating a thorough understanding and proactive stance on the potential indirect impacts of their research on human subjects and society at large. This would align with NeurIPS's emphasis on ethical responsibility in AI research.

**",0.5
