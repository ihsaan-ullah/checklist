<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }
    .container {
        margin: 20px auto;
        padding: 0 20px;
        position: relative;
    }
    .button {
        padding: 10px;
        font-size: 16px;
        text-align: center;
        background-color: #f93361;
        color: #fff;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        text-decoration: none;
    }
    .button:hover {
        background-color: #bc0530;
    }
    .content {
        padding-top: 60px;
        padding-bottom: 40px;
        margin-bottom: 20px;
    }
    h1 {
        margin-top: 0;
        margin-bottom: 30px;
    }
    hr {
        margin-top: 50px;
        margin-bottom: 50px;
    }
    .review {
        margin-bottom: 30px;
        border: 1px solid #ccc;
        padding: 20px;
        border-radius: 5px;
        background-color: #f9f9f9;
    }
    .review h2 {
        margin-top: 0;
    }
    .review p {
        margin: 10px 0;
    }
    .question {
        /* color: #0033ff; */
        color: #000;
    }
    .answer {
        /* color: #28a745; */
        color: #000;
    }
    .justification {
        /* color: #de750b; */
        color: #000;
    }
    .user_input {
        padding: 20px;
        border-radius: 5px;
        background-color: #fff;
        border: 1px solid #3a3a3a;
    }
    .llm_review {
        color: #000;
        padding: 20px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .llm_review-red {
        background-color: #eacfcf;
        border: 1px solid #FF0000;
    }
    .llm_review-green {
        background-color: #c6e9c6;
        border: 1px solid #008000;

    }
    .llm_review-orange {
        background-color: #ebdecf;
        border: 1px solid #FF8C00;
    }
    table {
        border-collapse: collapse;
    }
    th, td {
        padding: 8px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }
    th {
        background-color: #f2f2f2;
    }
    .score-label {
        display: inline-block;
        padding: 5px 15px;
        border-radius: 5px;
        text-decoration: none;
    }
    .score-green {
        background-color: #c6e9c6;
        color: #000;
        border: 1px solid #008000;
    }
    .score-red {
        background-color: #eacfcf;
        color: #000;
        border: 1px solid #FF0000;
    }
    .score-orange {
        background-color: #ebdecf;
        color: #000;
        border: 1px solid #FF8C00;
    }
    .score-blue {
        background-color: #c8d8e6;
        color: #1b455e;
        border: 1px solid #1b455e;
    }
    .score-purple {
        background-color: #cac4e7;
        color: #271b5e;
        border: 1px solid #271b5e;
    }
    .scroll-button {
        padding: 5px 15px;
        font-size: 14px;
        cursor: pointer;
        text-decoration: none;
        border-radius: 3px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .button-green{
        background-color: #c6e9c6;
        color: #000;
        border: 1px solid #008000;
    }
    .button-orange{
        background-color: #ebdecf;
        color: #000;
        border: 1px solid #FF8C00;
    }
    .button-orange:hover {
        background-color: #623600;
        color: #fff;
        border: none;
    }
    .button-green:hover {
        background-color: #015f01;
        color: #fff;
        border: none;
    }
    .move-to-top {
        padding: 5px 10px;
        font-size: 12px;
        color: #212121;
        border: 1px solid #212121;
        border-radius: 3px;
        cursor: pointer;
        text-decoration: none;
    }
    .move-to-top:hover {
        background-color: #212121;
        color: #fff;
    }
    .legend{
        margin-bottom: 10px;
    }
</style>
</head>
<body>

<div class="container">
    <div class="content">
        <h1>Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks</h1>

        <hr>

        <!-- <h2>Scores</h2>
        <table>
            <tr>
                <td><strong>Paper Quality Score:</strong></td>
                <td><span class="score-label score-blue">0.57</span></td>
            </tr>
            <tr>
                <td><strong>LLM Accuracy:</strong></td>
                <td><span class="score-label score-purple">0.4</span></td>
            </tr>
        </table>

        <hr> -->

        <h2>Review Summary</h2>
        <div>
            <span class="legend score-label score-green">&nbsp; &nbsp;</span> The LLM found no significant concern
            <br>
            <span class="legend score-label score-orange">&nbsp; &nbsp;</span> The LLM proposes possible improvements
        </div>
        <table>
            <tr>
              <th>Question</th>
              <th></th>
            </tr>
            
            <tr id="summary-question-1">
                <td>1. Claims</td>
                <td><a href="#question-1" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-2">
                <td>2. Limitations</td>
                <td><a href="#question-2" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-3">
                <td>3. Theoritical assumptions and proofs</td>
                <td><a href="#question-3" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-4">
                <td>4. Experiments reproducibility</td>
                <td><a href="#question-4" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-5">
                <td>5. Code and data accessibility</td>
                <td><a href="#question-5" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-6">
                <td>6. Experimental settings/details</td>
                <td><a href="#question-6" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-7">
                <td>7. Error bars</td>
                <td><a href="#question-7" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-8">
                <td>8. Compute resources</td>
                <td><a href="#question-8" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-9">
                <td>9. NeurIPS code of ethics</td>
                <td><a href="#question-9" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-10">
                <td>10. Impacts</td>
                <td><a href="#question-10" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-11">
                <td>11. Safeguards</td>
                <td><a href="#question-11" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-12">
                <td>12. Credits</td>
                <td><a href="#question-12" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-13">
                <td>13. Documentation</td>
                <td><a href="#question-13" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-14">
                <td>14. Human subjects</td>
                <td><a href="#question-14" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-15">
                <td>15. Risks</td>
                <td><a href="#question-15" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
        </table>

        <!-- Survey -->
        <div>
            <p>After reading through the LLM’s response below, <strong>please complete the following survey about your experience:</strong></p>
            <a class="button" href="https://docs.google.com/forms/d/e/1FAIpQLScr4fjvUGhtiTzBfsqm5CCVvAGafp3sLSSB_Txz2YHhnLiiyw/viewform?usp=pp_url&entry.1830873891=VW5kZXJzdGFuZGluZyBhbmQgU2ltcGxpZnlpbmcgQXJjaGl0ZWN0dXJlIFNlYXJjaCBpbiBTcGF0aW8tVGVtcG9yYWwgR3JhcGggTmV1cmFsIE5ldHdvcmtz" target="_blank">Click to submit post submission survey</a>
            <p>Your feedback is critical to evaluate the effectiveness of this LLM checklist assistant!</p>
        </div>

        <hr>

        <h2>Detailed Review</h2>
        
        
        <div class="review" id="question-1">
            <div class="user_input">
                <p><strong class="question">Question # 1:</strong> Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Our main claims are listed in section 1 and highlight this paper's contributions.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer and justification align well with the guidelines. The main claims described in the abstract and introduction of the paper clearly articulate the contributions and scope of the work, emphasizing the improvement over existing Neural Architecture Search (NAS) for SpatioTemporal Graph Neural Networks (STGNNs) by addressing hyperparameter integration and search space reduction. The authors detailed their main contributions, which involve a disentangled framework for understanding and simplifying architecture search in STGNNs, an enhanced evaluation of model configurations through an expanded hyperparameter study, and a methodological approach that leads to state-of-the-art results using a simple random search method. These claims are substantiated in the paper through empirical experiments and a systematic examination of design choices, hyperparameters, and architecture configurations.<br>To refine this submission further and improve paper quality, the authors may consider focusing on the following key improvements based on the provided guidelines:<br><ol><li> <b>Clarity and Detail in Contribution Claims</b>: Ensure that the claims made in the abstract and introduction are both clearly reflective of and directly linked to the empirical results presented later in the paper. Each claim should have a corresponding result section that explicitly supports it.</li><br><li> <b>Explicit Mention of Limitations and Assumptions</b>: While the paper does an excellent job of outlining the contributions and the novel approaches taken, it could benefit from a clearer articulation of any assumptions underlying the study and any potential limitations of the proposed methods. Doing so will align with the guideline that calls for acknowledgment of limitations alongside the claims.</li><br><li> <b>Comparison with State-of-the-Art Methods</b>: The paper stands out by proposing a simplified method that outperforms both hand-crafted models and other NAS approaches. However, to strengthen the paper further, consider providing a deeper analysis of how and why the proposed method outperforms these alternatives, including potential limitations of the compared methods that the proposed approach successfully addresses.</li><br><li> <b>Broader Impact and Generality of Findings</b>: Extend the discussion on how the proposed approaches and findings can be generalized or applied to domains beyond traffic prediction and STGNNs. This could enhance the paper's contribution by demonstrating the broader applicability of the proposed framework and methods.</li><br><li> <b>Reproducibility and Open Science</b>: Confirm that all aspects of the proposed methods, including the code and datasets (subject to any necessary licenses), are accessible for replication and further study by other researchers. Mention the availability of these resources in the paper, which can drive the field forward by enabling others to build upon this work.</li><br></ol>In light of the guidelines and considering the content and contributions of the paper as described, the proposed answer and justification are aligned but could be significantly strengthened by addressing the points mentioned above.<br></div>
            <a class="move-to-top" href="#summary-question-1">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-2">
            <div class="user_input">
                <p><strong class="question">Question # 2:</strong> Does the paper discuss the limitations of the work performed by the authors?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We have discussed the potential limitations in section 6, including the application domain and considered temporal modeling methods.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors have claimed to discuss the limitations of their work in section 6 of their paper, mentioning the application domain and considered temporal modeling methods as potential limitations. This implies the authors are aware of the expectation to explore and disclose the weaknesses or limitations of their research, which aligns with the guidelines set by the NeurIPS conference. However, a thorough examination of section 6 as described doesn't provide a detailed evaluation on robustness, assumptions, scope of claims, factors influencing the performance, computational efficiency, and concerns relating to privacy and fairness, which are encouraged by the guidelines.<br>Accordingly, to ensure the paper meets the high standards set by NeurIPS and aligns more closely with the expectations regarding disclosure of limitations, the following itemized, actionable feedback is suggested:<br><ol><li> <b>Explicitness on Assumptions and Robustness</b>: Clearly delineate any assumptions made by the proposed methods, such as data distribution assumptions or independence between features. Discuss how resilient the methods are against violations of these assumptions.</li><br><li> <b>Scope of Claims</b>: Clarify the contexts or datasets on which the proposed models were tested. If the evaluations were on a limited number of datasets, acknowledge this limitation and suggest the need for broader validation.</li><br><li> <b>Factors Affecting Performance</b>: While the impact of architecture and hyperparameters has been thoroughly explored, discuss other potential factors like data quality or external variables that might influence the model’s performance.</li><br><li> <b>Computational Efficiency and Scalability</b>: Provide an analysis on the computational resource requirements of the NAS approach for STGNNs, especially in comparison to manual design methods. Discuss how the proposed method scales with increased data volume or model complexity.</li><br><li> <b>Privacy and Fairness</b>: If applicable, speculate on the potential impacts of the proposed models on privacy and fairness. This is especially relevant if the models could be used on personal data or in scenarios where algorithmic bias could emerge.</li><br><li> <b>Broader Implications and Ethical Considerations</b>: Beyond the technical limitations, reflect on the broader implications of automating architecture search in spatio-temporal graph neural networks. Consider potential negative outcomes if these models are deployed in sensitive applications without adequate safeguards.</li><br><li> <b>Clarification and Expansion in Section 6</b>: Revise section 6 to incorporate the above points. Ensure each limitation is not just mentioned but is accompanied by a discussion on how it impacts the research findings, and where possible, propose future research directions to address these limitations.</li><br></ol>By addressing these points, the paper can offer a more comprehensive and transparent view on the limitations and ethical considerations of the proposed research, potentially improving its reception among reviewers and readers alike.<br></div>
            <a class="move-to-top" href="#summary-question-2">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-3">
            <div class="user_input">
                <p><strong class="question">Question # 3:</strong> For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper does not include theoretical results.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> Based on the provided paper content and the given question, the identification of discrepancies between the author's proposed answer/justification and the paper content, as well as the provision of itemized, actionable feedback are as follows:<br><ol><li> <b>Discrepancies Identified</b>:</li></ol>- The authors state "NA" because the paper does not include theoretical results. However, throughout the document, there is a significant emphasis on the formulation and understanding of Spatio-Temporal Graph Neural Networks (STGNNs). This includes the "Problem Definition" section where a formal mathematical proposition is introduced (distributional bi-level formulation), which arguably constitutes a theoretical result. The methodology sections also discuss the disentangled framework with detailed descriptions of measures and choices that influence model design and selection, which bares the characteristics of theoretical analysis.- The paper introduces novel frameworks and empirical observations that underpin the performance of various STGNN configurations, which should be supported by theoretical justification, yet the submission avoids presenting or classifying these findings as theoretical results, leading to a potential mismatch in the reported NA answer.<br><ol><li> <b>Feedback and Recommendations</b>:</li></ol>- <b>Clarify Theoretical Contributions</b>: Rework the sections that inherently contain theoretical analysis, such as the disentangled framework for STGNNs and the distributional analysis of architecture space. Clearly distinguish between purely empirical findings and those with a theoretical basis, including assumptions and derivations, even if they are intuitive or based on empirical results.- <b>Include Theoretical Justifications</b>: For the methodology section, particularly the distributional bi-level formulation and the principles derived for reducing the search space, provide a concise theoretical rationale. Discuss assumptions and how they impact the framework's design, even if these assumptions are empirically validated.- <b>Properly Reference Theoretical Backgrounds</b>: Ensure that any foundational theories or widely accepted theorems leveraged in constructing the framework are appropriately cited. If certain empirical findings inspire the framework's structure or the hyperparameter analysis, discuss the theoretical underpinning that justifies their generalizability.- <b>Section Referencing in Justification</b>: Should any aspect of the paper implicitly contribute to the theoretical understanding of STGNNs or NAS for STGNNs, specify these sections in the answer to the checklist question. This could include referencing sections discussing the problem definition, the proposed framework’s underpinnings, and any part of the paper where theoretical implications are discussed.- <b>Appendix for Expanded Proofs</b>: If the paper includes informal proofs or empirical observations with substantial theoretical implications, consider moving detailed proofs or extended theoretical discussions to the appendix. This would maintain the flow of the paper while still providing a rigorous theoretical foundation for interested readers.<br>By addressing these recommendations, the authors can decrease the gap between the actual content of the paper and the provided checklist answer, enhancing the paper's clarity on its theoretical contributions.<br></div>
            <a class="move-to-top" href="#summary-question-3">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-4">
            <div class="user_input">
                <p><strong class="question">Question # 4:</strong> Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> The proposed answer and justification appear to align well with the requirement for disclosing information sufficient for reproducing the main experimental results. The authors indicate that Sections 4, 5, and Appendices B, C, D, E detail the experimental settings, datasets, baselines, implementation, and results. Moreover, providing a GitHub link for their code should enhance reproducibility, assuming the repository includes comprehensive code and necessary documentation.<br>However, to provide precise and actionable feedback:<br><ol><li> <b>Code Repository Check</b>: It’s vital that the GitHub repository is publicly accessible and contains not just the code but also detailed instructions on how to set up the environment, data preprocessing steps (if any), and the exact commands to reproduce the results. If this aspect is missing or unclear, it must be addressed.</li><br><li> <b>Dataset Licensing and Access</b>: The authors should ensure that any datasets used in the paper are not only mentioned but also accompanied by information on their accessibility and licenses. If datasets are proprietary, or if there are any restrictions on their use, these should be clearly stated. If not already done, this should be included in the specified sections or appendices.</li><br><li> <b>Dependencies and Environment</b>: For true reproducibility, the paper should specify the versions of all major dependencies and the programming language used. A commonly used approach is to provide a Dockerfile or a requirements.txt (for Python projects) in the code repository. If this is missing, it should be created.</li><br><li> <b>Experimental Details and Hyperparameters</b>: Ensure that all hyperparameters and any architectural nuances critical for reproducing the reported results are comprehensively described in the mentioned sections or appendices. If any details that significantly affect the reproducibility are omitted, they must be included in a revised version.</li><br><li> <b>Baseline Comparisons and Data Processing</b>: Any preprocessing steps, data splits, or specific settings used to derive results from baseline methods should be clearly documented. This is crucial when comparing new methods against existing work, to ensure fair and valid comparisons.</li><br><li> <b>Extended Validation on Scope of Reproducibility</b>: It would be beneficial if the paper could include a discussion on the limits of reproducibility, such as potential issues with different computational environments, dataset versions, or varying random seeds. A brief section discussing how to handle such discrepancies would be valuable.</li><br><li> <b>License for Released Code</b>: The paper or the GitHub repository should clearly state the license under which the code is released. This will inform the readers about how they can use, modify, and distribute the provided code.</li><br></ol>Considering the above points are addressed with the already positive indications from the submitted answer and justification, the paper seems to be on a solid path towards ensuring reproducibility. Ensuring the above recommendations can enhance the paper's compliance with reproducibility standards.<br><br></div>
            <a class="move-to-top" href="#summary-question-4">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-5">
            <div class="user_input">
                <p><strong class="question">Question # 5:</strong> Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> Feedback:<br><ol><li> Provide explicit instructions for data preparation and execution commands: The paper indicates that all data and code are openly accessible, with references to sections and the GitHub repository. However, it does not enumerate step-by-step instructions on how to prepare the data or the precise command lines to reproduce the experiments. For improvement, the authors should provide a clear, step-by-step guide within the repository's README or within the supplementary material detailing:</li></ol>- How to install dependencies.- Instructions for data preparation, including downloading the dataset(s), required pre-processing steps, and how to structure the data for the code.- Exact execution commands for reproducing the main results reported in the paper.<br><ol><li> Clarify the use of external data or requirements: While the paper provides general locations where the data can be found and mentions that datasets are from PeMS in Section B, it lacks details on whether all necessary preprocessing scripts are included and if there are any specific requirements or external datasets needed to fully reproduce the experiments beyond the mentioned datasets. Providing this information would assist in ensuring replicability.</li><br><li> Explain model parameter settings and hardware requirements: The paper briefly discusses various hyperparameters and architectural choices but does not specifically mention the final settings used for the reported experiments nor the computational resources required (e.g., type of GPUs, memory, etc.). Providing a detailed account of the hyperparameter settings used for the main results, along with any hardware specifications, would enhance reproducibility.</li><br><li> Improvement on anonymity for double-blind review process: The paper includes a GitHub link that could potentially compromise the anonymity required for NeurIPS submissions. While direct linking is useful for open access to code and data, the authors should ensure that the repository does not reveal their identity or affiliation to comply with the double-blind review policy. This can be managed by ensuring the GitHub repository is anonymized and does not link back to personal or institutional accounts that reveal the authors' identities.</li><br><li> Mention the license of the datasets and any legal restrictions: While the paper states that the datasets are publicly available, it is silent on their licensing terms. For full reproducibility and ethical use, the authors should clarify the license under which the datasets are available and any usage restrictions that come with them.</li><br><br></ol></div>
            <a class="move-to-top" href="#summary-question-5">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-6">
            <div class="user_input">
                <p><strong class="question">Question # 6:</strong> Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The author's proposed answer states that the paper specifies all the training and test details necessary to reproduce the results, pointing to sections and appendix B for the details on training and testing. However, the description provided does not mention the inclusion of certain critical aspects such as the type of optimizer, hyperparameter selection rationale, or details about data splits (beyond the high-level mention of their location in the paper).<br><b>Actionable feedback to improve the paper quality:</b><br><ol><li> <b>Detailed Hyperparameters Explanation:</b> Ensure section 5 and appendix B clearly describe each chosen hyperparameter, the range of values considered (if applicable), and the method used for selection (e.g., grid search, manual tuning). This should include not only the model hyperparameters but also any relevant details about the optimizer type, learning rate schedules, and any regularization techniques applied.</li><br><li> <b>Data Split Clarification:</b> Provide explicit details about how the datasets were split into training, validation, and test sets. This should include the percentage split or the exact number of samples in each set. If there are any non-standard splitting techniques used due to the nature of the data (e.g., temporal splits in time-series forecasting), these should be clearly justified.</li><br><li> <b>Optimizer Type and Configuration:</b> Specify the type of optimizer used (e.g., Adam, SGD) in the main text or in appendix B. Include any optimizer-specific parameters (like momentum in SGD or beta values in Adam) that were configured as part of the experiment setup.</li><br><li> <b>Rationale Behind Hyperparameter Choices:</b> Wherever possible, include justification for the selection of specific hyperparameters. If certain values were chosen based on previous literature, empirical tests, or other forms of analysis, this rationale should be documented to help readers understand the decision-making process.</li><br><li> <b>Reproducibility Checklist:</b> Consider adding a reproducibility checklist or a summary table in appendix B that consolidates all the experimental setup details mentioned throughout the paper. This tool could serve as a quick reference for readers interested in replicating the study's findings.</li><br><li> <b>Code Availability:</b> If not already provided, ensure there is a clear statement regarding the availability of the code used for the experiments. Include any necessary details for accessing the code repository, noting any dependencies or environment specifications required to run the code successfully.</li><br></ol>By addressing these points, the paper will offer a more comprehensive and transparent account of the experimental setup, aiding in reproducibility and providing the scientific community with a solid foundation for building upon the work.<br></div>
            <a class="move-to-top" href="#summary-question-6">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-7">
            <div class="user_input">
                <p><strong class="question">Question # 7:</strong> Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> Based on the comprehensive review of the paper "Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks," the following discrepancies and feedback concerning the reporting of statistical measures like error bars and their justification are noted:<br><ol><li> <b>Variance Reporting</b>: Although the paper mentions the inclusion of variance as error bars in sections 4 and 5, a clear description of the factors contributing to the variance (e.g., data splits, initialization methods) is missing. To align with the guidelines, authors should explicitly state what factors the error bars are capturing.</li><br><li> <b>Methodology for Calculating Error Bars</b>: The paper lacks a detailed explanation of the methodology used for calculating error bars. It is essential to specify whether a closed-form formula, a library function, bootstrap methods, or other statistical techniques were employed. This information is crucial for reproducibility and for readers to properly interpret the statistical significance of the results.</li><br><li> <b>Assumptions and Error Type</b>: The justification does not mention any assumptions made during the error bar calculation (e.g., normal distribution of errors) or clarify whether the error bars represent standard deviations or standard errors of the mean. Including this information would enhance the paper's clarity and the accuracy of the experimental results' interpretation.</li></ol><ol><li> <b>Reporting Confidence Intervals</b>: The guidelines suggest that if error bars assume normality without verification, it is better to report a 2-sigma error bar or explicitly state the confidence interval. The paper should consider adding a statement regarding the confidence level of the error bars to provide a clearer picture of the statistical significance.</li><br><li> <b>Treatment of Asymmetric Distributions</b>: The paper does not address how it handles asymmetrically distributed data, which is vital when considering symmetric error bars that might mislead the interpretation of data, especially in contexts like negative error rates. A brief discussion on this aspect could significantly improve the paper's robustness.</li><br></ol><b>Recommended Actionable Feedback</b>:<ul><li><b>Clarify Variance Sources</b>: Explicitly state the factors contributing to the observed variance directly within the sections reporting experimental results or in a dedicated methodology section.</li><li><b>Detail Error Bar Calculation Method</b>: Provide a clear explanation of how the error bars were calculated, including the specific methods and assumptions.</li><li><b>Specify Error Bar Type and Assumptions</b>: Clearly state whether error bars represent standard deviations or standard errors and mention any distributional assumptions made (e.g., normality of errors).</li><li><b>Report Confidence Intervals</b>: Include a statement on the confidence level of the reported error bars or consider providing 2-sigma error bars for a more robust statistical significance representation.</li><li><b>Address Asymmetric Distributions</b>: Discuss the approach for dealing with asymmetric distributions and the implications for interpreting error bars in such contexts.</li><br></ul>Improving these aspects based on the provided feedback will enhance the paper's statistical reporting quality, ensuring better alignment with the NeurIPS guidelines and facilitating clearer, more interpretable findings for readers.<br></div>
            <a class="move-to-top" href="#summary-question-7">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-8">
            <div class="user_input">
                <p><strong class="question">Question # 8:</strong> For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors claim that in Section 5 of the paper, they report the GPU hours required for generating reasonable answers with respect to multiple baselines, which ostensibly provides information about the computational resources used for the experiments. However, the guidelines specify that detailed information on computer resources should include not only the total compute (e.g., GPU hours) but also the type of compute workers (CPU or GPU), internal cluster, or cloud provider, including relevant memory and storage details alongside. The guidelines also mention the importance of disclosing whether the full research project required more compute than what is reported for the experiments.<br>Given the guidelines and the content of the paper as described, here is the actionable feedback to improve this aspect of the paper:<br><ol><li> <b>Specify Compute Worker Types</b>: Clearly state whether CPUs or GPUs were used for the experiments. If GPUs were used, specify the model and version. This information aids reproducibility for readers with different computational resources.</li><br><li> <b>Detail on Memory and Storage</b>: Include specifics about the memory and storage capabilities of the compute resources utilized. For instance, mention the GPU RAM or the amount of system memory available during the experiments.</li><br><li> <b>Execution Time</b>: Beyond just GPU hours, provide estimates of the total time of execution for each experiment. If possible, distinguish between training and inference times.</li><br><li> <b>Variability in Compute Resources</b>: If different parts of the experiments were conducted with varying levels of computational resources (e.g., different GPUs or using a cloud provider for some experiments but not others), this variability should be clearly reported.</li><br><li> <b>Extended Compute Disclosure</b>: Acknowledge if the reported experiments are a subset of a larger set of computations, including failed experiments or preliminary runs. This gives readers a clearer picture of the computational effort involved in the research project as a whole.</li><br><li> <b>Link Compute to Experimental Results</b>: Where applicable, connect the reported computational resources directly to the experimental results. For example, indicate how changes in compute resources might affect the reproducibility of the results or the feasibility of the experiments under different conditions.</li><br></ol>These suggestions aim to enhance the transparency and reproducibility of the computational aspects of the research.<br></div>
            <a class="move-to-top" href="#summary-question-8">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-9">
            <div class="user_input">
                <p><strong class="question">Question # 9:</strong> Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We review the code of ethics and our paper conform with it.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' response to the specific checklist question regarding adherence to the NeurIPS Code of Ethics is insufficiently detailed and does not appropriately address the question as per the guidelines provided. The authors simply state that they have reviewed the code and believe their paper conforms to it, without providing specific references to the paper sections that demonstrate this compliance. This lack of detail does not satisfy the requirement to indicate where in the paper evidence of ethical consideration can be found, making it hard to assess the claim of ethical compliance.<br><b>Actionable Feedback:</b><br><ol><li> <b>Specify Related Sections:</b> The authors should identify specific sections or aspects of their paper where ethical considerations, directly or indirectly related to the NeurIPS Code of Ethics, are discussed or can be inferred. This includes, but is not limited to, data privacy and consent (if real-world datasets are used), potential biases in their methods or data, and broader impacts of their research.</li><br><li> <b>Broader Impact Statement:</b> Although the paper might include a broader impact statement (as required by NeurIPS in recent years), the authors should ensure this section robustly covers potential negative and positive implications of their work. This includes discussing any societal, environmental, or ethical implications of their proposed STGNN architecture and its applications in traffic prediction or other domains.</li><br><li> <b>Data and Model Ethics:</b> Clarify the ethical considerations around the datasets used in the experiments, especially given the use of real-world traffic data. This involves detailing any permissions obtained, anonymization techniques employed, and any potential biases in the dataset. If not directly addressed in the paper, the authors should revise it to include such considerations.</li><br><li> <b>Fairness and Bias:</b> Considering the application of their work in the predictive modeling domain, the authors should discuss any steps taken or checks performed to ensure that their model does not perpetuate or exacerbate biases, especially since traffic prediction models can have broad implications for urban planning and potentially affect different populations unequally.</li><br><li> <b>Experimentation Ethics:</b> If the work involves or could involve human participants at any stage (e.g., data collection or validation studies), explicit mention of ethical approvals or considerations should be made.</li><br><li> <b>Compliance and Limitations:</b> Explicitly state any potential ethical limitations or areas where the proposed method might not fully align with the NeurIPS Code of Ethics, and how future work could address these limitations.</li><br></ol>By providing these details, the authors can significantly strengthen the ethical considerations of their paper, ensuring it meets NeurIPS standards and helps the scientific community assess the work's impact responsibly.<br></div>
            <a class="move-to-top" href="#summary-question-9">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-10">
            <div class="user_input">
                <p><strong class="question">Question # 10:</strong> Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We discuss the broader impacts in the section "Broader Impact Statement".</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed author's Answer and Justification align well with the NeurIPS Guidelines; however, the extensive nature of the paper's "Broader Impact Statement" warrants a more detailed examination and critique. The Paper, "Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks," touches on several critical aspects of STGNNs and Neural Architecture Search (NAS), which inherently carry potential societal impacts.<br>Feedback to Improve Paper Quality:<br><ol><li> <b>Expand on Societal Impact Clarity</b>: It appears the paper leans heavily towards technical advancements without a balanced, in-depth discussion on the societal implications. Despite stating the inclusion of "Broader Impact Statement," the paper should elaborate on both positive and negative societal impacts in more granular detail, supporting the Guidelines' emphasis on considering such impacts.</li><br><li> <b>Potential Negative Impacts Discussion</b>: To strengthen the paper, it's crucial to elucidate potential negative societal impacts explicitly. For instance, address how the enhanced efficiency and predictive capabilities of STGNNs, while beneficial in traffic management and urban planning, might also raise privacy concerns, or how they could be misused in surveillance, echoing the Guidelines' concern for privacy, security, and misuse.</li><br><li> <b>Mitigation Strategy Inclusion</b>: Following the Guidelines, discuss potential mitigation strategies for any highlighted negative impacts. The current broader impact discussion could be enriched by proposing specific mechanisms to prevent misuse, such as ethical guidelines for deployment, transparency in model functioning, or technological safeguards against privacy violations.</li><br><li> <b>Expound on Positive Impacts</b>: While the advancements in NAS for STGNNs likely have numerous benefits, such as improving urban mobility and reducing traffic congestion, the paper should offer a more detailed exploration of these positive societal impacts. This should include a discussion on how these improvements can contribute to societal well-being, economic efficiency, and environmental sustainability.</li><br><li> <b>Consideration of Fairness</b>: Reflect on the deployment scenario of the technology in different regions and its access across various social groups. There's a need to address fairness concerns, ensuring that the technology's benefits do not disproportionately favor some segments over others, adhering to the Guidelines’ emphasis on fairness.</li><br><li> <b>Clarify Integration of Societal Considerations in Research Design</b>: Elaborate on how societal impact considerations have influenced the research design and methodology. Are there ethical boundaries set during algorithm development? How does the choice of data, or the architecture itself, reflect an awareness of potential societal impacts?</li><br></ol>In conclusion, while the authors indicate a "Broader Impact Statement,” the paper would benefit significantly from a more thorough and nuanced discussion on societal impacts, directly tied to the specific technological advancements proposed. Integrating these considerations will not only align the submission more closely with the NeurIPS guidelines but will also enrich the discourse around the responsible development and deployment of AI technologies.<br><br></div>
            <a class="move-to-top" href="#summary-question-10">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-11">
            <div class="user_input">
                <p><strong class="question">Question # 11:</strong> Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper poses no such risks.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors have provided an "NA" response to the question regarding safeguards for responsible release of data or models, suggesting that their paper poses no such risks. However, given the paper's focus on SpatioTemporal Graph Neural Networks (STGNNs) and the accompanying mention of publicly available datasets and model release via GitHub, this response warrants a closer inspection for compliance with the NeurIPS guidelines.<br>Given that the paper involves the use of traffic prediction models, which bear implications for privacy and potentially sensitive data usage, especially when derived from public datasets or scraped data, the justification for an "NA" response seems overly simplistic and potentially negligent of broader ethical considerations associated with data sensitivity, privacy, and responsible AI development.<br><h3>Actionable feedback:</h3><br><ol><li> <b>Clarify Data Sourcing and Privacy Considerations:</b> While the "NA" response might be technically accurate under the assumption that all employed datasets were ethically sourced and publicly available, it's crucial to elaborate on any privacy considerations taken or permissions acquired for using these datasets, especially if any data was scraped or collected from public sources.</li><br><li> <b>Explicitly Address Potential Misuse:</b> Given the application domain (traffic prediction), it is essential to articulate any foreseeable misuses of the released model or data, even if the risk is low. This includes misuse scenarios such as leveraging traffic prediction data for unwarranted surveillance or manipulation of traffic management systems, and how such risks have been mitigated.</li><br><li> <b>Add Safeguard Measures:</b> Reflect on and incorporate safeguard measures for the responsible release of your models and data. This may involve:</li></ol>- Usage guidelines that outline acceptable and ethical use cases.- Access controls or agreements that require users to commit to ethical usage before accessing the models or datasets.- Including an ethical consideration section discussing broader impacts and responsible AI principles in relation to your work.<br><ol><li> <b>Revise the Justification Statement:</b> Amend the justification statement to reflect the added considerations and safeguards. Clearly indicate the sections where these issues are discussed, ensuring transparency and adherence to ethical guidelines.</li><br><li> <b>Consult NeurIPS Ethical Guidelines:</b> Align the discussion on data and model release safeguards with the principles outlined in the NeurIPS ethical guidelines, ensuring that the paper not only advances technical knowledge but also promotes responsible AI development practices.</li><br></ol>By addressing these points, the authors can strengthen the paper's ethical considerations, align with NeurIPS submission requirements, and contribute to the responsible advancement of AI technologies in sensitive application areas such as traffic prediction.<br><h3>0.5</h3></div>
            <a class="move-to-top" href="#summary-question-11">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-12">
            <div class="user_input">
                <p><strong class="question">Question # 12:</strong> Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed authors' "Answer" and "Justification" generally align well with the "Question" concerning proper crediting and license mentioning of the assets used in the paper. However, there are potential discrepancies and areas of improvement regarding thoroughness and compliance with the guidelines provided. Specifically:<br><ol><li> <b>Citing Original Creators and Licenses</b>: While the authors mention citing the sources of the data used in their paper, there is no explicit mention of including the original creators of any code or models that may have been utilized or adapted. This is important not only for proper attribution but also for respecting the licensing terms of those assets.</li><br><li> <b>Specific License Information</b>: The authors have not specified the license under which the datasets are available. Mentioning the GitHub repository provides accessibility to the data, but does not clarify the terms under which the data can be used. Different datasets might have different licenses that dictate how they can be used or modified. The inclusion of specific license details (e.g., CC-BY, MIT) for each dataset is crucial for transparency and legal compliance.</li><br><li> <b>Terms of Use</b>: Even if the license information is implicitly available through the linked GitHub repository, it's best practice for a paper to explicitly state any significant terms of use that could affect the reproducibility or ethical use of the datasets involved.</li><br><li> <b>Dataset Versioning</b>: The authors have not mentioned the version of the datasets used. Dataset contents can change over time (updates, corrections, etc.). Specifying the version of the dataset used for the experiments ensures the reproducibility of the research.</li><br></ol><b>Feedback for Improvement</b>:<br><ul><li><b>Explicitly Mention All Used Assets</b>: Ensure that all the datasets, code libraries, and any pre-trained models are clearly listed and cited in the paper. This includes not only the data but also any software or pre-existing models that the research builds upon.</li><br><li><b>Include Specific License Information</b>: Directly state the license for each asset used in the study. If the asset's license cannot be found, this should be explicitly mentioned along with any efforts made to determine the licensing terms.</li><br><li><b>Detailed Terms of Use</b>: If there are restrictions or notable terms of use for any dataset or codebase, clarify these terms within the paper. This could affect how others can use or build upon your work.</li><br><li><b>Version Information</b>: Provide version numbers or specific commit hashes for datasets and code repositories to facilitate exact replication of the research in future studies.</li><br><li><b>Repository Contents</b>: If assets are hosted on a repository like GitHub, ensure the repository itself includes a clear license file and any relevant copyright notices. If your own research outputs are shared (e.g., code or data), include appropriate licensing information for those as well.</li><br></ul>By addressing these points, the authors can improve the transparency and reproducibility of their work, contributing to the integrity and utility of the research for the broader community.<br></div>
            <a class="move-to-top" href="#summary-question-12">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-13">
            <div class="user_input">
                <p><strong class="question">Question # 13:</strong> Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> The paper does not release new assets.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed author's answer and justification appear to be in direct contradiction with the content provided in the paper itself. The checklist question focused on whether new assets introduced in the paper are well documented, with the provided answer being "NA" since the paper supposedly did not release new assets. However, the content clearly mentions that their code is available on GitHub, indicating the release of an asset.<br>Actionable feedback based on the provided guidelines and paper content:<br><ol><li> Correct the answer to the checklist question regarding the introduction of new assets. Given the paper mentions the availability of code on GitHub, the answer should be "Yes" instead of "NA".</li><br><li> Update the justification to accurately reflect where in the paper these assets are documented. Specifically, point to the section where the GitHub link is provided and ensure the description of the assets (code/model) includes details about training, license, limitations, etc., as per the guidelines.</li><br><li> If applicable, provide a concise but comprehensive documentation of the introduced assets. This documentation should ideally be available both within the paper (or supplementary materials) and alongside the assets on GitHub. It should include:</li></ol>- A brief description of the assets.- Instructions for use, including installation, dependencies, and execution steps.- A clear statement on the license under which the assets are released.- Any limitations or assumptions inherent in the assets.- Information on how consent was obtained for any data used.<br><ol><li> Ensure that any URLs or references to the assets are anonymized if required by the submission guidelines. If the current GitHub link directly includes identifiable information, consider setting up an anonymized link or providing a temporary view-only link until the review process is complete.</li><br><li> Finally, review the paper to ensure that all mentions of the assets are consistent with the provided documentation and guidelines. This includes checking for any further contradictions or oversights in how the assets are presented in the text.</li><br></ol>Following these steps will rectify the discrepancy in the authors' proposed answer and justification regarding the introduction of new assets, aligning the submission with the NeurIPS guidelines and improving the overall quality of the paper.<br><br></div>
            <a class="move-to-top" href="#summary-question-13">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-14">
            <div class="user_input">
                <p><strong class="question">Question # 14:</strong> For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper does not involve crowdsourcing nor research with human subjects.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> The proposed author's answer and justification seem consistent with the actual content of the paper. The paper indeed focuses on the Neural Architecture Search (NAS) within the scope of Spatio-Temporal Graph Neural Networks (STGNNs) and does not involve crowdsourcing experiments or research with human subjects. Therefore, the NA (Not Applicable) response to the checklist question about respecting crowdsourcing experiments and research with human subjects is valid and appropriate given the methods and data involved in this study.<br>However, to enhance the quality and clarity of the paper, some improvements could be made, not directly related to the question at hand but valuable for overall paper quality:<br><ol><li> <b>Clarify Participant Involvement:</b> Though not directly related to human subjects in the traditional sense, the involvement of any indirect human data (e.g., traffic patterns that might involve human behavior analysis) should be clarified. If any dataset used might potentially include human subjects data indirectly (e.g., traffic data could be considered to indirectly involve human subjects since it involves their movement), this should be stated and justified.</li><br><li> <b>Dataset Detailing:</b> Expand on the details of datasets used, including any ethical considerations about the data collection process, even if the data is publicly available. This is particularly important for traffic data, which can indirectly involve human behavior.</li><br><li> <b>Model Evaluation Considerations:</b> Though not applicable to the specific question, ensuring transparency in model evaluation, particularly when involving societal or infrastructural elements like traffic predictions, highlights the potential real-world impact and ethical considerations of the proposed NAS methods on societal and individual levels.</li><br><li> <b>Broader Ethics Statement:</b> Expand the discussion on broader ethical considerations of applying the developed NAS approach in real-world applications, especially those indirectly involving human subjects or having a societal impact, to align with NeurIPS's code of ethics.</li><br><li> <b>Compensation and Acknowledgment:</b> While the question on compensation is NA for this study, if there are any contributors or participants in the dataset creation, collection, or preprocessing phase, acknowledging their contribution and detailing any compensation (if applicable) might be relevant.</li><br><li> <b>Supplemental Material Consideration:</b> Though the main paper accurately addresses the checklist question, any supplementary materials involving additional experiments, user studies, or data exploration that might engage human subjects, directly or indirectly, should follow the same guideline and provide detailed ethics and participation information.</li><br></ol>Addressing these points can enhance the ethical rigor and transparency of the paper, providing a clearer understanding of the implications and considerations of the research within and beyond the academic community.<br><br></div>
            <a class="move-to-top" href="#summary-question-14">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-15">
            <div class="user_input">
                <p><strong class="question">Question # 15:</strong> Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper does not involve crowdsourcing nor research with human subjects.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> The authors assert that the question regarding research participant risks, disclosures, and Institutional Review Board (IRB) approvals is not applicable to their work since their paper does not involve crowdsourcing nor research with human subjects. Given that the paper focuses on "Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks," primarily involving computational models and algorithms with applications in traffic prediction, this assertion seems consistent with the content and aims outlined in the paper. The utilization of publicly available datasets for traffic prediction further supports the appropriateness of the NA answer to the IRB-related question since these types of datasets typically do not involve direct human subjects research or personal data that would require IRB oversight.<br>However, providing more detail on the nature of the datasets (e.g., anonymization of data, public availability, and ethical considerations in data collection) could strengthen the justification. Although the use of secondary data generally falls outside the purview of IRB review, addressing any potential ethical considerations around the data (even if to confirm their non-applicability) would align with the NeurIPS commitment to ethical standards.<br>Feedback for improvement, based on the given guidelines, includes:<br><ol><li> Clarify the ethical considerations around the datasets used, confirming their compatibility with ethical standards for research. Even if the paper is computational, briefly discussing the origins of the datasets and any ethical review they underwent could enhance transparency.</li><li> Expand the justification by briefly mentioning how the datasets are typically used in the research community and any standard practices for their ethical use.</li><li> Although not required, a brief statement on the broader ethical implications of the research findings (e.g., potential societal impact or misuse of predictive models) could provide a more comprehensive view on ethics, aligning with the NeurIPS encouragement towards consideration of broader impacts.</li><br></ol>Reviewing the content against the NeurIPS requirements and standards, the response and justification provided by the authors are aligned with their paper’s computational and methodological focus, not directly involving human subjects or requiring IRB review. However, slightly expanding on the ethical considerations of data use and the societal impact of their work could enhance compliance with the spirit of the guidelines.<br></div>
            <a class="move-to-top" href="#summary-question-15">↑ Back to summary</a>
        </div>
        

        <!-- Survey -->
        <div>
            <p>Please complete the following survey about your experience:</p>
            <a class="button" href="https://docs.google.com/forms/d/e/1FAIpQLScr4fjvUGhtiTzBfsqm5CCVvAGafp3sLSSB_Txz2YHhnLiiyw/viewform?usp=pp_url&entry.1830873891=VW5kZXJzdGFuZGluZyBhbmQgU2ltcGxpZnlpbmcgQXJjaGl0ZWN0dXJlIFNlYXJjaCBpbiBTcGF0aW8tVGVtcG9yYWwgR3JhcGggTmV1cmFsIE5ldHdvcmtz" target="_blank">Click to submit post submission survey</a>
            <p>Your feedback is critical to evaluate the effectiveness of this LLM checklist assistant!</p>
        </div>
        
    </div>
</div>

</body>
</html>