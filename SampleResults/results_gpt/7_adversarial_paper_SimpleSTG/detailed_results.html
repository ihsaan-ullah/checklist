<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        box-sizing: border-box;
    }
    .container {
        margin: 20px auto;
        padding: 0 20px;
        position: relative;
    }
    .button {
        padding: 10px;
        font-size: 16px;
        text-align: center;
        background-color: #f93361;
        color: #fff;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        text-decoration: none;
    }
    .button:hover {
        background-color: #bc0530;
    }
    .content {
        padding-top: 60px;
        padding-bottom: 40px;
        margin-bottom: 20px;
    }
    h1 {
        margin-top: 0;
        margin-bottom: 30px;
    }
    hr {
        margin-top: 50px;
        margin-bottom: 50px;
    }
    .review {
        margin-bottom: 30px;
        border: 1px solid #ccc;
        padding: 20px;
        border-radius: 5px;
        background-color: #f9f9f9;
    }
    .review h2 {
        margin-top: 0;
    }
    .review p {
        margin: 10px 0;
    }
    .question {
        /* color: #0033ff; */
        color: #000;
    }
    .answer {
        /* color: #28a745; */
        color: #000;
    }
    .justification {
        /* color: #de750b; */
        color: #000;
    }
    .user_input {
        padding: 20px;
        border-radius: 5px;
        background-color: #fff;
        border: 1px solid #3a3a3a;
    }
    .llm_review {
        color: #000;
        padding: 20px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .llm_review-red {
        background-color: #eacfcf;
        border: 1px solid #FF0000;
    }
    .llm_review-green {
        background-color: #c6e9c6;
        border: 1px solid #008000;

    }
    .llm_review-orange {
        background-color: #ebdecf;
        border: 1px solid #FF8C00;
    }
    table {
        border-collapse: collapse;
    }
    th, td {
        padding: 8px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }
    th {
        background-color: #f2f2f2;
    }
    .score-label {
        display: inline-block;
        padding: 5px 15px;
        border-radius: 5px;
        text-decoration: none;
    }
    .score-green {
        background-color: #c6e9c6;
        color: #000;
        border: 1px solid #008000;
    }
    .score-red {
        background-color: #eacfcf;
        color: #000;
        border: 1px solid #FF0000;
    }
    .score-orange {
        background-color: #ebdecf;
        color: #000;
        border: 1px solid #FF8C00;
    }
    .score-blue {
        background-color: #c8d8e6;
        color: #1b455e;
        border: 1px solid #1b455e;
    }
    .score-purple {
        background-color: #cac4e7;
        color: #271b5e;
        border: 1px solid #271b5e;
    }
    .scroll-button {
        padding: 5px 15px;
        font-size: 14px;
        cursor: pointer;
        text-decoration: none;
        border-radius: 3px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .button-green{
        background-color: #c6e9c6;
        color: #000;
        border: 1px solid #008000;
    }
    .button-orange{
        background-color: #ebdecf;
        color: #000;
        border: 1px solid #FF8C00;
    }
    .button-orange:hover {
        background-color: #623600;
        color: #fff;
        border: none;
    }
    .button-green:hover {
        background-color: #015f01;
        color: #fff;
        border: none;
    }
    .move-to-top {
        padding: 5px 10px;
        font-size: 12px;
        color: #212121;
        border: 1px solid #212121;
        border-radius: 3px;
        cursor: pointer;
        text-decoration: none;
    }
    .move-to-top:hover {
        background-color: #212121;
        color: #fff;
    }
    .legend{
        margin-bottom: 10px;
    }
</style>
</head>
<body>

<div class="container">
    <div class="content">
        <h1>Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks</h1>

        <hr>

        <!-- <h2>Scores</h2>
        <table>
            <tr>
                <td><strong>Paper Quality Score:</strong></td>
                <td><span class="score-label score-blue">0.6</span></td>
            </tr>
            <tr>
                <td><strong>LLM Accuracy:</strong></td>
                <td><span class="score-label score-purple">0.37</span></td>
            </tr>
        </table>

        <hr> -->

        <h2>Review Summary</h2>
        <div>
            <span class="legend score-label score-green">&nbsp; &nbsp;</span> The LLM found no significant concern
            <br>
            <span class="legend score-label score-orange">&nbsp; &nbsp;</span> The LLM proposes possible improvements
        </div>
        <table>
            <tr>
              <th>Question</th>
              <th></th>
            </tr>
            
            <tr id="summary-question-1">
                <td>1. Claims</td>
                <td><a href="#question-1" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-2">
                <td>2. Limitations</td>
                <td><a href="#question-2" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-3">
                <td>3. Theoritical assumptions and proofs</td>
                <td><a href="#question-3" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-4">
                <td>4. Experiments reproducibility</td>
                <td><a href="#question-4" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-5">
                <td>5. Code and data accessibility</td>
                <td><a href="#question-5" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-6">
                <td>6. Experimental settings/details</td>
                <td><a href="#question-6" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-7">
                <td>7. Error bars</td>
                <td><a href="#question-7" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-8">
                <td>8. Compute resources</td>
                <td><a href="#question-8" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-9">
                <td>9. NeurIPS code of ethics</td>
                <td><a href="#question-9" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-10">
                <td>10. Impacts</td>
                <td><a href="#question-10" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-11">
                <td>11. Safeguards</td>
                <td><a href="#question-11" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-12">
                <td>12. Credits</td>
                <td><a href="#question-12" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-13">
                <td>13. Documentation</td>
                <td><a href="#question-13" class="scroll-button button-orange">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-14">
                <td>14. Human subjects</td>
                <td><a href="#question-14" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
            <tr id="summary-question-15">
                <td>15. Risks</td>
                <td><a href="#question-15" class="scroll-button button-green">Detailed Review</a></td>
                </tr>
            
        </table>

        <!-- Survey -->
        <div>
            <p>After reading through the LLM’s response below, <strong>please complete the following survey about your experience:</strong></p>
            <a class="button" href="https://docs.google.com/forms/d/e/1FAIpQLScr4fjvUGhtiTzBfsqm5CCVvAGafp3sLSSB_Txz2YHhnLiiyw/viewform?usp=pp_url&entry.1830873891=VW5kZXJzdGFuZGluZyBhbmQgU2ltcGxpZnlpbmcgQXJjaGl0ZWN0dXJlIFNlYXJjaCBpbiBTcGF0aW8tVGVtcG9yYWwgR3JhcGggTmV1cmFsIE5ldHdvcmtz" target="_blank">Click to submit post submission survey</a>
            <p>Your feedback is critical to evaluate the effectiveness of this LLM checklist assistant!</p>
        </div>

        <hr>

        <h2>Detailed Review</h2>
        
        
        <div class="review" id="question-1">
            <div class="user_input">
                <p><strong class="question">Question # 1:</strong> Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Our main claims are listed in section 1 and highlight this paper's contributions.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer indicates that the main claims made in the abstract and introduction accurately reflect the contributions and scope of their paper. However, their justification broadly mentions that these claims are highlighted in section 1 without providing a fine-grained cross-reference within the paper to where each specific claim is substantiated. This approach could make it harder for readers to verify the claims against the provided evidence.<br><b>Feedback:</b><br><ol><li> <b>Detail Specific Sections of Contribution</b>: Rather than mentioning section 1 in a broad sense, point out exact subsections or even paragraphs where each claim is detailed. If the paper abstract claims advancements or improvements in specific aspects of Spatio-Temporal Graph Neural Networks (STGNNs) or Neural Architecture Search (NAS), the authors should reference the sections where these advancements are experimentally validated or theoretically argued.</li><br><li> <b>Clarify Contribution with Respect to Issues Highlighted</b>: Given that the abstract mentions tackling issues with existing NAS methods for STGNNs, such as integrating hyperparameters into the NAS framework and reducing the search space, it's critical to pinpoint where in the paper these solutions are proposed, and their effectiveness is demonstrated.</li><br><li> <b>Address Limitations and Assumptions</b>: If there are aspirational goals or assumptions mentioned in the abstract or introduction, ensure these are also reflected upon in the discussed sections with a clear indication of any limitations encountered, as per the guidelines.</li><br><li> <b>Improve Abstract and Introduction Consistency</b>: Ensure that the scope and contributions mentioned in the abstract and introduction are mirrored throughout the paper. If the abstract mentions surpassing state-of-the-art models, the introduction and subsequent sections should detail this comparison in depth.</li><br><li> <b>Empirical Evidence and Theoretical Justification</b>: For every main claim, there should be empirical evidence provided through experiments or theoretical justification referenced directly within the justification for the answer to this question. This will improve the paper's credibility and allow reviewers to easily assess the validity of the claims.</li><br><li> <b>Revisit the Justification Section</b>: Adding more detail to the justification section to guide readers directly to where they can find support for the claims can significantly enhance the quality and integrity of the paper.</li><br></ol>The justification provided by the authors could be enhanced by directly linking the paper's claims to specific evidences or sections within the document, which would make it easier for readers and reviewers to validate these claims against the results and discussions presented.</div>
            <a class="move-to-top" href="#summary-question-1">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-2">
            <div class="user_input">
                <p><strong class="question">Question # 2:</strong> Does the paper discuss the limitations of the work performed by the authors?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We have discussed the potential limitations in section 6, including the application domain and considered temporal modeling methods.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors' proposed answer and justification generally align with the guidelines for discussing the limitations of the work performed. They pointed out the section where the limitations are discussed and provided specific aspects covered, such as the application domain and temporal modeling methods. However, to further enhance the quality and integrity of the paper, the following feedback focusing on significant improvements is provided based on the guidelines:<br><ol><li> <b>Expand the Discussion on Limitations</b>: While Section 6 mentions potential limitations, the depth and breadth of this discussion could be improved. The authors should consider elaborating on how the limitations might affect the generalizability and applicability of their proposed models. For instance, discussing the implications of their approach in diverse datasets or under different assumptions could provide clearer insights into the potential scope of their work.</li><br><li> <b>Robustness to Assumptions</b>: The paper should critically evaluate how robust their proposed framework is to violations of underlying assumptions. For example, the authors assume the adequacy of spatial and temporal modeling within the STGNN framework but do not discuss how deviations from this assumption might impact performance. This discussion could include theoretical considerations or empirical evidence from varied datasets.</li><br><li> <b>Scalability Discussion</b>: While the authors mention simplifying the architecture search for STGNNs, the computational efficiency and scalability of their proposed methods are not adequately discussed. Especially in the context of large-scale networks or high-frequency data, these aspects are crucial for practical applications. An in-depth analysis of computational requirements, efficiency trade-offs, and scalability to larger datasets would significantly enhance the paper's value.</li><br><li> <b>Reflection on Performance Factors</b>: The paper could benefit from a more detailed reflection on factors influencing the performance of their proposed methods. Beyond architecture and hyperparameters, there might be external factors or data characteristics that significantly impact model performance. Discussing such factors and providing guidelines for practitioners on when and how their proposed framework might be best applied would be advantageous.</li><br><li> <b>Addressing Privacy and Fairness</b>: If applicable, considering the growing concern around data privacy and algorithmic fairness, the authors should reflect on these aspects concerning their framework. Even if not directly relevant to their current study, acknowledging the potential limitations and future directions regarding privacy and fairness could demonstrate responsible AI research practices.</li><br><li> <b>Enhanced Honesty in Limitation Reporting</b>: Encourage the authors to be more forthcoming about uncertainties and potential criticisms of their work. This includes a critical self-evaluation of the study's limitations and an exploration of unresolved questions that could pave the way for future research.</li><br></ol>In conclusion, addressing these areas would not only align with the best practices outlined in the guidelines but also significantly enhance the paper's contribution, clarity, and integrity, ensuring it provides a comprehensive understanding of the proposed work's strengths and limitations.<br><br></div>
            <a class="move-to-top" href="#summary-question-2">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-3">
            <div class="user_input">
                <p><strong class="question">Question # 3:</strong> For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We have discussed the assumptions in the section 4 when we introduce graph characteristics.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The authors’ proposed answer and justification indicate a "Yes" response to the checklist question regarding whether each theoretical result provides the full set of assumptions and a complete (and correct) proof. The authors refer to Section 4 for discussions on assumptions, but the paper content overall does not explicitly delineate between the assumptions for the theoretical results and the proofs for such results in a structured manner that aligns with the guidelines' recommendations for clarity and completeness.<br><h3>Feedback for Improvement:</h3><ol><li> <b>Numbering of Theorems and Proofs:</b> Ensure all theorems, lemmas, and corresponding proofs in the paper are distinctly numbered and referenced within the text. This includes any results presented in the main body and supplementary material. This practice will facilitate easier navigation and review of the theoretical contributions of the paper.</li><br><li> <b>Clear Statement of Assumptions:</b> Each theorem or lemma should explicitly state or reference the assumptions under which the claim holds, directly at the beginning of the statement. If certain assumptions are generic across all theoretical results, consider summarizing these in a preliminary section but ensure specific references are made in each result.</li><br><li> <b>Location and Completeness of Proofs:</b> Confirm that proofs for all theoretical results are either included within the main document or clearly indicated in the supplementary material if length or detail necessitates external documentation. For proofs in the main paper, consider adding short proof sketches for intuition; for those in the supplemental, ensure there’s a direct link or clear guide on where to find these details.</li><br><li> <b>Cross-referencing Theorems and Lemmas:</b> Improve the interlinking of theoretical results by referencing dependent lemmas or previously established theorems within new proofs. This not only strengthens the narrative but also validates the reliance on earlier results.</li><br><li> <b>Guidance on Locating Assumptions and Proofs:</b> Amend the text in Section 4 or wherever the assumptions and proofs are discussed, to include explicit guidance on where the reader can find detailed proofs (e.g., "See Appendix A for the complete proof of Theorem 1"). This advice also applies to assumptions; guide the reader to specific sections or supplementary material where assumptions are elaborated.</li><br><li> <b>Assessment of Assumptions' Implications:</b> Provide a discussion on the implications of the assumptions for the practical application of the theoretical results. This includes any limitations these assumptions may impose and how they align with existing literature or real-world scenarios.</li><br></ol>In doing so, the paper would address critical elements of theoretical rigor and contribute more effectively to scholarly discourse by offering clear, accessible, and verifiable theoretical insights.<br></div>
            <a class="move-to-top" href="#summary-question-3">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-4">
            <div class="user_input">
                <p><strong class="question">Question # 4:</strong> Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We discuss the experiment settings in section 4, section 5 and appendix B, C, D, E including dataset, baseline, implementation and results. We also include our code in the github link https://github.com/AutoML-Research/SimpleSTG.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> The authors' proposed answer and justification seem to align well with the standards for reproducibility set forth by the NeurIPS conference guidelines. The Answer asserts full disclosure of necessary information for reproducing the main experimental results, citing specific sections of the paper and including a GitHub link for the code. This addresses both dimensions of reproducibility: access to code and detailed methodological descriptions.<br>However, without explicit access to the actual paper content and the GitHub repository, this review cannot definitively verify the completeness of the details or the functionality of the provided code. The general expectation from a rigor point of view is that the descriptions in the cited sections (experiment settings, dataset descriptions, baseline comparisons, implementation details, and results discussion) should meet certain criteria:<br><ol><li> <b>Dataset Description:</b> Availability, preprocessing steps, partition (train/validation/test split), and any generation process for synthetic data should be clearly articulated. Especially, any non-public dataset must have a thorough description to enable replication with alternative similar datasets.</li><br><li> <b>Baseline Comparisons:</b> The reproducibility of results heavily depends on precise baseline model descriptions, including hyperparameters and any custom modifications. This ensures that comparisons are fair and reproducible.</li><br><li> <b>Implementation Details:</b> Details like the version of software/dependencies, algorithmic peculiarities not covered by pseudocode, initialization methods, and hyperparameter selection process (including any grid search or random search bounds) are essential.</li><br><li> <b>Specificity in Results Discussion:</b> For completeness, statistical validation of results (e.g., reporting mean and standard deviation across multiple runs, error bars if applicable) increases reproducibility. This also includes any deviations from expected performance and potential reasons.</li><br><li> <b>Accessibility and Usability of Provided Code:</b> The existence of a GitHub repository is positive; however, it should ideally be accompanied by a README file detailing the environment setup, dataset preparation steps, execution instructions, and a license specifying terms of use. The code should be organized and commented sufficiently to understand the implementation logic and flow.</li><br><h3>Actionable Feedback</h3><br></ol>To improve the paper quality regarding reproducibility:<br><ul><li><b>Clarify Data Accessibility:</b> Ensure that the paper explicitly states how to access all datasets used in experiments or provides detailed guidance on how to simulate any unavailable datasets.</li><li><b>Enhance Description of Baselines:</b> Include precise configuration details for all baseline models, including hyperparameters and any software versions used.</li><li><b>Expand Implementation Details:</b> Beyond algorithmic descriptions, provide insights into practical challenges encountered during implementation and how they were resolved.</li><li><b>Provide Detailed Code Documentation:</b> Enhance the GitHub repository with extensive documentation covering setup, execution, and potentially a Dockerfile to simplify environment replication.</li><li><b>Statistical Analysis of Results:</b> If not already included, augment the results section with a deeper statistical analysis to strengthen the validity of the claims.</li><br></ul>In summary, while the authors have made an affirmative statement towards reproducibility, the verification of their claim relies on the depth of the details provided in the mentioned sections and the usability of the code in the repository. Assuming all stated resources are in place and thorough, this paper would be considered to have met the NeurIPS reproducibility criteria effectively.<br></div>
            <a class="move-to-top" href="#summary-question-4">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-5">
            <div class="user_input">
                <p><strong class="question">Question # 5:</strong> Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> All the data and code are open access. The dataset information is mentioned in section 4, 5, Appendix B and the code is available in github https://github.com/ AutoML-Research/SimpleSTG.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The Authors' Proposed Answer states that both data and code are openly accessible, with references to specific sections detailing dataset information and a GitHub URL for code access. This aligns well with the NeurIPS guidelines requiring sufficient detail for reproducing experimental results. However, for a comprehensive review, an examination of the provided sections and the linked GitHub repository is necessary to confirm that detailed instructions, including environmental setup, data preprocessing steps, and exact command lines for reproducing results, are indeed provided as required by the guidelines.<br>Actionable feedback to improve the paper quality based on provided guidelines and the given context might include:<br><ol><li> <b>Explicit Instructions for Environment Setup:</b> Ensure the GitHub README or supplementary materials explicitly mention the software environment and dependencies required to run the code. This includes specific versions of programming languages, libraries, and any other software necessary.</li><br><li> <b>Data Accessibility and Preparation Details:</b> Confirm that the GitHub repository or supplementary materials contain clear instructions on how to access, download, and prepare the data used in the experiments. This should cover how to preprocess the data if necessary, including any scripts provided for this purpose.</li><br><li> <b>Reproducibility of Results:</b> Verify that the exact command lines needed to reproduce the main results are included. This should cover training, evaluation, and any data processing steps. If any results cannot be reproduced due to computational constraints or other reasons, this should be clearly stated, explaining which results may differ and why.</li><br><li> <b>Anonymity Considerations:</b> Since the review process is double-blind, ensure that any links to code repositories or supplemental material are anonymized properly to preserve the review integrity. This may involve using a neutral URL redirector or providing code/data through a platform that does not reveal author identities.</li><br><li> <b>Licensing Information:</b> If not already included, provide information regarding the license under which the code and data are released. This ensures that users understand how they can use the resources legally.</li><br><li> <b>Baseline Comparisons:</b> If scripts for reproducing baseline comparisons are not included, explain why. If possible, offer scripts or guidance on how to replicate baseline model evaluations to allow for a fair comparison with the proposed method.</li><br><li> <b>Additional Resources:</b> If supplementary material or external resources are used (e.g., pre-trained models, additional datasets for comparison), ensure these are also accessible and adequately documented for users to employ in their experiments.</li><br></ol>To conclude, a review based on the NeurIPS guidelines and author responses suggests a close examination of the provided resources is crucial for determining the reproducibility of the results. Clear, step-by-step instructions that align with the experiment descriptions in the paper are essential for meeting NeurIPS standards for code and data sharing.<br><br></div>
            <a class="move-to-top" href="#summary-question-5">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-6">
            <div class="user_input">
                <p><strong class="question">Question # 6:</strong> Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Hyperparameter sampling is one of our method designs as explained in section and 5. We also specify the training and test details in the same sections and appendix B.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed author's answer and justification appear to match the content and the statements in the paper, with detailed sections referenced for where relevant information can be found (sections and Appendix B). However, there are several areas for improvement based on the guidelines for NeurIPS submission and standard practices for ensuring replicability and understanding of the experiments conducted. Here are the recommended improvements:<br><ol><li> <b>Clarify Hyperparameter Selection Process</b>: Though the paper mentions hyperparameters are studied, it would benefit from a more detailed explanation of how specific hyperparameters were chosen (e.g., why certain learning rates or batch sizes were preferred). Including rationale or results from preliminary experiments could enhance this section.</li><br><li> <b>More Detailed Data Splits Information</b>: The paper should specify the splits (training, validation, test) more explicitly. The exact percentage or the number of instances in each split should be provided to ensure reproducibility. If there were any stratification or special considerations made during splitting, these should be detailed.</li><br><li> <b>Optimizer Details</b>: The type of optimizer is mentioned, but additional details such as specific configurations (e.g., momentum terms for SGD if applicable, epsilon values for Adam, etc.) would be helpful. This ensures that the experiments can be replicated accurately by readers.</li><br><li> <b>Explanation of Hyperparameter Range Choices</b>: While the paper describes the exploration of hyperparameter spaces, it should also provide justification for the range or values selected for exploration. This could include references to literature or empirical evidence from preliminary studies.</li><br><li> <b>Reporting Model Initialization Methods</b>: The initialization method for model weights can significantly affect performance, especially in deep learning architectures. The paper should specify how the models were initialized before training.</li><br><li> <b>Explicit Mention of Any Data Augmentation</b>: If any data augmentation techniques were utilized in the experiments (e.g., for increasing the robustness of the model to variations in input data), these should be clearly detailed.</li><br><li> <b>Software and Hardware Details</b>: To further aid in the reproducibility of the results, the paper should mention the software (including versions) used for experiments, such as the deep learning framework (PyTorch, TensorFlow, etc.), and hardware specifications (GPUs, CPUs).</li><br><li> <b>Random Seed Reporting</b>: For reproducibility, the paper should state if a random seed was used to ensure deterministic results during splitting of data, initialization of models, and any other stochastic process.</li><br><li> <b>Complete List of Hyperparameters for Best Model</b>: The paper mentions detailing hyperparameter settings in sections and an appendix. It would be advantageous to have a consolidated list or table of the hyperparameters used for the best-performing model(s).</li><br></ol>Feedback focusing on these key improvements can help the authors better align their answers and justifications with the NeurIPS submission guidelines, fundamentally improving the paper's quality in terms of clarity, thoroughness, and reproducibility.<br><br></div>
            <a class="move-to-top" href="#summary-question-6">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-7">
            <div class="user_input">
                <p><strong class="question">Question # 7:</strong> Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> Our main experiments in section 4 and 5 all include variance as error bars. The experiments are based on multiple runs with mean and variance calculated.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> <b>Feedback for Improvement:</b><br><ol><li> <b>Clarify Error Bar Calculation and Assumptions:</b> The justification mentions that variance is used as error bars, but it does not detail how these error bars are calculated (e.g., standard deviation or standard error) or the assumption behind the error distribution (normally distributed or not). Clarification on the calculation method and assumptions should be added to ensure transparency and replicability.</li><br><li> <b>State the Specificity of Variability Factors:</b> While it is mentioned that variance as error bars is reported, the exact factors of variability (e.g., initialization, train/test splits) captured by these error bars are not specified. For a complete understanding, explicitly list the variability factors the error bars represent.</li><br><li> <b>Verify and Correct the Reporting of Error Bars for Asymmetric Distributions:</b> Given the guideline on asymmetric distributions, ensure that the error bars reported do not assume symmetry if the underlying error distribution is asymmetric. If the error rates are potentially out of range (e.g., could result in negative values), this should be corrected or explicitly addressed.</li><br><li> <b>Provide References to Figures/Tables for Error Bars:</b> Although the justification mentions sections where error bars are included, for enhanced clarity and easy verification by the reader, refer specifically to figures or tables where these error bars can be visually inspected. This direct referencing aids in better locating and understanding the reported statistical significance.</li><br><li> <b>Detail on Statistical Significance Tests:</b> If any statistical significance testing beyond error bars was performed to substantiate the experiments’ findings, such details are missing. Including information on any statistical tests (e.g., t-tests, ANOVA) used to compare model performances would strengthen the validation of the results.</li><br><li> <b>Report Confidence Interval if Applicable:</b> In cases where a 2-sigma error bar or a specific confidence interval (CI) is more informative, especially when the normality of errors cannot be assumed, opt to report this instead. This detail is crucial for interpreting the precision of the estimated parameters and the reliability of the results.</li><br><li> <b>Address the Representation of Error Bars in Figures/Tables:</b> Ensure that error bars presented in figures or tables are clearly explained in the text, indicating whether they represent standard errors, standard deviations, or confidence intervals. This clarification is necessary for accurate interpretation of the data.</li><br></ol><b>Recommendation:</b> Address these points to improve the transparency, accuracy, and comprehensiveness of the statistical reporting in the paper. Incorporating these changes will significantly enhance the readers' trust in the experimental results and conclusions drawn from them.<br></div>
            <a class="move-to-top" href="#summary-question-7">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-8">
            <div class="user_input">
                <p><strong class="question">Question # 8:</strong> For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> In section 5, we reported the GPU hours that are required to generate reasonable answers w.r.t. multiple baselines.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The Proposed Author's Answer states that sufficient information on computer resources is provided in section 5 by reporting GPU hours required for generating reasonable answers compared to multiple baselines. However, the guidelines for this checklist question specifically require details on the type of compute workers (CPU or GPU, internal cluster, or cloud provider), including relevant memory and storage, as well as the amount of compute required for each individual experimental run and an estimate of the total compute. It also asks to disclose whether the full research project required more compute than the experiments reported. Merely reporting GPU hours does not entirely meet these criteria as it lacks details on the exact computational resources (e.g., specific GPU models, memory, storage details) and does not cover all individual experiments or provide an estimation of total compute beyond GPU hours.<br>Actionable feedback based on the provided guidelines includes:<br><ol><li> <b>Specify the Type of Compute Workers:</b> Authors should include details on whether GPUs or CPUs were used for the experiments. If GPUs, specify the models (e.g., NVIDIA Tesla V100, etc.). It would clarify compute requirements for reproduction.</li><br><li> <b>Memory and Storage Details:</b> The paper should disclose the amount of RAM and storage space used or required for running the experiments, offering a clearer picture of the resource demands.</li><br><li> <b>Detailed Computational Requirements:</b> Instead of only stating GPU hours, it would be more helpful to provide the computational requirements for each type of experiment conducted. This includes the time of execution, the memory used during the execution, and any other relevant compute resource information.</li><br><li> <b>Estimation of Total Compute for the Project:</b> To give readers a more comprehensive understanding of the project's compute demands, an estimation of total compute should be provided. This includes compute for preliminary or failed experiments not reported in the final paper.</li><br><li> <b>Clarification on Compute for Different Experiments:</b> If various experiments required different levels of computational resources (which is often the case, especially with different model architectures or dataset sizes), specify this variance where applicable.</li><br><li> <b>Explicit Mention of Computational Resources for Reproducibility:</b> Ensure that the computational resource information is clearly stated in a way that allows readers to replicate the experiments without ambiguity regarding the required compute environment.</li><br></ol>This feedback aims at enhancing the clarity and completeness of the computational resource disclosure, ensuring the research can be accurately replicated and evaluated by others in the field.<br><br></div>
            <a class="move-to-top" href="#summary-question-8">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-9">
            <div class="user_input">
                <p><strong class="question">Question # 9:</strong> Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We review the code of ethics and our paper conform with it.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The author's justification for the ethical compliance of their paper with NeurIPS Code of Ethics is overly broad and lacks specificity. They merely state that the paper conforms to the code of ethics without providing details on how or where in the paper these ethical considerations are addressed.<br>Feedback:<br><ol><li> <b>Specify Ethical Considerations</b>: Provide specific sections or aspects of the research where ethical considerations, particularly pertinent to the NeurIPS Code of Ethics, have been addressed. This includes, but is not limited to, considerations around the societal impact of the research, data privacy, and the potential for misuse of the research findings.</li><br><li> <b>Detail Data Usage and Privacy</b>: Elaborate on the data acquisition, usage, and privacy measures, especially if the datasets include human-derived data. Identify any potential biases in the data and discuss efforts to mitigate such biases to ensure fairness and transparency.</li><br><li> <b>Assess the Impact of Research</b>: Discuss the potential positive and negative impacts of your research on society, including any long-term implications. If there are risks associated with the implementation of your research findings, specify measures to mitigate these risks.</li><br><li> <b>Acknowledge Limitations and Misuse Potential</b>: Acknowledge the limitations of your study and discuss the potential for misuse of your research findings. Include measures taken to prevent or mitigate such misuse.</li><br><li> <b>Compliance and Legal Considerations</b>: If applicable, discuss any compliance and legal considerations related to your research, including any licenses for datasets and software used in the study.</li><br><li> <b>Inclusion and Diversity</b>: Comment on steps taken to ensure inclusivity and diversity in your research process. This could involve the composition of the research team, the selection of data, and the potential impact on diverse populations.</li><br><li> <b>Anonymity and Review Process</b>: Since the NeurIPS submission process requires anonymity, ensure that no part of the paper or the supplemental materials (e.g., GitHub links, dataset references) compromises this requirement.</li><br><li> <b>Environmental Impact</b>: If relevant, especially for computationally intensive tasks like Neural Architecture Search (NAS), discuss the energy consumption and environmental impact of your experiments. Consider mentioning any efforts to minimize this impact.</li><br></ol>By providing detailed responses to these aspects aligned with the NeurIPS Code of Ethics, the authors can significantly improve the ethical consideration transparency of their paper. The justification should tie directly back to the content of the paper and the guidelines provided by NeurIPS.<br><br></div>
            <a class="move-to-top" href="#summary-question-9">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-10">
            <div class="user_input">
                <p><strong class="question">Question # 10:</strong> Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We discuss the broader impacts in the section "Broader Impact Statement".</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The provided author's answer and justification align well with the guidelines specified for discussing the societal impact in the NeurIPS submission. The authors affirmatively answered the question regarding the discussion of both potential positive and negative societal impacts, referencing the "Broader Impact Statement" section. This direct reference to a specific section fulfills the guideline requirement to pinpoint where in the paper such discussions can be found.<br>However, without access to the actual paper content and based solely on the given text, it's challenging to evaluate the completeness or depth of the discussion on societal impacts. To improve the paper's quality regarding societal impacts, authors might consider the following suggestions based on the provided guidelines and common expectations:<br><ol><li> <b>Elaborate on Both Positive and Negative Aspects</b>: Ensure the "Broader Impact Statement" section thoroughly discusses the specific positive impacts (e.g., improvements in traffic prediction aiding in smarter transportation systems) and potential negative impacts (e.g., misuse of prediction models for surveillance or privacy intrusion).</li><br><li> <b>Mitigation Strategies for Negative Impacts</b>: If negative impacts are identified, the paper should discuss potential mitigation strategies. This could include technical safeguards, ethical guidelines for use, or policy recommendations.</li><br><li> <b>Contextualization Within Society</b>: Discuss how the work fits into broader societal contexts. This could involve considerations of fairness (does the technology benefit or disadvantage certain groups?), privacy (are individuals' data protected?), and security (is the system resilient to malicious uses?).</li><br><li> <b>Empirical Data and Speculative Risks</b>: Balance the discussion with both empirical data available (e.g., results from real-world deployments, if any) and speculative risks that have not yet materialized but are plausible given the nature of the technology.</li><br><li> <b>Engagement with Ethical Standards and Guidelines</b>: Reference ethical standards or guidelines that have been followed in the research, development, and envisioned deployment of the technology. This could include data protection regulations, ethical AI principles, etc.</li><br><li> <b>Update and Accuracy</b>: Make sure the broader impacts discussion is up-to-date with current societal and technological contexts. As the field of AI rapidly evolves, so too do the potential societal impacts and ethical considerations.</li><br><li> <b>Stakeholder Analysis</b>: Include a brief analysis of stakeholders potentially affected by the technology (e.g., urban planners, the general public, policymakers) and how the work considers their needs, challenges, and potential impact perceptions.</li><br></ol>Given the answer's adherence to the guidelines by directly referring to a section that discusses societal impact, but without evidence regarding the depth or quality of the discussion, the following score is assigned:<br>This score reflects a need for improvement, specifically in ensuring a comprehensive, balanced, and nuanced discussion of societal impacts, as described in the actionable feedback above.</div>
            <a class="move-to-top" href="#summary-question-10">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-11">
            <div class="user_input">
                <p><strong class="question">Question # 11:</strong> Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We have discussed the details of the used datasets and models in section 4, 5 and appendix B.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed author's answer states that safeguards for the responsible release of data or models that have a high risk for misuse are discussed in sections 4, 5, and appendix B of the paper. However, the detail provided in the justification does not directly address the safeguards put in place to prevent misuse of the data or models, only mentioning the sections where the datasets and models used in the paper are discussed. The NeurIPS checklist question specifically asks for a description of the safeguards for responsible release, which implies a necessity for specifics on how the datasets and models are managed to prevent misuse, not just where they are mentioned within the paper.<br>Feedback to improve the paper quality:<br><ol><li> <b>Clarify Safeguards in the Specified Sections</b>: Ensure that sections 4, 5, and appendix B explicitly describe the safeguards in place for the responsible release of data and models. This might include access control measures, licenses that specify acceptable use cases, or mechanisms to anonymize data or limit the model's output to prevent misuse.</li><br><li> <b>Specific Examples of Safeguards</b>: Provide specific examples or scenarios within the mentioned sections where safeguards have been implemented. If certain datasets are restricted, mention the rationale and the type of restrictions placed. If models have been designed to mitigate risks of misuse (e.g., by incorporating fairness or privacy considerations directly into the model architecture or training process), detail these strategies.</li><br><li> <b>Compliance and Ethical Considerations</b>: Discuss any compliance measures with existing regulations (e.g., GDPR for European datasets) or widely accepted ethical guidelines for AI research. This can also include adherence to best practices for AI safety and ethics as recommended by leading organizations in the field.</li><br><li> <b>Alternatives and Limitations</b>: If there are inherent risks that cannot be fully mitigated, discuss these openly and specify any limitations in the current safeguarding approaches. Also, propose alternative solutions or future work directions that could help to better address these limitations.</li><br><li> <b>Community Engagement and Feedback</b>: Mention any efforts taken to engage with the broader AI or domain-specific community (e.g., transportation for STGNNs) to assess and refine the safeguard measures. Discuss any feedback mechanisms in place for users of the data or models to report concerns or misuse.</li><br></ol>By addressing these points, the paper can provide a transparent, comprehensive account of how it contributes to responsible AI research practices, aligning with NeurIPS guidelines and setting a precedent for future submissions.<br><br></div>
            <a class="move-to-top" href="#summary-question-11">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-12">
            <div class="user_input">
                <p><strong class="question">Question # 12:</strong> Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We explicitly mentioned and cited the source of the data used in our paper [13, 1, 10]. These datasets can be accessed on GitHub https://github.com/Davidham3/ ASTGCN/tree/master/data.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> The proposed answer and justification seem aligned with the guidelines provided, as the authors have cited the sources of the datasets used in their paper. However, there are a few key areas where improvements can be made, notably in:<br><ol><li> <b>Explicit Mention of License and Terms of Use</b>: Although the authors have cited the sources of the data, they have not explicitly mentioned the licenses or terms of use for each dataset. Given the guidelines' emphasis on clearly stating this information, this is a notable omission.</li><br><li> <b>Version of the Assets Used</b>: It's also important to specify which version of the data or asset is used if applicable. This detail is critical for ensuring reproducibility and for others to understand exactly which dataset version was used, considering datasets often get updated or revised.</li><br><li> <b>Clarification on Any Modifications or Re-packaging</b>: If there were any modifications made to the datasets or if they were repackaged in any way, this needs to be clearly stated along with how the original license was respected or altered. This is important for transparency and for others who may wish to use the modified data in the future.</li><br></ol><b>Actionable Feedback</b>:<br>A. <b>License and Terms of Use</b>: Enhance the dataset description section by explicitly stating the license under which each dataset is released (e.g., CC-BY 4.0) and adhering to the terms of use as specified by the data providers. If specific licensing information is not available, it would be beneficial to reach out to the dataset creators when possible or to clearly mention the effort to find and respect the license in the paper.<br>B. <b>Version Information</b>: Include the specific version of the dataset that was used for the research. This information aids in replicability and ensures that future research can accurately compare results or build upon the work.<br>C. <b>Detail Any Modifications Made</b>: If the datasets were modified, provide a clear description of these modifications and ensure that any re-packaging of the datasets respects the original licensing terms. If the license changed, both the original and new licenses should be mentioned.<br>D. <b>Compliance and Permissions</b>: If emails or other communications were exchanged to get permission for use, it would bolster the paper's compliance narrative to briefly mention this, ensuring all use is properly sanctioned.<br>E. <b>Supplementary Material</b>: For complex licensing situations or extensive modifications, consider adding a supplementary material section or an appendix in the paper that details these nuances for clarity and transparency.<br>Incorporating these improvements will enhance the paper's quality regarding the respectful and transparent use of external assets, thus aligning it more closely with academic standards and the NeurIPS guidelines.<br></div>
            <a class="move-to-top" href="#summary-question-12">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-13">
            <div class="user_input">
                <p><strong class="question">Question # 13:</strong> Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
                <p><strong class="answer">Author's answer:</strong> Yes</p>
                <p><strong class="justification">Author's justification:</strong> We have included enough documentation in section 4, 5 and appendix B with respect to our introduced benchmarks.</p>
            </div>
            
            <div class="llm_review llm_review-orange "><strong>LLM Review:</strong><br> <b>Itemized, Actionable Feedback:</b><br><ol><li> <b>Clarify Documentation Sections</b>: The proposed author's justification mentions Sections 4, 5, and Appendix B for documentation on new assets. Ensure these sections specifically include clear documentation on the designed framework, training parameters, benchmark datasets used, and hyperparameter settings. Explicitly address how each component contributes to the research and its novelty, ensuring comprehensiveness.</li><br><li> <b>Structured Templates for Asset Documentation</b>: Incorporate structured templates for documenting the new assets, including datasets, code, and models. Templates should cover training details, license information, limitations, and consent for used assets. This structured approach will enhance reproducibility and transparency.</li><br><li> <b>License and Consent</b>: Clarify in the documentation (preferably in Appendix B or a dedicated section) the license under which the new assets are released and confirm that consent was obtained for the use of any proprietary or third-party assets. If any asset involves human data, detail the consent process.</li><br><li> <b>Anonymization of Assets for Review</b>: Ensure the paper's supplemental materials (code/datasets) are anonymized if they are directly linked. Consider using anonymized URLs or a generic repository to share the assets during the review process. Mention this anonymization explicitly in the documentation sections.</li><br><li> <b>Detail on Asset Use and Limitations</b>: Expand the documentation to include a detailed discussion on how the introduced assets (e.g., datasets, models) can be used by other researchers. Address potential limitations or biases in the assets to guide future work and applications wisely.</li><br><li> <b>Enhanced Meta-Information for Datasets</b>: For any introduced datasets, provide enhanced meta-information covering the collection process, data preprocessing steps, dataset splits (training/validation/testing), and any preprocessing scripts used. This information is critical for reproducibility and should be included in Appendix B or a dedicated dataset documentation section.</li><br><li> <b>Code Documentation and Usability</b>: Ensure the provided code is well-documented, with comments explaining critical sections and a README file detailing setup instructions, dependencies, and usage examples. Highlight any framework-specific requirements (e.g., TensorFlow, PyTorch versions) clearly.</li><br><li> <b>Benchmark and Evaluation Script</b>: Alongside the main assets, provide a benchmark script for evaluating the models or algorithms introduced. This script should automate the evaluation process against the provided or referenced datasets, showing how performance metrics are calculated.</li><br></ol></div>
            <a class="move-to-top" href="#summary-question-13">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-14">
            <div class="user_input">
                <p><strong class="question">Question # 14:</strong> For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper does not involve crowdsourcing nor research with human subjects.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> The authors' response and justification correctly highlight that their paper does not involve crowdsourcing experiments or research with human subjects, thus marking the checklist question as "NA" (Not Applicable). This aligns with the guidelines provided, which stipulate that if a paper does not involve certain elements (such as crowdsourcing or human subjects research), marking the question as "NA" is appropriate. Given the content of the paper, which focuses on architecture search in Spatio-Temporal Graph Neural Networks for traffic prediction, there is no indication or necessity for involvement with human subjects or crowdsourcing experiments. Therefore, there are no discrepancies between the authors' answer and the content of the paper regarding this specific checklist question.<br>To ensure the quality of the paper and adherence to NeurIPS guidelines, here are key areas for possible improvement, based on the documentation provided:<br><ol><li> <b>Verification of Dataset Ethics Compliance</b>: While not directly related to the question at hand, ensuring that any data used, even if not derived from human subjects directly, complies with ethical standards is paramount. The authors should verify and, if applicable, include statements confirming that their dataset sourcing and usage comply with ethical guidelines.</li><br><li> <b>Transparent Methodology</b>: Though this checklist item was marked as "NA", it provides a good point of reflection for the authors to ensure that all other aspects of their methodology are transparent and detailed. This includes data preprocessing, model training details, and hyperparameter selection processes not involving human subjects but are crucial for replicating the study's results.</li><br><li> <b>Detailing Contributions and Limitations</b>: While directly not applicable, this question hints at the broader need to clearly outline the paper's contributions and any limitations it may have, including potential biases in the non-human-derived data or in the model architecture search process.</li><br><li> <b>Comprehensive Experiment Design Documentation</b>: For completeness, the paper should ensure that all experiment designs, including those not involving humans, are documented in detail, allowing for reproducibility and scrutiny by the research community.</li><br><li> <b>Future Work Suggestions</b>: While the current research may not involve human subjects, identifying any potential for future work that might involve crowdsourcing or human subjects could enrich the discussion section. This might include user studies to evaluate the practical utility of the proposed models in real-world scenarios.</li><br></ol>By addressing these points, the authors can enhance the clarity, reproducibility, and ethical adherence of their research, ensuring it meets NeurIPS' high standards.<br><br></div>
            <a class="move-to-top" href="#summary-question-14">↑ Back to summary</a>
        </div>
        
        <div class="review" id="question-15">
            <div class="user_input">
                <p><strong class="question">Question # 15:</strong> Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
                <p><strong class="answer">Author's answer:</strong> NA</p>
                <p><strong class="justification">Author's justification:</strong> This paper does not involve crowdsourcing nor research with human subjects.</p>
            </div>
            
            <div class="llm_review llm_review-green "><strong>LLM Review:</strong><br> Feedback:<br><ol><li> Clarify the Scope of Human Subjects Research: The paper strongly argues that the study does not involve crowdsourcing or research with human subjects, thus marking the IRB approval question as not applicable (NA). However, given the application areas mentioned (e.g., traffic prediction, smart transportation), it's important to explicitly clarify early in the manuscript (preferably in the introduction or methodological framework sections) that no data was collected directly from human subjects or involved human interaction that would require ethical review. This will reinforce the NA justification regarding IRB approvals and ethical considerations.</li><br><li> Address Potential Indirect Risks: While direct human subject involvement is denied, the paper should also consider and briefly discuss any potential indirect risks or ethical implications associated with the deployment of the proposed models in real-world settings (e.g., privacy concerns related to traffic data, potential biases in traffic prediction models, and their implications). This discussion doesn’t have to be extensive but should acknowledge awareness of these issues, how they might be mitigated, and affirm the commitment to ethical AI development practices.</li><br><li> Data Source and Privacy: The manuscript mentions using datasets from public sources for traffic forecasting tasks. It would be prudent to include a brief statement about the sourcing of this data, ensuring it complies with privacy laws and guidelines, and confirming that the data usage does not introduce ethical concerns. If the datasets are publicly available and widely used in research, citing prior studies that utilized these datasets can help support this claim.</li><br><li> Expansion of the Justification Section: While the justification for marking the IRB question as NA is clear, expanding this section to briefly cover the above points will make the ethical considerations of the paper robust. This is increasingly important in AI research, and taking a proactive stance on these issues can only benefit the paper.</li><br><li> Future Work Section: It could be valuable to include a future work section that discusses potential applications of the study’s findings in real-world scenarios while acknowledging the necessary ethical considerations and measures that will be taken to address them. This will demonstrate a forward-looking approach to responsibly translating research findings into practice.</li><br><br></ol></div>
            <a class="move-to-top" href="#summary-question-15">↑ Back to summary</a>
        </div>
        

        <!-- Survey -->
        <div>
            <p>Please complete the following survey about your experience:</p>
            <a class="button" href="https://docs.google.com/forms/d/e/1FAIpQLScr4fjvUGhtiTzBfsqm5CCVvAGafp3sLSSB_Txz2YHhnLiiyw/viewform?usp=pp_url&entry.1830873891=VW5kZXJzdGFuZGluZyBhbmQgU2ltcGxpZnlpbmcgQXJjaGl0ZWN0dXJlIFNlYXJjaCBpbiBTcGF0aW8tVGVtcG9yYWwgR3JhcGggTmV1cmFsIE5ldHdvcmtz" target="_blank">Click to submit post submission survey</a>
            <p>Your feedback is critical to evaluate the effectiveness of this LLM checklist assistant!</p>
        </div>
        
    </div>
</div>

</body>
</html>